{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5073ae0a-f2d7-4f9e-95c1-3353fef4304d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import torch.nn as nn\n",
    "import spacy\n",
    "from anchor import anchor_text\n",
    "import pickle\n",
    "from myUtils import *\n",
    "\n",
    "SEED = 84\n",
    "torch.manual_seed(SEED)\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07fc3de-eb19-4f46-bb94-6ad8a334e26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 20\n",
    "data_dir = os.path.expanduser('~/.pytorch-datasets')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c658cc2a-6339-4653-ac26-1e2b96f08b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_dataset = 'binary' # What kind of dataset to train on trinary, fine_grained or binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfc53c2-fc06-416f-914f-cc37591bbdae",
   "metadata": {},
   "source": [
    "# Preparing the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7153763e-eeda-494d-8147-02b34ce6979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.data\n",
    "import torchtext.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d5c575-cdf1-487d-9838-36fe38ae0f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(fine_grained = False):\n",
    "    # torchtext Field objects parse text (e.g. a review) and create a tensor representation\n",
    "\n",
    "    # This Field object will be used for tokenizing the movie reviews text\n",
    "    # For this application, tokens ~= words\n",
    "\n",
    "    review_parser = torchtext.data.Field(\n",
    "        sequential=True, use_vocab=True, lower=True, dtype=torch.long,\n",
    "        tokenize='spacy', tokenizer_language='en_core_web_sm', init_token='sos', eos_token='eos'\n",
    "    )\n",
    "\n",
    "    # This Field object converts the text labels into numeric values (0,1,2)\n",
    "    label_parser = torchtext.data.Field(\n",
    "        is_target=True, sequential=False, unk_token=None, use_vocab=True\n",
    "    )\n",
    "    \n",
    "    # Load SST, tokenize the samples and labels\n",
    "    # ds_X are Dataset objects which will use the parsers to return tensors\n",
    "    ds_train, ds_valid, ds_test = torchtext.datasets.SST.splits(\n",
    "        review_parser, label_parser, root=data_dir, fine_grained=fine_grained\n",
    "    )\n",
    "\n",
    "\n",
    "    return review_parser, label_parser, ds_train, ds_valid, ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7292e6d5-3aa5-4858-a47d-c795617969b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(review_parser, label_parser, ds_train, min_freq):\n",
    "    review_parser.build_vocab(ds_train, min_freq = min_freq)\n",
    "    label_parser.build_vocab(ds_train, min_freq= min_freq)\n",
    "\n",
    "    print(f\"Number of tokens in training samples: {len(review_parser.vocab)}\")\n",
    "    print(f\"Number of tokens in training labels: {len(label_parser.vocab)}\")\n",
    "    return review_parser, label_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c2524d-ed93-48a5-80e0-ce808b7429b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset(ds, classes, type_ds):\n",
    "    print(f'Number of {type_ds} samples: {len(ds)}') \n",
    "    for class_type in classes:\n",
    "        number_of_class = len([example for example in ds if example.label == class_type])\n",
    "        print(f'Number of {class_type} examples {number_of_class}')\n",
    "    print('------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47261662-e4ba-4eb0-a95a-a9d27a55fbea",
   "metadata": {},
   "source": [
    "# Creating Binary Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f25d3b7-9b7f-4617-a906-3c5a316b0c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_neutral(ds):\n",
    "    ds.examples = [example for example in ds.examples if example.label != 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddeed4e-4d4d-44df-9a9b-858573a99612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binary_dataset():\n",
    "    review_parser, label_parser, ds_train, ds_valid, ds_test = load_dataset(fine_grained = False)\n",
    "    review_parser, label_parser = build_vocabulary(review_parser, label_parser, ds_train, min_freq=5)\n",
    "    filter_neutral(ds_train)\n",
    "    filter_neutral(ds_valid)\n",
    "    filter_neutral(ds_test)\n",
    "    print_dataset(ds_train, ['positive', 'negative'], 'training')\n",
    "    print_dataset(ds_valid, ['positive', 'negative'], 'validation')\n",
    "    print_dataset(ds_test, ['positive', 'negative'], 'test')\n",
    "    return review_parser, label_parser, ds_train, ds_valid, ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3e900bc1-e4dd-46ff-8322-dafc83666b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in training samples: 3548\n",
      "Number of tokens in training labels: 3\n",
      "Number of training samples: 6920\n",
      "Number of positive examples 3610\n",
      "Number of negative examples 3310\n",
      "------------------------------------------------------\n",
      "Number of validation samples: 872\n",
      "Number of positive examples 444\n",
      "Number of negative examples 428\n",
      "------------------------------------------------------\n",
      "Number of test samples: 1821\n",
      "Number of positive examples 909\n",
      "Number of negative examples 912\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "review_parser = None\n",
    "label_parser = None\n",
    "ds_train = None\n",
    "ds_valid = None\n",
    "ds_test = None\n",
    "\n",
    "review_parser, label_parser, ds_train, ds_valid, ds_test = create_binary_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfe5f92-979e-4ad7-8554-63d62cb4a735",
   "metadata": {},
   "source": [
    "## Forward Function For Getting Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a38f96-6bff-4843-aa81-fdc2f4fe8d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import models\n",
    "def forward_dl(model, dl, device, type_dl):\n",
    "    model.train(False)\n",
    "    num_samples = len(dl) * dl.batch_size\n",
    "    num_batches = len(dl)  \n",
    "    pbar_name = type(model).__name__\n",
    "    list_y_real = []\n",
    "    list_y_pred = []\n",
    "    pbar_file = sys.stdout\n",
    "    num_correct = 0\n",
    "    dl_iter = iter(dl)\n",
    "    for batch_idx in range(num_batches):\n",
    "        data = next(dl_iter)\n",
    "        x, y = data.text, data.label\n",
    "        list_y_real.append(y)\n",
    "        x = x.to(device)  # (S, B, E)\n",
    "        y = y.to(device)  # (B,)\n",
    "        with torch.no_grad():\n",
    "            if isinstance(model, models.VanillaGRU):\n",
    "                y_pred_log_proba = model(x)\n",
    "            elif isinstance(model, models.MultiHeadAttentionNet):\n",
    "                y_pred_log_proba, _ = model(x)\n",
    "            y_pred = torch.argmax(y_pred_log_proba, dim=1)\n",
    "            num_correct += torch.sum(y_pred == y).float().item()\n",
    "            list_y_pred.append(y_pred)\n",
    "    accuracy = 100.0 * num_correct / num_samples\n",
    "    print(f'Accuracy for {type_dl} is {accuracy}')\n",
    "    \n",
    "    all_y_real = torch.cat(list_y_real)\n",
    "    all_y_pred = torch.cat(list_y_pred)\n",
    "    return all_y_real, all_y_pred, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65d9936-0842-4972-919a-34fef1817fb7",
   "metadata": {},
   "source": [
    "## Loading Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d5b488-66f8-4069-b0b0-82382fe431cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformerUtils.hyperparams as hyperparams\n",
    "def load_hyperparams(model_type, type_dataset):\n",
    "    hp = None\n",
    "    if model_type == 'gru':\n",
    "        hp = hyperparams.hyperparams_for_gru_binary()\n",
    "    elif model_type == 'attention':\n",
    "        hp = hyperparams.hyperparams_for_attention_binary()\n",
    "    print(hp)\n",
    "    return hp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b98c466-feba-43ac-acbe-31411d7a2266",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fedbeb-8050-4159-9e23-611ec26b72b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformerUtils.models as models\n",
    "\n",
    "def load_model(model_name, path):\n",
    "    if model_name == 'gru':\n",
    "        hp = load_hyperparams(model_name, type_dataset)\n",
    "        model = models.VanillaGRU(review_parser.vocab, hp['embedding_dim'], hp['hidden_dim'], hp['num_layers'], hp['output_classes'], hp['dropout']).to(device)\n",
    "    elif model_name == 'attention':\n",
    "        hp = load_hyperparams(model_name, type_dataset)\n",
    "        model = models.MultiHeadAttentionNet(input_vocabulary=review_parser.vocab, embed_dim=hp['embedding_dim'], num_heads=hp['num_heads'], \n",
    "                                           dropout=hp['dropout'], two_attention_layers=hp['two_atten_layers'], output_classes=hp['output_classes']).to(device)\n",
    "    saved_state = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(saved_state[\"model_state\"])\n",
    "    print(model)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153fa98e-e8bc-4543-8f98-e5fde929cc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('gru' , 'transformerUtils/gru.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459d3230-e32a-435c-a0eb-b52ccf07133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 = pad 2=sos 3 = eof \n",
    "def tokenize(text, max_len):\n",
    "    sentence = review_parser.tokenize(text)\n",
    "    input_tokens = [2] + [review_parser.vocab.stoi[word] for word in sentence] + [3] + [1]*(max_len-len(sentence))\n",
    "\n",
    "    return input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d5f77028-4510-4882-aa43-847c91ab3cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentences(sentences):\n",
    "    max_len = max([len(sentence) for sentence in sentences])\n",
    "    sentences = torch.tensor([tokenize(sentence, max_len) for sentence in sentences]).to(device)\n",
    "    input_tokens = torch.transpose(sentences, 0, 1)\n",
    "    output = model(input_tokens)\n",
    "    return torch.argmax(output, dim=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0ae844-8cfc-48c9-b46f-c41106d4dd8e",
   "metadata": {},
   "source": [
    "# Anchor Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1da1181f-98b5-4f15-9896-d4a99cf231be",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c976b285-caee-439e-870d-a43850a4551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = anchor_text.AnchorText(nlp, ['negative', 'positive'], use_unk_distribution=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1311a37d-a716-4bc8-9bd4-e2681f1851c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, train_labels = [' '.join(example.text) for example in ds_train], [example.label for example in ds_train]\n",
    "test, test_labels = [' '.join(example.text) for example in ds_valid], [example.label for example in ds_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6358237f-3408-43ff-b1c7-b0c880396138",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_examples = [example for example in train if len(example) < 70 and len(example)>10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "42bf0d93-b907-426f-828a-ffc1c5df31b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( test, open( \"results/transformer_test.pickle\", \"wb\" ))\n",
    "pickle.dump( test_labels, open( \"results/transformer_test_labels.pickle\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "48f4f322-c91e-47b0-b483-e1a335f346c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number 0\n",
      "number 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-229544c427f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmy_utils\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextUtils\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"results/transformer_exps.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mexplanations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_explanations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anchor/myUtils.py\u001b[0m in \u001b[0;36mcompute_explanations\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'number '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mcur_exp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_exp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0mcur_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fit_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_exp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mcur_test_cov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_test_cov\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_fit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anchor/myUtils.py\u001b[0m in \u001b[0;36mget_exp\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_exp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monepass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_fit_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anchor/anchor/anchor_text.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[0;34m(self, text, classifier_fn, threshold, delta, tau, batch_size, onepass, use_proba, beam_size, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m             text, classifier_fn, onepass=onepass, use_proba=use_proba)\n\u001b[1;32m    176\u001b[0m         \u001b[0;31m# print words, true_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         exp = anchor_base.AnchorBaseBeam.anchor_beam(\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0msample_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mdesired_confidence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_on_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anchor/anchor/anchor_base.py\u001b[0m in \u001b[0;36manchor_beam\u001b[0;34m(sample_fn, delta, epsilon, batch_size, min_shared_samples, desired_confidence, beam_size, verbose, epsilon_stop, min_samples_start, max_anchor_size, verbose_every, stop_on_first, coverage_samples)\u001b[0m\n\u001b[1;32m    310\u001b[0m                                                                   state)\n\u001b[1;32m    311\u001b[0m             \u001b[0;31m# print tuples, beam_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             chosen_tuples = AnchorBaseBeam.lucb(\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0msample_fns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeam_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anchor/anchor/anchor_base.py\u001b[0m in \u001b[0;36mlucb\u001b[0;34m(sample_fns, initial_stats, epsilon, delta, batch_size, top_n, verbose, verbose_every)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mmeans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mut\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpositives\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mut\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mut\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mn_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mpositives\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msample_fns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m             \u001b[0mmeans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpositives\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anchor/anchor/anchor_base.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(n, t)\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0msample_fns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcomplete_sample_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msample_fns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anchor/anchor/anchor_base.py\u001b[0m in \u001b[0;36mcomplete_sample_fn\u001b[0;34m(t, n)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0msample_fns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcomplete_sample_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mraw_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0mcurrent_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'current_idx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;31m# idxs = range(state['data'].shape[0], state['data'].shape[0] + n)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anchor/anchor/anchor_text.py\u001b[0m in \u001b[0;36msample_fn\u001b[0;34m(present, num_samples, compute_labels)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperturber\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                     \u001b[0mraw_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anchor/anchor/anchor_text.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monepass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mreps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anchor/anchor/anchor_text.py\u001b[0m in \u001b[0;36mprobs\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monepass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anchor/anchor/anchor_text.py\u001b[0m in \u001b[0;36munmask\u001b[0;34m(self, text_with_mask)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmasked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.8/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mconvert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_id_to_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mskip_special_tokens\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m    454\u001b[0m                           \u001b[0;34m'iterations executed (and might lead to errors or silently give '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                           'incorrect results).', category=RuntimeWarning)\n\u001b[0;32m--> 456\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "my_utils = TextUtils(anchor_examples, test, explainer, predict_sentences, \"results/transformer_exps.pickle\")\n",
    "explanations = my_utils.compute_explanations(list(range(len(anchor_examples))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167b36ee-6331-4696-bf56-5a3210db057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( explanations, open( \"results/transformer_exps_list.pickle\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dafb290-ff2c-43a7-b5d9-ca096a1647d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Training Function\n",
    "\n",
    "### Saves all the the output in the output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d03c1c3-e77b-4bde-9b0d-d35aca342ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperparams\n",
    "import torch.optim as optim\n",
    "import models\n",
    "import training\n",
    "import plot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "def train_model(model_name, device):\n",
    "    NUM_EPOCHS = 100\n",
    "    if model_name == 'gru':\n",
    "        hp = load_hyperparams(model_name, type_dataset)\n",
    "        model = models.VanillaGRU(review_parser.vocab, hp['embedding_dim'], hp['hidden_dim'], hp['num_layers'], hp['output_classes'], hp['dropout']).to(device)\n",
    "    elif model_name == 'attention':\n",
    "        hp = load_hyperparams(model_name, type_dataset)\n",
    "        model = models.MultiHeadAttentionNet(input_vocabulary=review_parser.vocab, embed_dim=hp['embedding_dim'], num_heads=hp['num_heads'], \n",
    "                                           dropout=hp['dropout'], two_attention_layers=hp['two_atten_layers'], output_classes=hp['output_classes']).to(device)\n",
    "    print(model)\n",
    "    dl_train, dl_valid, dl_test = torchtext.data.BucketIterator.splits((ds_train, ds_valid, ds_test), batch_size=hp['batch_size'], shuffle=True, device=device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=hp['lr'])\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    \n",
    "    trainer = training.SentimentTrainer(model, loss_fn, optimizer, device)\n",
    "    checkpoint_filename = str(output_directory) +  '/' + model_name\n",
    "    print(f'Saving checkpoint with prefix: {checkpoint_filename}')\n",
    "    fit_res = trainer.fit(dl_train, dl_valid, NUM_EPOCHS, early_stopping = hp['early_stopping'], checkpoints = checkpoint_filename, params = hp)\n",
    "\n",
    "    fig, axes = plot.plot_fit(fit_res)\n",
    "    fig.savefig(output_directory / str(model_name + '.png'))\n",
    "    saved_state = torch.load(checkpoint_filename + '.pt', map_location=device)\n",
    "    model.load_state_dict(saved_state[\"model_state\"])\n",
    "    loaded_hp = saved_state[\"parameters\"]\n",
    "    print('----- Loaded params ------')\n",
    "    print(loaded_hp)\n",
    "    all_dataloaders = [dl_train, dl_valid, dl_test]\n",
    "    type_dls = ['train', 'valid', 'test']\n",
    "    accuracies = []\n",
    "    for dl, type_dl in zip(all_dataloaders, type_dls):\n",
    "        y_real, y_pred, accuracy = forward_dl(model, dl, device, type_dl)\n",
    "        df = compute_confusion_matrix(y_real, y_pred, model_name, type_dl)\n",
    "        accuracies.append(accuracy)\n",
    "        display(df)\n",
    "    numpy_accuracy = np.array(accuracies)\n",
    "    df = pd.DataFrame(numpy_accuracy, index = type_dls, dtype=float)\n",
    "    df.to_csv(output_directory / str('accuracies_' + model_name + '.csv')) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e441f304-4ae5-4c1d-afd5-941b77585aa2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Training GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16d185d-2147-483d-9304-d32103c19b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 84\n",
    "torch.manual_seed(SEED)\n",
    "train_model(model_type, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547e2ddf-b673-4748-82aa-8a016c7efbb6",
   "metadata": {},
   "source": [
    "# Training Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b834790e-aad3-4c29-9a2b-5f2ce1abec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 84\n",
    "torch.manual_seed(SEED)\n",
    "train_model('attention', device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5073ae0a-f2d7-4f9e-95c1-3353fef4304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import torch.nn as nn\n",
    "SEED = 84\n",
    "torch.manual_seed(SEED)\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f07fc3de-eb19-4f46-bb94-6ad8a334e26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "plt.rcParams['font.size'] = 20\n",
    "data_dir = os.path.expanduser('~/.pytorch-datasets')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c658cc2a-6339-4653-ac26-1e2b96f08b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_dataset = 'binary' # What kind of dataset to train on trinary, fine_grained or binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfc53c2-fc06-416f-914f-cc37591bbdae",
   "metadata": {},
   "source": [
    "# Preparing the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7153763e-eeda-494d-8147-02b34ce6979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.data\n",
    "import torchtext.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20d5c575-cdf1-487d-9838-36fe38ae0f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(fine_grained = False):\n",
    "    # torchtext Field objects parse text (e.g. a review) and create a tensor representation\n",
    "\n",
    "    # This Field object will be used for tokenizing the movie reviews text\n",
    "    # For this application, tokens ~= words\n",
    "\n",
    "    review_parser = torchtext.data.Field(\n",
    "        sequential=True, use_vocab=True, lower=True, dtype=torch.long,\n",
    "        tokenize='spacy', tokenizer_language='en_core_web_sm', init_token='sos', eos_token='eos'\n",
    "    )\n",
    "\n",
    "    # This Field object converts the text labels into numeric values (0,1,2)\n",
    "    label_parser = torchtext.data.Field(\n",
    "        is_target=True, sequential=False, unk_token=None, use_vocab=True\n",
    "    )\n",
    "    \n",
    "    # Load SST, tokenize the samples and labels\n",
    "    # ds_X are Dataset objects which will use the parsers to return tensors\n",
    "    ds_train, ds_valid, ds_test = torchtext.datasets.SST.splits(\n",
    "        review_parser, label_parser, root=data_dir, fine_grained=fine_grained\n",
    "    )\n",
    "\n",
    "\n",
    "    return review_parser, label_parser, ds_train, ds_valid, ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7292e6d5-3aa5-4858-a47d-c795617969b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(review_parser, label_parser, ds_train, min_freq):\n",
    "    review_parser.build_vocab(ds_train, min_freq = min_freq)\n",
    "    label_parser.build_vocab(ds_train, min_freq= min_freq)\n",
    "\n",
    "    print(f\"Number of tokens in training samples: {len(review_parser.vocab)}\")\n",
    "    print(f\"Number of tokens in training labels: {len(label_parser.vocab)}\")\n",
    "    return review_parser, label_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25c2524d-ed93-48a5-80e0-ce808b7429b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset(ds, classes, type_ds):\n",
    "    print(f'Number of {type_ds} samples: {len(ds)}') \n",
    "    for class_type in classes:\n",
    "        number_of_class = len([example for example in ds if example.label == class_type])\n",
    "        print(f'Number of {class_type} examples {number_of_class}')\n",
    "    print('------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47261662-e4ba-4eb0-a95a-a9d27a55fbea",
   "metadata": {},
   "source": [
    "# Creating Binary Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f25d3b7-9b7f-4617-a906-3c5a316b0c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_neutral(ds):\n",
    "    ds.examples = [example for example in ds.examples if example.label != 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ddeed4e-4d4d-44df-9a9b-858573a99612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binary_dataset():\n",
    "    review_parser, label_parser, ds_train, ds_valid, ds_test = load_dataset(fine_grained = False)\n",
    "    review_parser, label_parser = build_vocabulary(review_parser, label_parser, ds_train, min_freq=5)\n",
    "    filter_neutral(ds_train)\n",
    "    filter_neutral(ds_valid)\n",
    "    filter_neutral(ds_test)\n",
    "    print_dataset(ds_train, ['positive', 'negative'], 'training')\n",
    "    print_dataset(ds_valid, ['positive', 'negative'], 'validation')\n",
    "    print_dataset(ds_test, ['positive', 'negative'], 'test')\n",
    "    return review_parser, label_parser, ds_train, ds_valid, ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e900bc1-e4dd-46ff-8322-dafc83666b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in training samples: 3548\n",
      "Number of tokens in training labels: 3\n",
      "Number of training samples: 6920\n",
      "Number of positive examples 3610\n",
      "Number of negative examples 3310\n",
      "------------------------------------------------------\n",
      "Number of validation samples: 872\n",
      "Number of positive examples 444\n",
      "Number of negative examples 428\n",
      "------------------------------------------------------\n",
      "Number of test samples: 1821\n",
      "Number of positive examples 909\n",
      "Number of negative examples 912\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "review_parser = None\n",
    "label_parser = None\n",
    "ds_train = None\n",
    "ds_valid = None\n",
    "ds_test = None\n",
    "\n",
    "review_parser, label_parser, ds_train, ds_valid, ds_test = create_binary_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a850d66e-116d-4d80-a4c3-ed5a91020483",
   "metadata": {},
   "source": [
    "# Utility Functions For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fc912aa-6d79-4c5b-bd12-3708cf2cc2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "model_type = 'gru' # what kind of model to train gru or attention\n",
    "output_directory = Path('results')\n",
    "output_directory.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfe5f92-979e-4ad7-8554-63d62cb4a735",
   "metadata": {},
   "source": [
    "## Forward Function For Getting Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9a38f96-6bff-4843-aa81-fdc2f4fe8d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import models\n",
    "def forward_dl(model, dl, device, type_dl):\n",
    "    model.train(False)\n",
    "    num_samples = len(dl) * dl.batch_size\n",
    "    num_batches = len(dl)  \n",
    "    pbar_name = type(model).__name__\n",
    "    list_y_real = []\n",
    "    list_y_pred = []\n",
    "    pbar_file = sys.stdout\n",
    "    num_correct = 0\n",
    "    dl_iter = iter(dl)\n",
    "    for batch_idx in range(num_batches):\n",
    "        data = next(dl_iter)\n",
    "        x, y = data.text, data.label\n",
    "        list_y_real.append(y)\n",
    "        x = x.to(device)  # (S, B, E)\n",
    "        y = y.to(device)  # (B,)\n",
    "        with torch.no_grad():\n",
    "            if isinstance(model, models.VanillaGRU):\n",
    "                y_pred_log_proba = model(x)\n",
    "            elif isinstance(model, models.MultiHeadAttentionNet):\n",
    "                y_pred_log_proba, _ = model(x)\n",
    "            y_pred = torch.argmax(y_pred_log_proba, dim=1)\n",
    "            num_correct += torch.sum(y_pred == y).float().item()\n",
    "            list_y_pred.append(y_pred)\n",
    "    accuracy = 100.0 * num_correct / num_samples\n",
    "    print(f'Accuracy for {type_dl} is {accuracy}')\n",
    "    \n",
    "    all_y_real = torch.cat(list_y_real)\n",
    "    all_y_pred = torch.cat(list_y_pred)\n",
    "    return all_y_real, all_y_pred, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65d9936-0842-4972-919a-34fef1817fb7",
   "metadata": {},
   "source": [
    "## Loading Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7d5b488-66f8-4069-b0b0-82382fe431cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformerUtils.hyperparams as hyperparams\n",
    "def load_hyperparams(model_type, type_dataset):\n",
    "    hp = None\n",
    "    if model_type == 'gru':\n",
    "        hp = hyperparams.hyperparams_for_gru_binary()\n",
    "    elif model_type == 'attention':\n",
    "        hp = hyperparams.hyperparams_for_attention_binary()\n",
    "    print(hp)\n",
    "    return hp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b98c466-feba-43ac-acbe-31411d7a2266",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3fedbeb-8050-4159-9e23-611ec26b72b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformerUtils.models as models\n",
    "\n",
    "def load_model(model_name, path):\n",
    "    if model_name == 'gru':\n",
    "        hp = load_hyperparams(model_name, type_dataset)\n",
    "        model = models.VanillaGRU(review_parser.vocab, hp['embedding_dim'], hp['hidden_dim'], hp['num_layers'], hp['output_classes'], hp['dropout']).to(device)\n",
    "    elif model_name == 'attention':\n",
    "        hp = load_hyperparams(model_name, type_dataset)\n",
    "        model = models.MultiHeadAttentionNet(input_vocabulary=review_parser.vocab, embed_dim=hp['embedding_dim'], num_heads=hp['num_heads'], \n",
    "                                           dropout=hp['dropout'], two_attention_layers=hp['two_atten_layers'], output_classes=hp['output_classes']).to(device)\n",
    "    saved_state = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(saved_state[\"model_state\"])\n",
    "    print(model)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "153fa98e-e8bc-4543-8f98-e5fde929cc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_dim': 100, 'batch_size': 32, 'hidden_dim': 256, 'num_layers': 2, 'dropout': 0.7, 'lr': 0.0005, 'early_stopping': 5, 'output_classes': 2}\n",
      "VanillaGRU(\n",
      "  (embedding_layer): Embedding(3548, 100)\n",
      "  (GRU_layer): GRU(100, 256, num_layers=2, dropout=0.7)\n",
      "  (dropout_layer): Dropout(p=0.7, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = load_model('gru' , 'transformerUtils/gru.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "459d3230-e32a-435c-a0eb-b52ccf07133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 = pad 2=sos 3 = eof \n",
    "def tokenize(text, max_len):\n",
    "    sentence = review_parser.tokenize(text)\n",
    "    input_tokens = [2] + [review_parser.vocab.stoi[word] for word in sentence] + [3] + [1]*(max_len-len(sentence))\n",
    "\n",
    "    return input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d5f77028-4510-4882-aa43-847c91ab3cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentences(sentences):\n",
    "    max_len = max([len(sentence) for sentence in sentences])\n",
    "    sentences = torch.tensor([tokenize(sentence, max_len) for sentence in sentences])\n",
    "    input_tokens = torch.transpose(sentences, 0, 1).to(device)\n",
    "    output = model(input_tokens)\n",
    "    return torch.argmax(output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "84e4ef74-09e0-4bce-91af-b42f4fb2f379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1], device='cuda:0')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentences([\"very great movie\", \"worse movie\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dafb290-ff2c-43a7-b5d9-ca096a1647d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training Function\n",
    "\n",
    "### Saves all the the output in the output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d03c1c3-e77b-4bde-9b0d-d35aca342ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperparams\n",
    "import torch.optim as optim\n",
    "import models\n",
    "import training\n",
    "import plot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "def train_model(model_name, device):\n",
    "    NUM_EPOCHS = 100\n",
    "    if model_name == 'gru':\n",
    "        hp = load_hyperparams(model_name, type_dataset)\n",
    "        model = models.VanillaGRU(review_parser.vocab, hp['embedding_dim'], hp['hidden_dim'], hp['num_layers'], hp['output_classes'], hp['dropout']).to(device)\n",
    "    elif model_name == 'attention':\n",
    "        hp = load_hyperparams(model_name, type_dataset)\n",
    "        model = models.MultiHeadAttentionNet(input_vocabulary=review_parser.vocab, embed_dim=hp['embedding_dim'], num_heads=hp['num_heads'], \n",
    "                                           dropout=hp['dropout'], two_attention_layers=hp['two_atten_layers'], output_classes=hp['output_classes']).to(device)\n",
    "    print(model)\n",
    "    dl_train, dl_valid, dl_test = torchtext.data.BucketIterator.splits((ds_train, ds_valid, ds_test), batch_size=hp['batch_size'], shuffle=True, device=device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=hp['lr'])\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    \n",
    "    trainer = training.SentimentTrainer(model, loss_fn, optimizer, device)\n",
    "    checkpoint_filename = str(output_directory) +  '/' + model_name\n",
    "    print(f'Saving checkpoint with prefix: {checkpoint_filename}')\n",
    "    fit_res = trainer.fit(dl_train, dl_valid, NUM_EPOCHS, early_stopping = hp['early_stopping'], checkpoints = checkpoint_filename, params = hp)\n",
    "\n",
    "    fig, axes = plot.plot_fit(fit_res)\n",
    "    fig.savefig(output_directory / str(model_name + '.png'))\n",
    "    saved_state = torch.load(checkpoint_filename + '.pt', map_location=device)\n",
    "    model.load_state_dict(saved_state[\"model_state\"])\n",
    "    loaded_hp = saved_state[\"parameters\"]\n",
    "    print('----- Loaded params ------')\n",
    "    print(loaded_hp)\n",
    "    all_dataloaders = [dl_train, dl_valid, dl_test]\n",
    "    type_dls = ['train', 'valid', 'test']\n",
    "    accuracies = []\n",
    "    for dl, type_dl in zip(all_dataloaders, type_dls):\n",
    "        y_real, y_pred, accuracy = forward_dl(model, dl, device, type_dl)\n",
    "        df = compute_confusion_matrix(y_real, y_pred, model_name, type_dl)\n",
    "        accuracies.append(accuracy)\n",
    "        display(df)\n",
    "    numpy_accuracy = np.array(accuracies)\n",
    "    df = pd.DataFrame(numpy_accuracy, index = type_dls, dtype=float)\n",
    "    df.to_csv(output_directory / str('accuracies_' + model_name + '.csv')) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e441f304-4ae5-4c1d-afd5-941b77585aa2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Training GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16d185d-2147-483d-9304-d32103c19b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 84\n",
    "torch.manual_seed(SEED)\n",
    "train_model(model_type, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547e2ddf-b673-4748-82aa-8a016c7efbb6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Training Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b834790e-aad3-4c29-9a2b-5f2ce1abec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 84\n",
    "torch.manual_seed(SEED)\n",
    "train_model('attention', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9997811d-4a10-4d7a-8f93-0021653285bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(vocabulary, sentence):\n",
    "    input_tokens = torch.zeros(len(sentence) + 2).type(torch.long)\n",
    "    for index, word in enumerate(sentence):\n",
    "        input_tokens[index + 1] = vocabulary.stoi[word]\n",
    "    input_tokens[0] = vocabulary.stoi['sos']\n",
    "    input_tokens[-1] = vocabulary.stoi['eos']\n",
    "    return input_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

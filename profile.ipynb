{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5073ae0a-f2d7-4f9e-95c1-3353fef4304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import spacy\n",
    "from modified_anchor import anchor_text\n",
    "import pickle\n",
    "from myUtils import *\n",
    "from transformer.utils import *\n",
    "from dataset.dataset_loader import *\n",
    "import datetime\n",
    "%load_ext line_profiler\n",
    "\n",
    "SEED = 84\n",
    "torch.manual_seed(SEED)\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e586528b-69e9-42c2-94c4-f8f70352ed7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e75c7403-06b4-41a3-82f1-1bdfe2cde2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Profile printout saved to text file 'alon.txt'. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 0.002409 s\n",
       "File: /tmp/ipykernel_56833/2452235315.py\n",
       "Function: sum_of at line 1\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     1                                           def sum_of(N):\n",
       "     2         1          3.0      3.0      0.1      total = 0\n",
       "     3         1         11.0     11.0      0.5      check()\n",
       "     4         6          9.0      1.5      0.4      for i in range(5):\n",
       "     5         5       2318.0    463.6     96.2          L = [j ^ (j >> i) for j in range(N)]\n",
       "     6         5         67.0     13.4      2.8          total += sum(L)\n",
       "     7         1          1.0      1.0      0.0      return total\n",
       "\n",
       "Total time: 3e-06 s\n",
       "File: /tmp/ipykernel_56833/3948132612.py\n",
       "Function: check at line 1\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     1                                           def check():\n",
       "     2         1          1.0      1.0     33.3      x = 5\n",
       "     3         1          1.0      1.0     33.3      y=17\n",
       "     4         1          1.0      1.0     33.3      return 1*3*65"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%lprun -f sum_of -f check -T alon.txt sum_of(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f07fc3de-eb19-4f46-bb94-6ad8a334e26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "plt.rcParams['font.size'] = 20\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2444c8e5-2a8b-4797-9248-46565487f5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in training samples: 3307\n",
      "Number of tokens in training labels: 2\n"
     ]
    }
   ],
   "source": [
    "# can be sentiment/spam/offensive\n",
    "dataset_name = 'sentiment'\n",
    "review_parser, label_parser, ds_train, ds_val, _ = create_sentiment_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "153fa98e-e8bc-4543-8f98-e5fde929cc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_dim': 100, 'batch_size': 32, 'hidden_dim': 256, 'num_layers': 2, 'dropout': 0.3, 'lr': 5e-05, 'early_stopping': 5, 'output_classes': 2}\n",
      "VanillaGRU(\n",
      "  (embedding_layer): Embedding(3307, 100)\n",
      "  (GRU_layer): GRU(100, 256, num_layers=2, dropout=0.3)\n",
      "  (dropout_layer): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = load_model('gru' , f'transformer/{dataset_name}/gru.pt', review_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "459d3230-e32a-435c-a0eb-b52ccf07133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 = pad 2=sos 3 = eos\n",
    "def tokenize(text, max_len):\n",
    "    sentence = review_parser.tokenize(str(text))\n",
    "    input_tokens = [2] + [review_parser.vocab.stoi[word] for word in sentence] + [3] + [1]*(max_len-len(sentence))\n",
    "\n",
    "    return input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5f77028-4510-4882-aa43-847c91ab3cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentences(sentences):\n",
    "    half_length = len(sentences)//2\n",
    "    if(half_length>100):\n",
    "        return np.concatenate([predict_sentences(sentences[:half_length]), predict_sentences(sentences[half_length:])])\n",
    "    max_len = max([len(sentence) for sentence in sentences])\n",
    "    sentences = torch.tensor([tokenize(sentence, max_len) for sentence in sentences]).to(device)\n",
    "    input_tokens = torch.transpose(sentences, 0, 1)\n",
    "    output = model(input_tokens)\n",
    "\n",
    "    return torch.argmax(output, dim=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0ae844-8cfc-48c9-b46f-c41106d4dd8e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Anchor Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1da1181f-98b5-4f15-9896-d4a99cf231be",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c976b285-caee-439e-870d-a43850a4551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = anchor_text.AnchorText(nlp, ['positive', 'negative'], use_unk_distribution=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1311a37d-a716-4bc8-9bd4-e2681f1851c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, train_labels = [' '.join(example.text) for example in ds_train], [example.label for example in ds_train]\n",
    "test, test_labels = [' '.join(example.text) for example in ds_train], [example.label for example in ds_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6358237f-3408-43ff-b1c7-b0c880396138",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_examples = [example for example in train if len(example) < 90 and len(example)>20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e877c34b-3205-4643-b038-b16698e445cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2272"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anchor_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3ff652c-f954-4ce5-b940-4752f68b90c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "def get_ignored(anchor_sentences):\n",
    "    sentences = [[x.text for x in nlp(sentence)] for sentence in anchor_sentences]\n",
    "    min_occurence = 1\n",
    "    c = Counter()\n",
    "    stop_words = list(\".,- \\'\\\"\\s[]?():!;\")\n",
    "    stop_words.extend([\"--\", \"'s\", 'sos', 'eos'])\n",
    "    stop_words.extend(stopwords.words('english'))\n",
    "    \"\"\"\n",
    "    for sentence in sentences:\n",
    "        c.update(sentence)\n",
    "    sums = 0\n",
    "    for ignore_s in stop_words:\n",
    "        sums+=c[ignore_s]\n",
    "        del c[ignore_s]\n",
    "    print(sums)\n",
    "    ignored_anchors = stop_words\n",
    "    for key in c.keys():\n",
    "        if c[key]<=min_occurence:\n",
    "            ignored_anchors.append(key)\n",
    "    print(len(c.keys()))\n",
    "    return ignored_anchors\n",
    "    \"\"\"\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50cb3bc3-5a28-49a4-af65-914fd963edb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignored = get_ignored(anchor_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f52d42-f62d-4dab-a6a4-2c2e458b7433",
   "metadata": {},
   "source": [
    "## notice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a75c6b9-f4a4-4a2e-b9a7-32f0f99158bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignored = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ea55a5e-b66e-456f-acfa-62792485ce3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-09 16:59:40.851284\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "deb28ed9-5247-4fa3-9e25-860150791c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09e3b250-34e7-4232-afdb-76f1c3ee2221",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_examples = anchor_examples[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d13ac6a5-37a6-42e9-915d-148ff3ea9075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number 0\n",
      "[1.0]\n",
      "[1.0]\n",
      "[0.9572649572649573]\n",
      "[1.0]\n",
      "[0.8823529411764706]\n",
      "[0.9803921568627451]\n",
      "[0.9402985074626866]\n",
      "[0.9381443298969072]\n",
      "[0.8877551020408163]\n",
      "[0.89]\n",
      "[0.9298245614035088]\n",
      "[0.948051948051948]\n",
      "[1.0]\n",
      " \n",
      "*** Profile printout saved to text file 'profile.txt'. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "         5037417 function calls (4998212 primitive calls) in 8.333 seconds\n",
       "\n",
       "   Ordered by: cumulative time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "        1    0.000    0.000    8.334    8.334 {built-in method builtins.exec}\n",
       "        1    0.000    0.000    8.334    8.334 <string>:1(<module>)\n",
       "        1    0.000    0.000    8.334    8.334 myUtils.py:62(compute_explanations)\n",
       "        1    0.010    0.010    8.239    8.239 myUtils.py:39(get_exp)\n",
       "        1    0.002    0.002    8.229    8.229 anchor_text.py:190(explain_instance)\n",
       "        1    0.001    0.001    8.115    8.115 anchor_base.py:275(anchor_beam)\n",
       "      154    0.030    0.000    7.730    0.050 anchor_text.py:153(sample_fn)\n",
       "      148    0.000    0.000    7.696    0.052 anchor_base.py:228(<lambda>)\n",
       "      148    0.009    0.000    7.695    0.052 anchor_base.py:175(complete_sample_fn)\n",
       "     1468    0.035    0.000    6.948    0.005 anchor_text.py:89(sample)\n",
       "     1481    0.005    0.000    4.999    0.003 anchor_text.py:105(probs)\n",
       "      366    0.098    0.000    4.957    0.014 anchor_text.py:39(unmask)\n",
       "33498/520    0.100    0.000    2.319    0.004 module.py:1096(_call_impl)\n",
       "    16764    1.944    0.000    2.068    0.000 {method 'choice' of 'numpy.random.mtrand.RandomState' objects}\n",
       "      366    0.010    0.000    2.036    0.006 modeling_distilbert.py:605(forward)\n",
       "     1468    0.019    0.000    1.988    0.001 anchor_text.py:96(<listcomp>)\n",
       "      366    0.005    0.000    1.954    0.005 modeling_distilbert.py:509(forward)\n",
       "      366    0.014    0.000    1.866    0.005 modeling_distilbert.py:302(forward)\n",
       "     2196    0.115    0.000    1.827    0.001 modeling_distilbert.py:260(forward)\n",
       "     3355    0.808    0.000    1.761    0.001 anchor_text.py:62(<listcomp>)\n",
       "     2196    0.193    0.000    1.059    0.000 modeling_distilbert.py:171(forward)\n",
       "     6864    0.759    0.000    0.759    0.000 {method 'cpu' of 'torch._C._TensorBase' objects}\n",
       "      154    0.003    0.000    0.634    0.004 3768568908.py:1(predict_sentences)\n",
       "  1737092    0.598    0.000    0.598    0.000 {method 'get' of 'dict' objects}\n",
       "    14062    0.031    0.000    0.582    0.000 linear.py:102(forward)\n",
       "    14062    0.018    0.000    0.541    0.000 functional.py:1832(linear)\n",
       "    14062    0.518    0.000    0.518    0.000 {built-in method torch._C._nn.linear}\n",
       "     2196    0.004    0.000    0.438    0.000 modeling_distilbert.py:237(forward)\n",
       "     2196    0.009    0.000    0.434    0.000 modeling_utils.py:2299(apply_chunking_to_forward)\n",
       "  1684968    0.368    0.000    0.368    0.000 tokenization_utils_base.py:976(unk_token)\n",
       "      154    0.001    0.000    0.325    0.002 3768568908.py:6(<listcomp>)\n",
       "     1468    0.008    0.000    0.324    0.000 2852908370.py:2(tokenize)\n",
       "      301    0.321    0.001    0.321    0.001 {method 'astype' of 'numpy.ndarray' objects}\n",
       "     1468    0.006    0.000    0.306    0.000 utils.py:13(_spacy_tokenize)\n",
       "     2196    0.032    0.000    0.305    0.000 modeling_distilbert.py:240(ff_chunk)\n",
       "      154    0.005    0.000    0.278    0.002 models.py:52(forward)\n",
       "     1469    0.014    0.000    0.255    0.000 tokenizer.pyx:136(__call__)\n",
       "      154    0.002    0.000    0.246    0.002 rnn.py:824(forward)\n",
       "      154    0.238    0.002    0.238    0.002 {built-in method gru}\n",
       "     2458    0.003    0.000    0.193    0.000 tokenizer.pyx:233(_tokenize)\n",
       "     5124    0.016    0.000    0.192    0.000 normalization.py:188(forward)\n",
       "        1    0.000    0.000    0.179    0.179 anchor_base.py:64(lucb)\n",
       "     5124    0.016    0.000    0.172    0.000 functional.py:2332(layer_norm)\n",
       "      366    0.002    0.000    0.162    0.000 tokenization_utils_base.py:2143(encode)\n",
       "      366    0.003    0.000    0.160    0.000 tokenization_utils_base.py:2459(encode_plus)\n",
       "     2458    0.013    0.000    0.156    0.000 tokenizer.pyx:297(_attach_tokens)\n",
       "      366    0.002    0.000    0.155    0.000 tokenization_utils.py:586(_encode_plus)\n",
       "     5124    0.149    0.000    0.149    0.000 {built-in method layer_norm}\n",
       "     4392    0.138    0.000    0.138    0.000 {built-in method matmul}\n",
       "     3205    0.004    0.000    0.128    0.000 vocab.pyx:134(get)\n",
       "     2449    0.039    0.000    0.124    0.000 vocab.pyx:168(_new_lexeme)\n",
       "      366    0.001    0.000    0.120    0.000 tokenization_utils.py:607(get_input_ids)\n",
       "     2196    0.003    0.000    0.118    0.000 inspect.py:3091(signature)\n",
       "     2196    0.003    0.000    0.115    0.000 inspect.py:2839(from_callable)\n",
       "4392/2196    0.019    0.000    0.112    0.000 inspect.py:2206(_signature_from_callable)\n",
       "        1    0.000    0.000    0.111    0.111 anchor_text.py:144(get_sample_fn)\n",
       "      366    0.010    0.000    0.107    0.000 tokenization_utils.py:474(tokenize)\n",
       "        1    0.001    0.001    0.099    0.099 anchor_text.py:75(__init__)\n",
       "       13    0.000    0.000    0.093    0.007 myUtils.py:42(get_fit_examples)\n",
       "       13    0.041    0.003    0.088    0.007 myUtils.py:44(<listcomp>)\n",
       "     3355    0.086    0.000    0.086    0.000 {built-in method topk}\n",
       "     2196    0.082    0.000    0.082    0.000 {method 'masked_fill' of 'torch._C._TensorBase' objects}\n",
       "     2196    0.009    0.000    0.076    0.000 modeling_distilbert.py:196(unshape)\n",
       "     6588    0.015    0.000    0.075    0.000 modeling_distilbert.py:192(shape)\n",
       "      366    0.017    0.000    0.065    0.000 modeling_distilbert.py:111(forward)\n",
       "     2196    0.024    0.000    0.058    0.000 inspect.py:2112(_signature_from_function)\n",
       "      965    0.004    0.000    0.058    0.000 tokenization_bert.py:220(_tokenize)\n",
       "3471/3407    0.035    0.000    0.057    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
       "    14875    0.020    0.000    0.054    0.000 numerictypes.py:359(issubdtype)\n",
       "    71138    0.051    0.000    0.051    0.000 module.py:1164(__getattr__)\n",
       "    10980    0.050    0.000    0.050    0.000 {method 'transpose' of 'torch._C._TensorBase' objects}\n",
       "    10980    0.050    0.000    0.050    0.000 {method 'view' of 'torch._C._TensorBase' objects}\n",
       "    93933    0.024    0.000    0.048    0.000 {built-in method builtins.all}\n",
       "     2196    0.047    0.000    0.047    0.000 {method 'contiguous' of 'torch._C._TensorBase' objects}\n",
       "     4912    0.006    0.000    0.047    0.000 dropout.py:57(forward)\n",
       "     1468    0.033    0.000    0.045    0.000 utils.py:14(<listcomp>)\n",
       "    31604    0.035    0.000    0.042    0.000 getlimits.py:382(__new__)\n",
       "     4912    0.022    0.000    0.041    0.000 functional.py:1152(dropout)\n",
       "     2562    0.004    0.000    0.040    0.000 functional.py:1544(gelu)\n",
       "      366    0.008    0.000    0.037    0.000 anchor_text.py:108(<listcomp>)\n",
       "      886    0.037    0.000    0.037    0.000 {built-in method tensor}\n",
       "     2196    0.003    0.000    0.036    0.000 functional.py:1650(softmax)\n",
       "     2562    0.036    0.000    0.036    0.000 {built-in method torch._C._nn.gelu}\n",
       "    11845    0.034    0.000    0.034    0.000 {method 'join' of 'str' objects}\n",
       "     2458    0.007    0.000    0.033    0.000 tokenizer.pyx:245(_split_affixes)\n",
       "      965    0.005    0.000    0.033    0.000 tokenization_bert.py:378(tokenize)\n",
       "     1469    0.009    0.000    0.033    0.000 doc.pyx:175(__init__)\n",
       "      366    0.004    0.000    0.032    0.000 tokenization_utils_base.py:2857(prepare_for_model)\n",
       "    29750    0.020    0.000    0.032    0.000 numerictypes.py:285(issubclass_)\n",
       "     2196    0.032    0.000    0.032    0.000 {method 'softmax' of 'torch._C._TensorBase' objects}\n",
       "      886    0.003    0.000    0.032    0.000 sparse.py:157(forward)\n",
       "    33498    0.031    0.000    0.031    0.000 {built-in method torch._C._get_tracing_state}\n",
       "      366    0.022    0.000    0.029    0.000 tokenization_utils.py:90(split)\n",
       "      886    0.002    0.000    0.029    0.000 functional.py:1950(embedding)\n",
       "     3355    0.027    0.000    0.029    0.000 anchor_text.py:20(exp_normalize)\n",
       "     1889    0.003    0.000    0.028    0.000 <__array_function__ internals>:2(prod)\n",
       "      886    0.027    0.000    0.027    0.000 {built-in method embedding}\n",
       "   182000    0.025    0.000    0.025    0.000 myUtils.py:44(<genexpr>)\n",
       "       35    0.000    0.000    0.024    0.001 <__array_function__ internals>:2(concatenate)\n",
       "       12    0.000    0.000    0.023    0.002 <__array_function__ internals>:2(vstack)\n",
       "       12    0.000    0.000    0.023    0.002 shape_base.py:223(vstack)\n",
       "     2196    0.010    0.000    0.023    0.000 inspect.py:1800(_signature_bound_method)\n",
       "     1889    0.003    0.000    0.022    0.000 fromnumeric.py:2912(prod)\n",
       "     4392    0.017    0.000    0.022    0.000 inspect.py:2760(__init__)\n",
       "     1469    0.002    0.000    0.021    0.000 doc.pyx:92(_get_chunker)\n",
       "     1890    0.006    0.000    0.020    0.000 fromnumeric.py:70(_wrapreduction)\n",
       "     1469    0.003    0.000    0.019    0.000 util.py:67(get_lang_class)\n",
       "     1331    0.006    0.000    0.018    0.000 tokenization_utils_base.py:1237(all_special_tokens_extended)\n",
       "     2928    0.004    0.000    0.018    0.000 tokenization_utils.py:553(convert_tokens_to_ids)\n",
       "     3195    0.017    0.000    0.018    0.000 tokenizer.pyx:406(find_suffix)\n",
       "      160    0.018    0.000    0.018    0.000 {built-in method numpy.zeros}\n",
       "     2449    0.012    0.000    0.017    0.000 lex_attrs.py:150(word_shape)\n",
       "     4392    0.010    0.000    0.017    0.000 inspect.py:2477(__init__)\n",
       "      732    0.005    0.000    0.015    0.000 file_utils.py:2268(__post_init__)\n",
       "    21551    0.009    0.000    0.015    0.000 tokenizer.pyx:220(_try_cache)\n",
       "     7230    0.015    0.000    0.015    0.000 {method 'numpy' of 'torch._C._TensorBase' objects}\n",
       "4890/2449    0.006    0.000    0.014    0.000 util.py:439(_get_attr_unless_lookup)\n",
       "     2445    0.013    0.000    0.014    0.000 tokenizer.pyx:378(find_infix)\n",
       "    44986    0.014    0.000    0.014    0.000 {built-in method builtins.issubclass}\n",
       "      965    0.001    0.000    0.014    0.000 tokenization_utils_base.py:1227(all_special_tokens)\n",
       "     7468    0.003    0.000    0.013    0.000 tokenization_utils.py:575(_convert_token_to_id_with_added_voc)\n",
       "     2262    0.013    0.000    0.013    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
       "     2449    0.008    0.000    0.013    0.000 lex_attrs.py:84(like_num)\n",
       "      366    0.001    0.000    0.012    0.000 <string>:2(__init__)\n",
       "    32577    0.007    0.000    0.012    0.000 doc.pyx:315(__iter__)\n",
       "     4912    0.012    0.000    0.012    0.000 {built-in method dropout}\n",
       "      965    0.004    0.000    0.011    0.000 tokenization_bert.py:485(_clean_text)\n",
       "     1482    0.002    0.000    0.011    0.000 <__array_function__ internals>:2(where)\n",
       "     1469    0.003    0.000    0.011    0.000 catalogue.py:85(get)\n",
       "     2196    0.003    0.000    0.011    0.000 inspect.py:2853(replace)\n",
       "      366    0.010    0.000    0.010    0.000 {built-in method ones}\n",
       "     2196    0.010    0.000    0.010    0.000 {method 'expand_as' of 'torch._C._TensorBase' objects}\n",
       "      366    0.002    0.000    0.010    0.000 tokenization_utils_base.py:2658(pad)\n",
       "    24931    0.010    0.000    0.010    0.000 {built-in method builtins.getattr}\n",
       "     7468    0.006    0.000    0.010    0.000 tokenization_bert.py:234(_convert_token_to_id)\n",
       "     1468    0.010    0.000    0.010    0.000 2852908370.py:4(<listcomp>)\n",
       "        1    0.001    0.001    0.009    0.009 language.py:424(__call__)\n",
       "      732    0.002    0.000    0.009    0.000 tokenization_utils_base.py:190(__init__)\n",
       "     1351    0.004    0.000    0.009    0.000 tokenization_bert.py:426(_run_split_on_punc)\n",
       "     25/4    0.000    0.000    0.009    0.002 model.py:161(__call__)\n",
       "     1331    0.004    0.000    0.008    0.000 tokenization_utils_base.py:1210(special_tokens_map_extended)\n",
       "    42347    0.007    0.000    0.008    0.000 {built-in method builtins.isinstance}\n",
       "     2449    0.006    0.000    0.008    0.000 lex_attrs.py:124(like_url)\n",
       "     2196    0.005    0.000    0.008    0.000 inspect.py:493(unwrap)\n",
       "     3204    0.007    0.000    0.008    0.000 tokenizer.pyx:392(find_prefix)\n",
       "     1351    0.005    0.000    0.007    0.000 tokenization_bert.py:507(tokenize)\n",
       "     5490    0.005    0.000    0.007    0.000 enum.py:289(__call__)\n",
       "     1469    0.003    0.000    0.007    0.000 catalogue.py:153(_get)\n",
       "      732    0.002    0.000    0.007    0.000 __init__.py:981(__init__)\n",
       "        4    0.000    0.000    0.007    0.002 model.py:130(predict)\n",
       "    52350    0.007    0.000    0.007    0.000 {method 'append' of 'list' objects}\n",
       "    31026    0.004    0.000    0.007    0.000 doc.pyx:673(__pyx_fuse_0push_back)\n",
       "     1098    0.004    0.000    0.007    0.000 tokenization_utils_base.py:2198(_get_padding_truncation_strategies)\n",
       "     4890    0.004    0.000    0.007    0.000 lookups.py:237(__contains__)\n",
       "      732    0.001    0.000    0.007    0.000 tokenization_bert.py:247(build_inputs_with_special_tokens)\n",
       "    20072    0.007    0.000    0.007    0.000 {built-in method torch._C._has_torch_function_variadic}\n",
       "      154    0.006    0.000    0.006    0.000 {method 'to' of 'torch._C._TensorBase' objects}\n",
       "     5740    0.006    0.000    0.006    0.000 {method 'size' of 'torch._C._TensorBase' objects}\n",
       "     15/3    0.000    0.000    0.006    0.002 feed_forward.py:43(begin_update)\n",
       "     5066    0.004    0.000    0.006    0.000 _VF.py:25(__getattr__)\n",
       "      839    0.006    0.000    0.006    0.000 {built-in method numpy.array}\n",
       "    39102    0.006    0.000    0.006    0.000 {built-in method builtins.len}\n",
       " 1464/732    0.002    0.000    0.005    0.000 file_utils.py:2324(__getitem__)\n",
       "     5124    0.003    0.000    0.005    0.000 __init__.py:31(__get__)\n",
       "        2    0.000    0.000    0.005    0.003 api.py:293(begin_update)\n",
       "     1469    0.003    0.000    0.005    0.000 catalogue.py:49(__contains__)\n",
       "      732    0.002    0.000    0.005    0.000 _collections_abc.py:824(update)\n",
       "    31107    0.005    0.000    0.005    0.000 token.pxd:19(cinit)\n",
       "      732    0.003    0.000    0.005    0.000 dataclasses.py:1022(fields)\n",
       "     2449    0.003    0.000    0.005    0.000 lex_attrs.py:34(is_ascii)\n",
       "      154    0.000    0.000    0.005    0.000 activation.py:1296(forward)\n",
       "     2562    0.004    0.000    0.004    0.000 file_utils.py:2331(__setattr__)\n",
       "      965    0.002    0.000    0.004    0.000 tokenization_bert.py:448(_tokenize_chinese_chars)\n",
       "       11    0.000    0.000    0.004    0.000 layernorm.py:60(begin_update)\n",
       "      366    0.000    0.000    0.004    0.000 tokenization_utils.py:451(num_special_tokens_to_add)\n",
       "      732    0.002    0.000    0.004    0.000 file_utils.py:2343(to_tuple)\n",
       "      154    0.000    0.000    0.004    0.000 functional.py:1748(log_softmax)\n",
       "     1098    0.001    0.000    0.004    0.000 tokenization_utils_base.py:1129(cls_token_id)\n",
       "     2449    0.003    0.000    0.004    0.000 lex_attrs.py:27(is_punct)\n",
       "    17698    0.004    0.000    0.004    0.000 {method 'startswith' of 'str' objects}\n",
       "     6588    0.003    0.000    0.004    0.000 inspect.py:2809(<genexpr>)\n",
       "     3325    0.003    0.000    0.004    0.000 tokenization_utils.py:273(_is_control)\n",
       "      154    0.004    0.000    0.004    0.000 {method 'log_softmax' of 'torch._C._TensorBase' objects}\n",
       "      154    0.004    0.000    0.004    0.000 {built-in method argmax}\n",
       "     1098    0.001    0.000    0.004    0.000 tokenization_utils_base.py:1102(sep_token_id)\n",
       "      321    0.000    0.000    0.004    0.000 {method 'sum' of 'numpy.ndarray' objects}\n",
       "     3281    0.002    0.000    0.003    0.000 tokenization_bert.py:108(whitespace_tokenize)\n",
       "      154    0.003    0.000    0.003    0.000 {built-in method zeros}\n",
       "2196/1464    0.003    0.000    0.003    0.000 configuration_utils.py:242(__getattribute__)\n",
       "     2449    0.002    0.000    0.003    0.000 lex_attrs.py:120(like_email)\n",
       "      160    0.001    0.000    0.003    0.000 anchor_base.py:45(dlow_bernoulli)\n",
       "     4890    0.002    0.000    0.003    0.000 lex_attrs.py:177(lower)\n",
       "        8    0.000    0.000    0.003    0.000 resnet.py:28(begin_update)\n",
       "     2879    0.002    0.000    0.003    0.000 tokenization_utils.py:285(_is_punctuation)\n",
       "        1    0.000    0.000    0.003    0.003 pipes.pyx:397(__call__)\n",
       "      322    0.000    0.000    0.003    0.000 _methods.py:45(_sum)\n",
       "      366    0.003    0.000    0.003    0.000 tokenization_utils.py:237(cut_text)\n",
       "        1    0.000    0.000    0.003    0.003 pipes.pyx:409(predict)\n",
       "      366    0.001    0.000    0.003    0.000 tokenization_bert.py:300(create_token_type_ids_from_sequences)\n",
       "    30341    0.003    0.000    0.003    0.000 {method 'items' of 'dict' objects}\n",
       "        2    0.000    0.000    0.003    0.002 api.py:308(predict)\n",
       "       18    0.003    0.000    0.003    0.000 ops.pyx:514(gemm)\n",
       "     4898    0.003    0.000    0.003    0.000 {spacy.strings.get_string_id}\n",
       "     1481    0.003    0.000    0.003    0.000 {method 'copy' of 'numpy.ndarray' objects}\n",
       "      6/1    0.000    0.000    0.003    0.003 feed_forward.py:38(predict)\n",
       "        3    0.000    0.000    0.003    0.001 api.py:370(uniqued_fwd)\n",
       "     9824    0.003    0.000    0.003    0.000 {built-in method torch._C._has_torch_function_unary}\n",
       "        1    0.000    0.000    0.003    0.003 anchor_base.py:431(<listcomp>)\n",
       "    13230    0.003    0.000    0.003    0.000 {built-in method unicodedata.category}\n",
       "       13    0.001    0.000    0.003    0.000 anchor_base.py:243(get_anchor_from_tuple)\n",
       "       11    0.000    0.000    0.003    0.000 maxout.py:72(begin_update)\n",
       "      790    0.003    0.000    0.003    0.000 doc.pyx:779(_realloc)\n",
       "      315    0.002    0.000    0.003    0.000 anchor_base.py:26(kl_bernoulli)\n",
       "      366    0.001    0.000    0.003    0.000 tokenization_utils_base.py:1141(mask_token_id)\n",
       "     1464    0.001    0.000    0.002    0.000 file_utils.py:2347(<genexpr>)\n",
       "     2938    0.002    0.000    0.002    0.000 catalogue.py:130(get_entry_point)\n",
       "    23569    0.002    0.000    0.002    0.000 {built-in method builtins.ord}\n",
       "     5876    0.002    0.000    0.002    0.000 catalogue.py:160(<genexpr>)\n",
       "     5124    0.002    0.000    0.002    0.000 {built-in method torch._C._get_cudnn_enabled}\n",
       "    16489    0.002    0.000    0.002    0.000 {method 'isalpha' of 'str' objects}\n",
       "     1469    0.002    0.000    0.002    0.000 vocab.pyx:78(__get__)\n",
       "     5490    0.002    0.000    0.002    0.000 enum.py:586(__new__)\n",
       "     4392    0.002    0.000    0.002    0.000 inspect.py:158(isfunction)\n",
       "     2449    0.002    0.000    0.002    0.000 lex_attrs.py:112(is_currency)\n",
       "     2967    0.001    0.000    0.002    0.000 {built-in method builtins.hasattr}\n",
       "     3355    0.002    0.000    0.002    0.000 serialize.py:29(_numba_unpickle)\n",
       "     3325    0.001    0.000    0.002    0.000 tokenization_utils.py:261(_is_whitespace)\n",
       "     2449    0.001    0.000    0.002    0.000 lex_attrs.py:213(is_stop)\n",
       "      366    0.001    0.000    0.002    0.000 grad_mode.py:124(__enter__)\n",
       "     2196    0.002    0.000    0.002    0.000 {built-in method math.sqrt}\n",
       "      155    0.001    0.000    0.002    0.000 anchor_base.py:33(dup_bernoulli)\n",
       "     9700    0.002    0.000    0.002    0.000 {method 'lower' of 'str' objects}\n",
       "     2196    0.001    0.000    0.002    0.000 inspect.py:513(_is_wrapper)\n",
       "    16027    0.002    0.000    0.002    0.000 {method 'isupper' of 'str' objects}\n",
       "      732    0.001    0.000    0.002    0.000 grad_mode.py:213(__init__)\n",
       "     2449    0.002    0.000    0.002    0.000 {method 'match' of 're.Pattern' objects}\n",
       "      366    0.001    0.000    0.002    0.000 grad_mode.py:128(__exit__)\n",
       "      366    0.001    0.000    0.002    0.000 file_utils.py:2161(is_tensor)\n",
       "     2449    0.001    0.000    0.002    0.000 lex_attrs.py:197(is_lower)\n",
       "     1331    0.002    0.000    0.002    0.000 {built-in method fromkeys}\n",
       "     2196    0.002    0.000    0.002    0.000 {method 'values' of 'mappingproxy' objects}\n",
       "        4    0.000    0.000    0.002    0.000 resnet.py:17(predict)\n",
       "       15    0.000    0.000    0.002    0.000 layernorm.py:104(_get_moments)\n",
       "     2449    0.001    0.000    0.001    0.000 lex_attrs.py:189(is_alpha)\n",
       "      366    0.001    0.000    0.001    0.000 grad_mode.py:119(__init__)\n",
       "        4    0.000    0.000    0.001    0.000 layernorm.py:50(predict)\n",
       "      154    0.000    0.000    0.001    0.000 rnn.py:228(check_forward_args)\n",
       "      732    0.001    0.000    0.001    0.000 file_utils.py:2337(__setitem__)\n",
       "     2449    0.001    0.000    0.001    0.000 lex_attrs.py:193(is_digit)\n",
       "     1890    0.001    0.000    0.001    0.000 fromnumeric.py:71(<dictcomp>)\n",
       "     2449    0.001    0.000    0.001    0.000 lex_attrs.py:205(is_title)\n",
       "     2449    0.001    0.000    0.001    0.000 lex_attrs.py:201(is_space)\n",
       "     2361    0.001    0.000    0.001    0.000 {method 'count' of 'str' objects}\n",
       "     2449    0.001    0.000    0.001    0.000 lex_attrs.py:209(is_upper)\n",
       "     2449    0.001    0.000    0.001    0.000 lex_attrs.py:185(suffix)\n",
       "      154    0.001    0.000    0.001    0.000 {built-in method transpose}\n",
       "     6588    0.001    0.000    0.001    0.000 inspect.py:2527(name)\n",
       "      965    0.001    0.000    0.001    0.000 tokenization_utils_base.py:1234(<listcomp>)\n",
       "     2449    0.001    0.000    0.001    0.000 lex_attrs.py:55(is_bracket)\n",
       "       39    0.000    0.000    0.001    0.000 anchor_base.py:10(matrix_subset)\n",
       "       13    0.001    0.000    0.001    0.000 {built-in method _pickle.dump}\n",
       "     1098    0.001    0.000    0.001    0.000 types.py:171(__get__)\n",
       "      733    0.000    0.000    0.001    0.000 abc.py:96(__instancecheck__)\n",
       "      9/3    0.000    0.000    0.001    0.000 api.py:161(begin_update)\n",
       "     3325    0.001    0.000    0.001    0.000 tokenization_bert.py:461(_is_chinese_char)\n",
       "     3294    0.001    0.000    0.001    0.000 dataclasses.py:1037(<genexpr>)\n",
       "     2449    0.001    0.000    0.001    0.000 lex_attrs.py:60(is_quote)\n",
       "     4392    0.001    0.000    0.001    0.000 {method 'isidentifier' of 'str' objects}\n",
       "      366    0.000    0.000    0.001    0.000 configuration_utils.py:370(use_return_dict)\n",
       "      9/3    0.000    0.000    0.001    0.000 api.py:163(<listcomp>)\n",
       "     18/6    0.000    0.000    0.001    0.000 api.py:255(wrap)\n",
       "      366    0.001    0.000    0.001    0.000 container.py:210(__iter__)\n",
       "     6588    0.001    0.000    0.001    0.000 inspect.py:2539(kind)\n",
       "      296    0.001    0.000    0.001    0.000 _dtype.py:34(__str__)\n",
       "        4    0.000    0.000    0.001    0.000 maxout.py:64(predict)\n",
       "     1351    0.001    0.000    0.001    0.000 tokenization_bert.py:446(<listcomp>)\n",
       "     1401    0.001    0.000    0.001    0.000 {built-in method builtins.max}\n",
       "     4898    0.001    0.000    0.001    0.000 {method 'replace' of 'str' objects}\n",
       "     5366    0.001    0.000    0.001    0.000 {method 'isdigit' of 'str' objects}\n",
       "     2449    0.001    0.000    0.001    0.000 lex_attrs.py:85(is_left_punct)\n",
       "     4392    0.001    0.000    0.001    0.000 {built-in method builtins.callable}\n",
       "     1830    0.001    0.000    0.001    0.000 file_utils.py:2278(<genexpr>)\n",
       "       15    0.000    0.000    0.001    0.000 {method 'var' of 'numpy.ndarray' objects}\n",
       "       12    0.000    0.000    0.001    0.000 hash_embed.py:56(begin_update)\n",
       "       15    0.000    0.000    0.001    0.000 _methods.py:194(_var)\n",
       "      733    0.001    0.000    0.001    0.000 {built-in method _abc._abc_instancecheck}\n",
       "     2449    0.001    0.000    0.001    0.000 {method 'islower' of 'str' objects}\n",
       "     3281    0.001    0.000    0.001    0.000 {method 'split' of 'str' objects}\n",
       "     2449    0.001    0.000    0.001    0.000 lex_attrs.py:107(is_right_punct)\n",
       "     3918    0.001    0.000    0.001    0.000 __init__.py:17(_return_en)\n",
       "       14    0.000    0.000    0.001    0.000 {built-in method builtins.print}\n",
       "     2449    0.001    0.000    0.001    0.000 lex_attrs.py:181(prefix)\n",
       "     1148    0.001    0.000    0.001    0.000 {built-in method builtins.min}\n",
       "      366    0.000    0.000    0.001    0.000 tokenization_utils.py:491(<genexpr>)\n",
       "       20    0.000    0.000    0.001    0.000 {method 'mean' of 'numpy.ndarray' objects}\n",
       "      366    0.001    0.000    0.001    0.000 tokenization_utils_base.py:3089(_pad)\n",
       "     2196    0.001    0.000    0.001    0.000 {built-in method sys.getrecursionlimit}\n",
       "      395    0.001    0.000    0.001    0.000 {method 'nonzero' of 'numpy.ndarray' objects}\n",
       "       14    0.000    0.000    0.001    0.000 <__array_function__ internals>:2(hstack)\n",
       "       20    0.000    0.000    0.001    0.000 _methods.py:161(_mean)\n",
       "     4392    0.001    0.000    0.001    0.000 inspect.py:2845(parameters)\n",
       "       28    0.000    0.000    0.001    0.000 iostream.py:500(write)\n",
       "      154    0.000    0.000    0.001    0.000 anchor_text.py:184(<listcomp>)\n",
       "     2352    0.001    0.000    0.001    0.000 {method 'endswith' of 'str' objects}\n",
       "      965    0.001    0.000    0.001    0.000 {method 'union' of 'set' objects}\n",
       "     6390    0.001    0.000    0.001    0.000 tokenizer.pyx:94(__get__)\n",
       "     2562    0.001    0.000    0.001    0.000 file_utils.py:2274(<genexpr>)\n",
       "       14    0.000    0.000    0.001    0.000 shape_base.py:286(hstack)\n",
       "     5649    0.001    0.000    0.001    0.000 tokenizer.pyx:70(__get__)\n",
       "     6408    0.001    0.000    0.001    0.000 tokenizer.pyx:86(__get__)\n",
       "     1098    0.000    0.000    0.001    0.000 tokenization_utils_base.py:229(__getitem__)\n",
       "     3667    0.001    0.000    0.001    0.000 {method 'extend' of 'list' objects}\n",
       "     1098    0.001    0.000    0.001    0.000 {built-in method torch._C.is_grad_enabled}\n",
       "     2196    0.001    0.000    0.001    0.000 {built-in method builtins.id}\n",
       "     1889    0.001    0.000    0.001    0.000 fromnumeric.py:2907(_prod_dispatcher)\n",
       "     3294    0.001    0.000    0.001    0.000 {method 'keys' of 'collections.OrderedDict' objects}\n",
       "     4048    0.001    0.000    0.001    0.000 {method 'items' of 'collections.OrderedDict' objects}\n",
       "       86    0.000    0.000    0.001    0.000 describe.py:36(__get__)\n",
       "     3163    0.001    0.000    0.001    0.000 {method 'lstrip' of 'str' objects}\n",
       "      154    0.000    0.000    0.001    0.000 rnn.py:198(check_input)\n",
       "     4890    0.000    0.000    0.000    0.000 tokenizer.pyx:78(__get__)\n",
       "     4890    0.000    0.000    0.000    0.000 tokenizer.pyx:102(__get__)\n",
       "      732    0.000    0.000    0.000    0.000 file_utils.py:2326(<dictcomp>)\n",
       "      154    0.000    0.000    0.000    0.000 3768568908.py:5(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 _ml.py:178(begin_update)\n",
       "     2449    0.000    0.000    0.000    0.000 {method 'istitle' of 'str' objects}\n",
       "     3281    0.000    0.000    0.000    0.000 {method 'strip' of 'str' objects}\n",
       "     1482    0.000    0.000    0.000    0.000 multiarray.py:321(where)\n",
       "     2458    0.000    0.000    0.000    0.000 tokenizer.pyx:357(_save_cached)\n",
       "     2449    0.000    0.000    0.000    0.000 {method 'isspace' of 'str' objects}\n",
       "     1464    0.000    0.000    0.000    0.000 __init__.py:1011(__setitem__)\n",
       "     1469    0.000    0.000    0.000    0.000 catalogue.py:144(check_exists)\n",
       "       30    0.000    0.000    0.000    0.000 iostream.py:206(schedule)\n",
       "     1098    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1007(cls_token)\n",
       "      366    0.000    0.000    0.000    0.000 file_utils.py:2153(is_torch_fx_proxy)\n",
       "      292    0.000    0.000    0.000    0.000 {method 'reshape' of 'numpy.ndarray' objects}\n",
       "     2196    0.000    0.000    0.000    0.000 inspect.py:2531(default)\n",
       "     2063    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
       "      366    0.000    0.000    0.000    0.000 tokenization_utils_base.py:3312(_eventual_warn_about_too_long_sequence)\n",
       "     1098    0.000    0.000    0.000    0.000 enum.py:689(value)\n",
       "     1098    0.000    0.000    0.000    0.000 tokenization_utils_base.py:986(sep_token)\n",
       "      366    0.000    0.000    0.000    0.000 modeling_utils.py:304(get_head_mask)\n",
       "     2196    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n",
       "      732    0.000    0.000    0.000    0.000 tokenization_bert.py:209(do_lower_case)\n",
       "        3    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(unique)\n",
       "       86    0.000    0.000    0.000    0.000 mem.py:31(__getitem__)\n",
       "        3    0.000    0.000    0.000    0.000 arraysetops.py:138(unique)\n",
       "        1    0.000    0.000    0.000    0.000 anchor_base.py:118(make_tuples)\n",
       "      366    0.000    0.000    0.000    0.000 __init__.py:1013(__iter__)\n",
       "        3    0.000    0.000    0.000    0.000 arraysetops.py:310(_unique1d)\n",
       "      367    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1018(mask_token)\n",
       "      732    0.000    0.000    0.000    0.000 {built-in method torch._C._set_grad_enabled}\n",
       "      154    0.000    0.000    0.000    0.000 rnn.py:223(check_hidden_size)\n",
       "      154    0.000    0.000    0.000    0.000 rnn.py:209(get_expected_hidden_size)\n",
       "       11    0.000    0.000    0.000    0.000 layernorm.py:92(_begin_update_scale_shift)\n",
       "      148    0.000    0.000    0.000    0.000 {method 'update' of 'set' objects}\n",
       "      732    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method io.open}\n",
       "       15    0.000    0.000    0.000    0.000 ops.pyx:603(maxout)\n",
       "       30    0.000    0.000    0.000    0.000 socket.py:480(send)\n",
       "       15    0.000    0.000    0.000    0.000 layernorm.py:128(_forward)\n",
       "       35    0.000    0.000    0.000    0.000 _methods.py:65(_count_reduce_items)\n",
       "       42    0.000    0.000    0.000    0.000 ops.pyx:490(allocate)\n",
       "        1    0.000    0.000    0.000    0.000 check.py:142(checked_function)\n",
       "      823    0.000    0.000    0.000    0.000 {method 'rstrip' of 'str' objects}\n",
       "      732    0.000    0.000    0.000    0.000 tokenization_utils_base.py:645(convert_to_tensors)\n",
       "        1    0.000    0.000    0.000    0.000 softmax.py:17(predict)\n",
       "        8    0.000    0.000    0.000    0.000 convolution.py:32(begin_update)\n",
       "       24    0.000    0.000    0.000    0.000 ops.pyx:473(asarray)\n",
       "      155    0.000    0.000    0.000    0.000 describe.py:22(__get__)\n",
       "      366    0.000    0.000    0.000    0.000 _jit_internal.py:957(is_scripting)\n",
       "      366    0.000    0.000    0.000    0.000 {method 'values' of 'collections.OrderedDict' objects}\n",
       "       12    0.000    0.000    0.000    0.000 ops.pyx:660(seq2col)\n",
       "      111    0.000    0.000    0.000    0.000 _asarray.py:110(asanyarray)\n",
       "        3    0.000    0.000    0.000    0.000 feature_extracter.py:12(begin_update)\n",
       "        3    0.000    0.000    0.000    0.000 feature_extracter.py:14(<listcomp>)\n",
       "      366    0.000    0.000    0.000    0.000 tokenization_utils.py:812(prepare_for_tokenization)\n",
       "        3    0.000    0.000    0.000    0.000 feature_extracter.py:17(_get_feats)\n",
       "       12    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(atleast_2d)\n",
       "       14    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(atleast_1d)\n",
       "        6    0.000    0.000    0.000    0.000 ops.pyx:138(flatten)\n",
       "      154    0.000    0.000    0.000    0.000 {method 'dim' of 'torch._C._TensorBase' objects}\n",
       "       75    0.000    0.000    0.000    0.000 stringsource:657(memoryview_cwrapper)\n",
       "       30    0.000    0.000    0.000    0.000 threading.py:1071(is_alive)\n",
       "        2    0.000    0.000    0.000    0.000 _ml.py:213(_add_padding)\n",
       "        1    0.000    0.000    0.000    0.000 pipes.pyx:427(set_annotations)\n",
       "       13    0.000    0.000    0.000    0.000 myUtils.py:6(__init__)\n",
       "      366    0.000    0.000    0.000    0.000 file_utils.py:415(is_torch_fx_available)\n",
       "      154    0.000    0.000    0.000    0.000 rnn.py:234(permute_hidden)\n",
       "      366    0.000    0.000    0.000    0.000 file_utils.py:334(is_torch_available)\n",
       "       13    0.000    0.000    0.000    0.000 {method 'flush' of '_io.BufferedWriter' objects}\n",
       "        3    0.000    0.000    0.000    0.000 {method 'to_array' of 'spacy.tokens.doc.Doc' objects}\n",
       "        4    0.000    0.000    0.000    0.000 convolution.py:29(predict)\n",
       "       12    0.000    0.000    0.000    0.000 shape_base.py:82(atleast_2d)\n",
       "        3    0.000    0.000    0.000    0.000 doc.pyx:701(to_array (wrapper))\n",
       "        3    0.000    0.000    0.000    0.000 doc.pyx:701(to_array)\n",
       "       14    0.000    0.000    0.000    0.000 shape_base.py:24(atleast_1d)\n",
       "       18    0.000    0.000    0.000    0.000 _asarray.py:183(ascontiguousarray)\n",
       "       12    0.000    0.000    0.000    0.000 ops.pyx:715(hash)\n",
       "        1    0.000    0.000    0.000    0.000 ops.pyx:278(softmax)\n",
       "        1    0.000    0.000    0.000    0.000 ops.pyx:536(affine)\n",
       "       36    0.000    0.000    0.000    0.000 stringsource:999(memoryview_fromslice)\n",
       "       86    0.000    0.000    0.000    0.000 mem.py:28(__contains__)\n",
       "        1    0.000    0.000    0.000    0.000 language.py:462(make_doc)\n",
       "        3    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(cumsum)\n",
       "       28    0.000    0.000    0.000    0.000 iostream.py:418(_is_master_process)\n",
       "        3    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(diff)\n",
       "        6    0.000    0.000    0.000    0.000 fromnumeric.py:52(_wrapfunc)\n",
       "       26    0.000    0.000    0.000    0.000 shape_base.py:219(_vhstack_dispatcher)\n",
       "       28    0.000    0.000    0.000    0.000 iostream.py:437(_schedule_flush)\n",
       "        2    0.000    0.000    0.000    0.000 _ml.py:792(flatten)\n",
       "      111    0.000    0.000    0.000    0.000 stringsource:345(__cinit__)\n",
       "        3    0.000    0.000    0.000    0.000 fromnumeric.py:2446(cumsum)\n",
       "        3    0.000    0.000    0.000    0.000 function_base.py:1153(diff)\n",
       "       13    0.000    0.000    0.000    0.000 anchor_base.py:249(<lambda>)\n",
       "        8    0.000    0.000    0.000    0.000 {function Table.__contains__ at 0x7fe2394a2a60}\n",
       "       30    0.000    0.000    0.000    0.000 threading.py:1017(_wait_for_tstate_lock)\n",
       "        6    0.000    0.000    0.000    0.000 _asarray.py:23(asarray)\n",
       "        8    0.000    0.000    0.000    0.000 lookups.py:218(__getitem__)\n",
       "       68    0.000    0.000    0.000    0.000 doc.pyx:673(__pyx_fuse_1push_back)\n",
       "        9    0.000    0.000    0.000    0.000 api.py:239(split_backward)\n",
       "       13    0.000    0.000    0.000    0.000 myUtils.py:47(get_test_cov)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'cumsum' of 'numpy.ndarray' objects}\n",
       "        4    0.000    0.000    0.000    0.000 ops.pyx:157(unflatten)\n",
       "        1    0.000    0.000    0.000    0.000 anchor_text.py:147(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(amax)\n",
       "       26    0.000    0.000    0.000    0.000 shape_base.py:208(_arrays_for_stack_dispatcher)\n",
       "       28    0.000    0.000    0.000    0.000 {built-in method posix.getpid}\n",
       "       13    0.000    0.000    0.000    0.000 anchor_text.py:208(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2617(amax)\n",
       "        1    0.000    0.000    0.000    0.000 anchor_base.py:129(<listcomp>)\n",
       "        9    0.000    0.000    0.000    0.000 api.py:246(<listcomp>)\n",
       "       13    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}\n",
       "        3    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(nonzero)\n",
       "       30    0.000    0.000    0.000    0.000 iostream.py:96(_event_pipe)\n",
       "       38    0.000    0.000    0.000    0.000 {built-in method numpy.core._multiarray_umath.normalize_axis_index}\n",
       "       30    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}\n",
       "      111    0.000    0.000    0.000    0.000 stringsource:372(__dealloc__)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'argsort' of 'numpy.ndarray' objects}\n",
       "       26    0.000    0.000    0.000    0.000 anchor_explanation.py:12(names)\n",
       "       13    0.000    0.000    0.000    0.000 anchor_explanation.py:54(coverage)\n",
       "       75    0.000    0.000    0.000    0.000 stringsource:663(memoryview_check)\n",
       "        1    0.000    0.000    0.000    0.000 doc.pyx:1022(extend_tensor)\n",
       "       22    0.000    0.000    0.000    0.000 ops.pyx:122(dropout)\n",
       "        1    0.000    0.000    0.000    0.000 anchor_base.py:171(get_sample_fns)\n",
       "        3    0.000    0.000    0.000    0.000 fromnumeric.py:1827(nonzero)\n",
       "       13    0.000    0.000    0.000    0.000 anchor_explanation.py:7(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 anchor_base.py:232(get_initial_statistics)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method _pickle.loads}\n",
       "        1    0.000    0.000    0.000    0.000 check.py:59(has_shape_inner)\n",
       "        1    0.000    0.000    0.000    0.000 nonproj.pyx:133(deprojectivize (wrapper))\n",
       "        8    0.000    0.000    0.000    0.000 {function Table.__getitem__ at 0x7fe2394a2940}\n",
       "        1    0.000    0.000    0.000    0.000 anchor_text.py:148(<listcomp>)\n",
       "        6    0.000    0.000    0.000    0.000 {built-in method numpy.empty}\n",
       "       13    0.000    0.000    0.000    0.000 __init__.py:145(_DType_reduce)\n",
       "        1    0.000    0.000    0.000    0.000 nonproj.pyx:133(deprojectivize)\n",
       "       30    0.000    0.000    0.000    0.000 threading.py:513(is_set)\n",
       "       30    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
       "        2    0.000    0.000    0.000    0.000 util.py:143(copy_array)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'sort' of 'list' objects}\n",
       "       13    0.000    0.000    0.000    0.000 anchor_explanation.py:38(precision)\n",
       "       35    0.000    0.000    0.000    0.000 multiarray.py:143(concatenate)\n",
       "        9    0.000    0.000    0.000    0.000 api.py:166(<listcomp>)\n",
       "       18    0.000    0.000    0.000    0.000 api.py:250(sink_return)\n",
       "       54    0.000    0.000    0.000    0.000 stringsource:518(__getbuffer__)\n",
       "       36    0.000    0.000    0.000    0.000 stringsource:976(__dealloc__)\n",
       "       13    0.000    0.000    0.000    0.000 anchor_base.py:320(<lambda>)\n",
       "       13    0.000    0.000    0.000    0.000 anchor_text.py:209(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 arc_eager.pyx:553(finalize_doc)\n",
       "        8    0.000    0.000    0.000    0.000 convolution.py:37(_get_finish_update)\n",
       "        2    0.000    0.000    0.000    0.000 doc.pyx:1225(set_children_from_heads)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'flatten' of 'numpy.ndarray' objects}\n",
       "       36    0.000    0.000    0.000    0.000 stringsource:559(__get__)\n",
       "       14    0.000    0.000    0.000    0.000 shape_base.py:20(_atleast_1d_dispatcher)\n",
       "        1    0.000    0.000    0.000    0.000 anchor_base.py:355(<listcomp>)\n",
       "       13    0.000    0.000    0.000    0.000 myUtils.py:80(<lambda>)\n",
       "       22    0.000    0.000    0.000    0.000 ops.pyx:124(lambda1)\n",
       "        3    0.000    0.000    0.000    0.000 arraysetops.py:125(_unpack_tuple)\n",
       "       12    0.000    0.000    0.000    0.000 shape_base.py:78(_atleast_2d_dispatcher)\n",
       "        7    0.000    0.000    0.000    0.000 util.py:14(<lambda>)\n",
       "       12    0.000    0.000    0.000    0.000 ops.pyx:209(get_dropout_mask)\n",
       "        2    0.000    0.000    0.000    0.000 api.py:309(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 _ml.py:795(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 doc.pyx:1255(_set_lr_kids_and_edges)\n",
       "        1    0.000    0.000    0.000    0.000 pipes.pyx:1106(__get__)\n",
       "        2    0.000    0.000    0.000    0.000 api.py:294(<listcomp>)\n",
       "        1    0.000    0.000    0.000    0.000 pipes.pyx:411(genexpr)\n",
       "        1    0.000    0.000    0.000    0.000 pipes.pyx:85(require_model)\n",
       "        2    0.000    0.000    0.000    0.000 util.py:340(get_cuda_stream)\n",
       "        3    0.000    0.000    0.000    0.000 function_base.py:1149(_diff_dispatcher)\n",
       "        3    0.000    0.000    0.000    0.000 fromnumeric.py:2442(_cumsum_dispatcher)\n",
       "        3    0.000    0.000    0.000    0.000 arraysetops.py:133(_unique_dispatcher)\n",
       "        3    0.000    0.000    0.000    0.000 fromnumeric.py:1823(_nonzero_dispatcher)\n",
       "        5    0.000    0.000    0.000    0.000 doc.pyx:328(__len__)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
       "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2612(_amax_dispatcher)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_utils = TextUtils(anchor_examples, test, explainer, predict_sentences, ignored,f\"profile.pickle\")\n",
    "set_seed()\n",
    "%prun -s cumtime -T profile.txt my_utils.compute_explanations(list(range(len(anchor_examples))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1bd996-c9ac-4704-b77e-472c29d051bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### my_utils = TextUtils(anchor_examples, test, explainer, predict_sentences, ignored,f\"profile.pickle\")\n",
    "%lprun -s -m modified_anchor.anchor_text -m modified_anchor.anchor_base -m myUtils -T profile.txt  my_utils.compute_explanations(list(range(len(anchor_examples))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891a9e98-7566-43f6-8eb1-1ed446033ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5073ae0a-f2d7-4f9e-95c1-3353fef4304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "import spacy\n",
    "from optimized_anchor import anchor_text, anchor_base\n",
    "import pickle\n",
    "import myUtils\n",
    "from myUtils import *\n",
    "from dataset.dataset_loader import *\n",
    "import datetime\n",
    "%load_ext line_profiler\n",
    "\n",
    "SEED = 84\n",
    "torch.manual_seed(SEED)\n",
    "warnings.simplefilter(\"ignore\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2444c8e5-2a8b-4797-9248-46565487f5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in training samples: 6407\n",
      "Number of tokens in training labels: 2\n"
     ]
    }
   ],
   "source": [
    "# can be sentiment/spam/offensive\n",
    "dataset_name = 'sentiment'\n",
    "text_parser, label_parser, ds_train, ds_val = get_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6358237f-3408-43ff-b1c7-b0c880396138",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, train_labels, test, test_labels, anchor_examples = preprocess_examples(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1da1181f-98b5-4f15-9896-d4a99cf231be",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50cb3bc3-5a28-49a4-af65-914fd963edb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignored = get_ignored(anchor_examples)\n",
    "normal_occurences = get_occurences(anchor_examples)\n",
    "anchor_base.AnchorBaseBeam.best_group = BestGroup('check', normal_occurences, filter_anchors = False, desired_optimize = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f52d42-f62d-4dab-a6a4-2c2e458b7433",
   "metadata": {},
   "source": [
    "## notice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec7977c8-6e39-4b91-94f0-3a934abc63f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignored = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09e3b250-34e7-4232-afdb-76f1c3ee2221",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_examples = anchor_examples[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "042a3661-59b9-4f72-b84b-8d640ab46247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 10 22:17:35 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.54       Driver Version: 510.54       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 6000     Off  | 00000000:AF:00.0 Off |                  Off |\n",
      "| 33%   37C    P8    17W / 260W |      8MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2479      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c976b285-caee-439e-870d-a43850a4551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize = True\n",
    "anchor_text.AnchorText.set_optimize(optimize)\n",
    "explainer = anchor_text.AnchorText(nlp, ['positive', 'negative'], use_unk_distribution=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3dbad78-2676-47ab-97c8-2ffbfe09e81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model = torch.jit.load(f'models/tinybert/{dataset_name}/traced.pt').to(device)\n",
    "model = model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D', use_fast = False)\n",
    "myUtils.model = model\n",
    "myUtils.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "beffd1f6-fc2a-467b-b7fb-1c7eb2980739",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = explainer.tg.bert_tokenizer([\"i [MASK] to [MASK] party yesterday\"], \n",
    "          add_special_tokens=True, return_tensors=\"pt\", padding=True)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "930db238-c1a9-46f1-9612-81a20067c443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 1045,  103, 2000,  103, 2283, 7483,  102]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "36dafa42-8510-47f1-a804-b2041bca1252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 4])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tokenized[0] == explainer.tg.bert_tokenizer.mask_token_id).nonzero().squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f25c2a0-428e-412b-b0f5-97a7e03294ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_seperate = [explainer.tg.bert_tokenizer.encode(\"i went to a party yesterday\", \n",
    "          add_special_tokens=True) for _ in range(100)]\n",
    "tokenized_seperate = [torch.tensor([t], device=device) for t in tokenized_seperate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "762cc6d8-d9b1-4a26-8b30-73944ae3928a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[unused0]', '[unused1]']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d13ac6a5-37a6-42e9-915d-148ff3ea9075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number 0\n",
      "[0.18181818181818182]\n",
      "[0.2727272727272727]\n",
      "[0.2727272727272727]\n",
      "[0.7142857142857143]\n",
      "[0.971830985915493]\n",
      "[0.8571428571428571]\n",
      "[0.5454545454545454]\n",
      "[0.18181818181818182]\n",
      "[0.36363636363636365]\n",
      "number 1\n",
      "[0.36363636363636365]\n",
      "[0.2727272727272727]\n",
      "[0.2727272727272727]\n",
      "[0.18181818181818182]\n",
      "[0.2727272727272727]\n",
      "[0.2727272727272727]\n",
      "[0.36363636363636365]\n",
      "[0.18181818181818182]\n",
      "[0.09090909090909091]\n",
      "[0.45454545454545453]\n",
      "[0.5454545454545454]\n",
      "[0.2727272727272727]\n",
      "[0.09090909090909091]\n",
      "[0.45454545454545453]\n",
      " \n",
      "*** Profile printout saved to text file 'check/profile.txt'. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "         1355547 function calls (1355347 primitive calls) in 0.862 seconds\n",
       "\n",
       "   Ordered by: cumulative time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "        1    0.000    0.000    0.865    0.865 {built-in method builtins.exec}\n",
       "        1    0.000    0.000    0.865    0.865 <string>:1(<module>)\n",
       "        1    0.000    0.000    0.865    0.865 myUtils.py:290(compute_explanations)\n",
       "        2    0.002    0.001    0.863    0.432 myUtils.py:265(get_exp)\n",
       "        2    0.000    0.000    0.861    0.431 anchor_text.py:250(explain_instance)\n",
       "        2    0.001    0.000    0.761    0.380 anchor_base.py:255(anchor_beam)\n",
       "       63    0.004    0.000    0.749    0.012 anchor_text.py:209(sample_fn)\n",
       "       59    0.000    0.000    0.730    0.012 anchor_base.py:217(<lambda>)\n",
       "       59    0.002    0.000    0.730    0.012 anchor_base.py:186(complete_sample_fn)\n",
       "       63    0.005    0.000    0.649    0.010 anchor_text.py:105(sample)\n",
       "       86    0.002    0.000    0.616    0.007 anchor_text.py:134(probs)\n",
       "       79    0.072    0.001    0.599    0.008 anchor_text.py:43(unmask)\n",
       "      142    0.233    0.002    0.233    0.002 module.py:1096(_call_impl)\n",
       "     2109    0.104    0.000    0.200    0.000 anchor_text.py:72(<listcomp>)\n",
       "        2    0.000    0.000    0.124    0.062 anchor_base.py:71(lucb)\n",
       "      405    0.003    0.000    0.101    0.000 anchor_text.py:121(<listcomp>)\n",
       "        2    0.000    0.000    0.100    0.050 anchor_text.py:196(get_sample_fn)\n",
       "  1078247    0.100    0.000    0.100    0.000 {method 'get' of 'dict' objects}\n",
       "     3496    0.087    0.000    0.098    0.000 {method 'choice' of 'numpy.random.mtrand.RandomState' objects}\n",
       "       63    0.001    0.000    0.097    0.002 myUtils.py:24(predict_sentences)\n",
       "       79    0.000    0.000    0.092    0.001 tokenization_utils_base.py:2334(__call__)\n",
       "       79    0.000    0.000    0.092    0.001 tokenization_utils_base.py:2555(batch_encode_plus)\n",
       "       79    0.000    0.000    0.090    0.001 tokenization_utils.py:662(_batch_encode_plus)\n",
       "        2    0.002    0.001    0.079    0.040 anchor_text.py:91(__init__)\n",
       "      242    0.000    0.000    0.073    0.000 tokenization_utils.py:689(get_input_ids)\n",
       "      242    0.072    0.000    0.072    0.000 {method 'tolist' of 'torch._C._TensorBase' objects}\n",
       "      242    0.005    0.000    0.067    0.000 tokenization_utils.py:474(tokenize)\n",
       "      521    0.001    0.000    0.031    0.000 tokenization_bert.py:220(_tokenize)\n",
       "      521    0.003    0.000    0.021    0.000 tokenization_bert.py:378(tokenize)\n",
       "       79    0.001    0.000    0.016    0.000 tokenization_utils.py:747(_batch_prepare_for_model)\n",
       "        2    0.001    0.000    0.015    0.008 language.py:424(__call__)\n",
       "      242    0.003    0.000    0.014    0.000 anchor_text.py:147(<listcomp>)\n",
       "     50/8    0.000    0.000    0.014    0.002 model.py:161(__call__)\n",
       "      242    0.010    0.000    0.014    0.000 tokenization_utils.py:90(split)\n",
       "     2109    0.011    0.000    0.011    0.000 anchor_text.py:24(exp_normalize)\n",
       "        8    0.000    0.000    0.011    0.001 model.py:130(predict)\n",
       "     30/6    0.000    0.000    0.010    0.002 feed_forward.py:43(begin_update)\n",
       "      242    0.001    0.000    0.009    0.000 tokenization_utils_base.py:2857(prepare_for_model)\n",
       "     2108    0.002    0.000    0.009    0.000 tokenization_utils.py:553(convert_tokens_to_ids)\n",
       "     1005    0.003    0.000    0.009    0.000 tokenization_utils_base.py:1237(all_special_tokens_extended)\n",
       "        4    0.000    0.000    0.008    0.002 api.py:293(begin_update)\n",
       "      242    0.000    0.000    0.008    0.000 re.py:203(sub)\n",
       "      763    0.001    0.000    0.007    0.000 tokenization_utils_base.py:1227(all_special_tokens)\n",
       "      242    0.003    0.000    0.007    0.000 {method 'sub' of 're.Pattern' objects}\n",
       "       22    0.000    0.000    0.007    0.000 layernorm.py:60(begin_update)\n",
       "      142    0.007    0.000    0.007    0.000 {built-in method tensor}\n",
       "     3496    0.003    0.000    0.007    0.000 numerictypes.py:359(issubdtype)\n",
       "     4942    0.002    0.000    0.007    0.000 tokenization_utils.py:575(_convert_token_to_id_with_added_voc)\n",
       "      521    0.002    0.000    0.006    0.000 tokenization_bert.py:485(_clean_text)\n",
       "      242    0.006    0.000    0.006    0.000 {built-in method topk}\n",
       "      829    0.003    0.000    0.006    0.000 tokenization_bert.py:426(_run_split_on_punc)\n",
       "       36    0.005    0.000    0.005    0.000 ops.pyx:514(gemm)\n",
       "      479    0.001    0.000    0.005    0.000 tokenization_utils_base.py:190(__init__)\n",
       "  529/453    0.004    0.000    0.005    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
       "        2    0.000    0.000    0.005    0.003 pipes.pyx:397(__call__)\n",
       "       16    0.000    0.000    0.005    0.000 resnet.py:28(begin_update)\n",
       "      305    0.005    0.000    0.005    0.000 {method 'cpu' of 'torch._C._TensorBase' objects}\n",
       "     4942    0.003    0.000    0.005    0.000 tokenization_bert.py:234(_convert_token_to_id)\n",
       "        2    0.000    0.000    0.005    0.002 pipes.pyx:409(predict)\n",
       "        4    0.000    0.000    0.005    0.001 api.py:308(predict)\n",
       "       22    0.000    0.000    0.005    0.000 maxout.py:72(begin_update)\n",
       "     12/2    0.000    0.000    0.005    0.002 feed_forward.py:38(predict)\n",
       "     6992    0.004    0.000    0.005    0.000 getlimits.py:382(__new__)\n",
       "        6    0.000    0.000    0.005    0.001 api.py:370(uniqued_fwd)\n",
       "     7274    0.003    0.000    0.005    0.000 tokenization_utils.py:507(<lambda>)\n",
       "       63    0.000    0.000    0.004    0.000 myUtils.py:25(<listcomp>)\n",
       "     1005    0.002    0.000    0.004    0.000 tokenization_utils_base.py:1210(special_tokens_map_extended)\n",
       "       44    0.000    0.000    0.004    0.000 <__array_function__ internals>:2(concatenate)\n",
       "     6992    0.003    0.000    0.004    0.000 numerictypes.py:285(issubclass_)\n",
       "        6    0.000    0.000    0.004    0.001 <__array_function__ internals>:2(vstack)\n",
       "        6    0.000    0.000    0.004    0.001 shape_base.py:223(vstack)\n",
       "      829    0.002    0.000    0.004    0.000 tokenization_bert.py:507(tokenize)\n",
       "     5424    0.003    0.000    0.003    0.000 {method 'join' of 'str' objects}\n",
       "       79    0.001    0.000    0.003    0.000 tokenization_utils_base.py:2658(pad)\n",
       "      484    0.000    0.000    0.003    0.000 tokenization_bert.py:247(build_inputs_with_special_tokens)\n",
       "       73    0.003    0.000    0.003    0.000 {built-in method numpy.zeros}\n",
       "      242    0.001    0.000    0.003    0.000 tokenization_utils.py:503(<listcomp>)\n",
       "      400    0.001    0.000    0.003    0.000 tokenization_utils_base.py:2198(_get_padding_truncation_strategies)\n",
       "      521    0.001    0.000    0.003    0.000 tokenization_bert.py:448(_tokenize_chinese_chars)\n",
       "        8    0.000    0.000    0.003    0.000 resnet.py:17(predict)\n",
       "    31318    0.003    0.000    0.003    0.000 {method 'append' of 'list' objects}\n",
       "      479    0.001    0.000    0.002    0.000 __init__.py:981(__init__)\n",
       "        8    0.000    0.000    0.002    0.000 layernorm.py:50(predict)\n",
       "     2469    0.002    0.000    0.002    0.000 tokenization_utils.py:285(_is_punctuation)\n",
       "     8056    0.002    0.000    0.002    0.000 {built-in method builtins.getattr}\n",
       "      648    0.002    0.000    0.002    0.000 {method 'binomial' of 'numpy.random.mtrand.RandomState' objects}\n",
       "    16778    0.002    0.000    0.002    0.000 {built-in method builtins.isinstance}\n",
       "       30    0.000    0.000    0.002    0.000 layernorm.py:104(_get_moments)\n",
       "      829    0.001    0.000    0.002    0.000 tokenization_bert.py:415(_run_strip_accents)\n",
       "     2777    0.001    0.000    0.002    0.000 tokenization_utils.py:273(_is_control)\n",
       "     2420    0.001    0.000    0.002    0.000 re.py:270(escape)\n",
       "      726    0.000    0.000    0.002    0.000 tokenization_utils_base.py:1129(cls_token_id)\n",
       "      242    0.000    0.000    0.002    0.000 tokenization_utils.py:451(num_special_tokens_to_add)\n",
       "      479    0.000    0.000    0.002    0.000 tokenization_utils_base.py:645(convert_to_tensors)\n",
       "      726    0.000    0.000    0.002    0.000 tokenization_utils_base.py:1102(sep_token_id)\n",
       "      479    0.001    0.000    0.002    0.000 _collections_abc.py:824(update)\n",
       "    18368    0.002    0.000    0.002    0.000 {built-in method builtins.len}\n",
       "        8    0.000    0.000    0.002    0.000 maxout.py:64(predict)\n",
       "       79    0.001    0.000    0.002    0.000 anchor_text.py:49(<listcomp>)\n",
       "    10610    0.002    0.000    0.002    0.000 {built-in method builtins.issubclass}\n",
       "     18/6    0.000    0.000    0.002    0.000 api.py:161(begin_update)\n",
       "      242    0.000    0.000    0.001    0.000 tokenization_bert.py:300(create_token_type_ids_from_sequences)\n",
       "     1871    0.001    0.000    0.001    0.000 tokenization_bert.py:108(whitespace_tokenize)\n",
       "      261    0.001    0.000    0.001    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
       "     18/6    0.000    0.000    0.001    0.000 api.py:163(<listcomp>)\n",
       "    36/12    0.000    0.000    0.001    0.000 api.py:255(wrap)\n",
       "      242    0.001    0.000    0.001    0.000 tokenization_utils.py:237(cut_text)\n",
       "     5184    0.001    0.000    0.001    0.000 tokenization_utils_base.py:976(unk_token)\n",
       "      439    0.001    0.000    0.001    0.000 {built-in method numpy.array}\n",
       "     2766    0.001    0.000    0.001    0.000 {method 'items' of 'collections.OrderedDict' objects}\n",
       "    18494    0.001    0.000    0.001    0.000 {method 'items' of 'dict' objects}\n",
       "     2420    0.001    0.000    0.001    0.000 {method 'translate' of 'str' objects}\n",
       "      170    0.000    0.000    0.001    0.000 _asarray.py:23(asarray)\n",
       "     2777    0.001    0.000    0.001    0.000 tokenization_utils.py:261(_is_whitespace)\n",
       "    12439    0.001    0.000    0.001    0.000 {method 'groups' of 're.Match' objects}\n",
       "       30    0.000    0.000    0.001    0.000 {method 'var' of 'numpy.ndarray' objects}\n",
       "      165    0.000    0.000    0.001    0.000 {method 'sum' of 'numpy.ndarray' objects}\n",
       "    10005    0.001    0.000    0.001    0.000 {built-in method unicodedata.category}\n",
       "       30    0.001    0.000    0.001    0.000 _methods.py:194(_var)\n",
       "       63    0.001    0.000    0.001    0.000 {built-in method argmax}\n",
       "       24    0.000    0.000    0.001    0.000 hash_embed.py:56(begin_update)\n",
       "      167    0.000    0.000    0.001    0.000 _methods.py:45(_sum)\n",
       "       32    0.000    0.000    0.001    0.000 {method 'mean' of 'numpy.ndarray' objects}\n",
       "       32    0.000    0.000    0.001    0.000 _methods.py:161(_mean)\n",
       "        4    0.000    0.000    0.001    0.000 _ml.py:178(begin_update)\n",
       "      172    0.000    0.000    0.001    0.000 describe.py:36(__get__)\n",
       "       63    0.000    0.000    0.001    0.000 anchor_base.py:52(dlow_bernoulli)\n",
       "      242    0.000    0.000    0.001    0.000 tokenization_utils_base.py:1141(mask_token_id)\n",
       "     5067    0.001    0.000    0.001    0.000 {method 'startswith' of 'str' objects}\n",
       "     2777    0.001    0.000    0.001    0.000 tokenization_bert.py:461(_is_chinese_char)\n",
       "      172    0.000    0.000    0.001    0.000 tokenization_utils_base.py:1112(pad_token_id)\n",
       "      407    0.000    0.000    0.001    0.000 <__array_function__ internals>:2(where)\n",
       "     2109    0.001    0.000    0.001    0.000 serialize.py:29(_numba_unpickle)\n",
       "      140    0.000    0.000    0.001    0.000 grad_mode.py:128(__exit__)\n",
       "      124    0.001    0.000    0.001    0.000 anchor_base.py:33(kl_bernoulli)\n",
       "       25    0.000    0.000    0.001    0.000 {built-in method builtins.print}\n",
       "     8023    0.001    0.000    0.001    0.000 {built-in method builtins.ord}\n",
       "      763    0.001    0.000    0.001    0.000 tokenization_utils_base.py:1234(<listcomp>)\n",
       "     5994    0.001    0.000    0.001    0.000 {method 'lower' of 'str' objects}\n",
       "     1005    0.001    0.000    0.001    0.000 {built-in method fromkeys}\n",
       "      305    0.001    0.000    0.001    0.000 {method 'numpy' of 'torch._C._TensorBase' objects}\n",
       "      280    0.000    0.000    0.001    0.000 grad_mode.py:213(__init__)\n",
       "       50    0.000    0.000    0.001    0.000 iostream.py:500(write)\n",
       "       20    0.000    0.000    0.001    0.000 <__array_function__ internals>:2(hstack)\n",
       "       23    0.001    0.000    0.001    0.000 {method 'flush' of '_io.BufferedWriter' objects}\n",
       "      642    0.000    0.000    0.001    0.000 enum.py:289(__call__)\n",
       "      140    0.000    0.000    0.001    0.000 grad_mode.py:124(__enter__)\n",
       "       20    0.000    0.000    0.001    0.000 shape_base.py:286(hstack)\n",
       "       61    0.000    0.000    0.000    0.000 anchor_base.py:40(dup_bernoulli)\n",
       "       23    0.000    0.000    0.000    0.000 {built-in method _pickle.dump}\n",
       "      172    0.000    0.000    0.000    0.000 mem.py:31(__getitem__)\n",
       "      428    0.000    0.000    0.000    0.000 {method 'copy' of 'numpy.ndarray' objects}\n",
       "        6    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(unique)\n",
       "      829    0.000    0.000    0.000    0.000 tokenization_bert.py:446(<listcomp>)\n",
       "        6    0.000    0.000    0.000    0.000 arraysetops.py:138(unique)\n",
       "        6    0.000    0.000    0.000    0.000 arraysetops.py:310(_unique1d)\n",
       "      481    0.000    0.000    0.000    0.000 abc.py:96(__instancecheck__)\n",
       "       53    0.000    0.000    0.000    0.000 iostream.py:206(schedule)\n",
       "      140    0.000    0.000    0.000    0.000 grad_mode.py:119(__init__)\n",
       "      242    0.000    0.000    0.000    0.000 tokenization_utils_base.py:3089(_pad)\n",
       "       30    0.000    0.000    0.000    0.000 ops.pyx:603(maxout)\n",
       "      563    0.000    0.000    0.000    0.000 types.py:171(__get__)\n",
       "       22    0.000    0.000    0.000    0.000 layernorm.py:92(_begin_update_scale_shift)\n",
       "      456    0.000    0.000    0.000    0.000 {built-in method builtins.min}\n",
       "      242    0.000    0.000    0.000    0.000 re.py:289(_compile)\n",
       "     1871    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}\n",
       "      451    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
       "      242    0.000    0.000    0.000    0.000 tokenization_utils.py:491(<genexpr>)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_base.py:125(make_tuples)\n",
       "     2179    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}\n",
       "        4    0.000    0.000    0.000    0.000 tokenizer.pyx:136(__call__)\n",
       "       62    0.000    0.000    0.000    0.000 _methods.py:65(_count_reduce_items)\n",
       "       30    0.000    0.000    0.000    0.000 layernorm.py:128(_forward)\n",
       "        2    0.000    0.000    0.000    0.000 check.py:142(checked_function)\n",
       "      642    0.000    0.000    0.000    0.000 enum.py:586(__new__)\n",
       "      481    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
       "       53    0.000    0.000    0.000    0.000 socket.py:480(send)\n",
       "       84    0.000    0.000    0.000    0.000 ops.pyx:490(allocate)\n",
       "      286    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
       "     1911    0.000    0.000    0.000    0.000 {method 'lstrip' of 'str' objects}\n",
       "      142    0.000    0.000    0.000    0.000 {built-in method torch._C._get_tracing_state}\n",
       "     1871    0.000    0.000    0.000    0.000 {method 'strip' of 'str' objects}\n",
       "      521    0.000    0.000    0.000    0.000 {method 'union' of 'set' objects}\n",
       "        2    0.000    0.000    0.000    0.000 softmax.py:17(predict)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method io.open}\n",
       "       48    0.000    0.000    0.000    0.000 ops.pyx:473(asarray)\n",
       "       16    0.000    0.000    0.000    0.000 convolution.py:32(begin_update)\n",
       "      420    0.000    0.000    0.000    0.000 {built-in method torch._C.is_grad_enabled}\n",
       "        4    0.000    0.000    0.000    0.000 doc.pyx:175(__init__)\n",
       "      310    0.000    0.000    0.000    0.000 describe.py:22(__get__)\n",
       "      874    0.000    0.000    0.000    0.000 __init__.py:1011(__setitem__)\n",
       "      294    0.000    0.000    0.000    0.000 {method 'nonzero' of 'numpy.ndarray' objects}\n",
       "       12    0.000    0.000    0.000    0.000 ops.pyx:138(flatten)\n",
       "        6    0.000    0.000    0.000    0.000 feature_extracter.py:12(begin_update)\n",
       "      726    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1007(cls_token)\n",
       "        6    0.000    0.000    0.000    0.000 feature_extracter.py:14(<listcomp>)\n",
       "      829    0.000    0.000    0.000    0.000 {built-in method unicodedata.normalize}\n",
       "      276    0.000    0.000    0.000    0.000 {method 'reshape' of 'numpy.ndarray' objects}\n",
       "      726    0.000    0.000    0.000    0.000 tokenization_utils_base.py:986(sep_token)\n",
       "        6    0.000    0.000    0.000    0.000 feature_extracter.py:17(_get_feats)\n",
       "       79    0.000    0.000    0.000    0.000 tokenization_utils_base.py:2371(_is_valid_text_input)\n",
       "      395    0.000    0.000    0.000    0.000 tokenization_utils_base.py:229(__getitem__)\n",
       "       24    0.000    0.000    0.000    0.000 ops.pyx:660(seq2col)\n",
       "      321    0.000    0.000    0.000    0.000 tokenization_utils_base.py:268(items)\n",
       "        2    0.000    0.000    0.000    0.000 language.py:462(make_doc)\n",
       "      330    0.000    0.000    0.000    0.000 tokenization_utils_base.py:997(pad_token)\n",
       "     1084    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
       "        4    0.000    0.000    0.000    0.000 _ml.py:213(_add_padding)\n",
       "        2    0.000    0.000    0.000    0.000 pipes.pyx:427(set_annotations)\n",
       "       83    0.000    0.000    0.000    0.000 {built-in method builtins.all}\n",
       "       61    0.000    0.000    0.000    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
       "      150    0.000    0.000    0.000    0.000 stringsource:657(memoryview_cwrapper)\n",
       "      484    0.000    0.000    0.000    0.000 tokenization_bert.py:209(do_lower_case)\n",
       "      162    0.000    0.000    0.000    0.000 _asarray.py:110(asanyarray)\n",
       "      242    0.000    0.000    0.000    0.000 tokenization_utils_base.py:3312(_eventual_warn_about_too_long_sequence)\n",
       "      172    0.000    0.000    0.000    0.000 mem.py:28(__contains__)\n",
       "        6    0.000    0.000    0.000    0.000 {method 'to_array' of 'spacy.tokens.doc.Doc' objects}\n",
       "        8    0.000    0.000    0.000    0.000 convolution.py:29(predict)\n",
       "       72    0.000    0.000    0.000    0.000 stringsource:999(memoryview_fromslice)\n",
       "       20    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(atleast_1d)\n",
       "        6    0.000    0.000    0.000    0.000 doc.pyx:701(to_array (wrapper))\n",
       "        2    0.000    0.000    0.000    0.000 anchor_base.py:435(<listcomp>)\n",
       "        6    0.000    0.000    0.000    0.000 doc.pyx:701(to_array)\n",
       "        4    0.000    0.000    0.000    0.000 doc.pyx:92(_get_chunker)\n",
       "       24    0.000    0.000    0.000    0.000 ops.pyx:715(hash)\n",
       "      280    0.000    0.000    0.000    0.000 {built-in method torch._C._set_grad_enabled}\n",
       "       23    0.000    0.000    0.000    0.000 anchor_base.py:232(get_anchor_from_tuple)\n",
       "        2    0.000    0.000    0.000    0.000 ops.pyx:536(affine)\n",
       "      321    0.000    0.000    0.000    0.000 tokenization_utils_base.py:2797(<genexpr>)\n",
       "      158    0.000    0.000    0.000    0.000 file_utils.py:2188(_is_numpy)\n",
       "       36    0.000    0.000    0.000    0.000 _asarray.py:183(ascontiguousarray)\n",
       "      563    0.000    0.000    0.000    0.000 enum.py:689(value)\n",
       "        2    0.000    0.000    0.000    0.000 ops.pyx:278(softmax)\n",
       "      158    0.000    0.000    0.000    0.000 __init__.py:1013(__iter__)\n",
       "        4    0.000    0.000    0.000    0.000 util.py:67(get_lang_class)\n",
       "      800    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}\n",
       "        6    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(diff)\n",
       "        6    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(cumsum)\n",
       "       53    0.000    0.000    0.000    0.000 threading.py:1071(is_alive)\n",
       "      484    0.000    0.000    0.000    0.000 tokenization_utils_base.py:2802(<genexpr>)\n",
       "      244    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1018(mask_token)\n",
       "       20    0.000    0.000    0.000    0.000 shape_base.py:24(atleast_1d)\n",
       "        4    0.000    0.000    0.000    0.000 _ml.py:792(flatten)\n",
       "      477    0.000    0.000    0.000    0.000 {method 'rstrip' of 'str' objects}\n",
       "       12    0.000    0.000    0.000    0.000 fromnumeric.py:52(_wrapfunc)\n",
       "       23    0.000    0.000    0.000    0.000 myUtils.py:230(__init__)\n",
       "        6    0.000    0.000    0.000    0.000 fromnumeric.py:2446(cumsum)\n",
       "        6    0.000    0.000    0.000    0.000 function_base.py:1153(diff)\n",
       "      222    0.000    0.000    0.000    0.000 stringsource:345(__cinit__)\n",
       "      407    0.000    0.000    0.000    0.000 multiarray.py:321(where)\n",
       "      158    0.000    0.000    0.000    0.000 tokenization_utils_base.py:2792(<genexpr>)\n",
       "       59    0.000    0.000    0.000    0.000 {method 'update' of 'set' objects}\n",
       "       18    0.000    0.000    0.000    0.000 api.py:239(split_backward)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_text.py:199(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 myUtils.py:222(monitor)\n",
       "       50    0.000    0.000    0.000    0.000 iostream.py:418(_is_master_process)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_text.py:203(<listcomp>)\n",
       "        8    0.000    0.000    0.000    0.000 ops.pyx:157(unflatten)\n",
       "      242    0.000    0.000    0.000    0.000 tokenization_utils.py:812(prepare_for_tokenization)\n",
       "       18    0.000    0.000    0.000    0.000 api.py:246(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_base.py:137(<listcomp>)\n",
       "        6    0.000    0.000    0.000    0.000 {method 'cumsum' of 'numpy.ndarray' objects}\n",
       "        6    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(atleast_2d)\n",
       "        2    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(amax)\n",
       "        4    0.000    0.000    0.000    0.000 catalogue.py:85(get)\n",
       "       26    0.000    0.000    0.000    0.000 shape_base.py:219(_vhstack_dispatcher)\n",
       "        6    0.000    0.000    0.000    0.000 {method 'writerow' of '_csv.writer' objects}\n",
       "       53    0.000    0.000    0.000    0.000 threading.py:1017(_wait_for_tstate_lock)\n",
       "       75    0.000    0.000    0.000    0.000 doc.pyx:315(__iter__)\n",
       "      140    0.000    0.000    0.000    0.000 _jit_internal.py:957(is_scripting)\n",
       "       46    0.000    0.000    0.000    0.000 tokenizer.pyx:220(_try_cache)\n",
       "        2    0.000    0.000    0.000    0.000 fromnumeric.py:2617(amax)\n",
       "       50    0.000    0.000    0.000    0.000 {built-in method posix.getpid}\n",
       "        6    0.000    0.000    0.000    0.000 shape_base.py:82(atleast_2d)\n",
       "        6    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(nonzero)\n",
       "       50    0.000    0.000    0.000    0.000 iostream.py:437(_schedule_flush)\n",
       "        4    0.000    0.000    0.000    0.000 catalogue.py:49(__contains__)\n",
       "       23    0.000    0.000    0.000    0.000 myUtils.py:275(get_test_cov)\n",
       "       68    0.000    0.000    0.000    0.000 {built-in method numpy.core._multiarray_umath.normalize_axis_index}\n",
       "       22    0.000    0.000    0.000    0.000 myUtils.py:192(update_normal)\n",
       "      158    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
       "      222    0.000    0.000    0.000    0.000 stringsource:372(__dealloc__)\n",
       "        2    0.000    0.000    0.000    0.000 fromnumeric.py:70(_wrapreduction)\n",
       "       23    0.000    0.000    0.000    0.000 anchor_base.py:238(<lambda>)\n",
       "       26    0.000    0.000    0.000    0.000 shape_base.py:208(_arrays_for_stack_dispatcher)\n",
       "        6    0.000    0.000    0.000    0.000 {method 'argsort' of 'numpy.ndarray' objects}\n",
       "        4    0.000    0.000    0.000    0.000 catalogue.py:153(_get)\n",
       "       23    0.000    0.000    0.000    0.000 anchor_text.py:269(<listcomp>)\n",
       "      150    0.000    0.000    0.000    0.000 stringsource:663(memoryview_check)\n",
       "        2    0.000    0.000    0.000    0.000 nonproj.pyx:133(deprojectivize (wrapper))\n",
       "       44    0.000    0.000    0.000    0.000 ops.pyx:122(dropout)\n",
       "        4    0.000    0.000    0.000    0.000 vocab.pyx:78(__get__)\n",
       "        2    0.000    0.000    0.000    0.000 check.py:59(has_shape_inner)\n",
       "        2    0.000    0.000    0.000    0.000 nonproj.pyx:133(deprojectivize)\n",
       "        2    0.000    0.000    0.000    0.000 doc.pyx:1022(extend_tensor)\n",
       "        6    0.000    0.000    0.000    0.000 fromnumeric.py:1827(nonzero)\n",
       "       53    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}\n",
       "        2    0.000    0.000    0.000    0.000 anchor_text.py:204(<listcomp>)\n",
       "       53    0.000    0.000    0.000    0.000 iostream.py:96(_event_pipe)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_base.py:221(get_initial_statistics)\n",
       "       12    0.000    0.000    0.000    0.000 {built-in method numpy.empty}\n",
       "       69    0.000    0.000    0.000    0.000 token.pxd:19(cinit)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_base.py:182(get_sample_fns)\n",
       "       23    0.000    0.000    0.000    0.000 anchor_explanation.py:54(coverage)\n",
       "        4    0.000    0.000    0.000    0.000 util.py:143(copy_array)\n",
       "       36    0.000    0.000    0.000    0.000 api.py:250(sink_return)\n",
       "        8    0.000    0.000    0.000    0.000 catalogue.py:130(get_entry_point)\n",
       "       16    0.000    0.000    0.000    0.000 convolution.py:37(_get_finish_update)\n",
       "       23    0.000    0.000    0.000    0.000 anchor_explanation.py:12(names)\n",
       "       18    0.000    0.000    0.000    0.000 api.py:166(<listcomp>)\n",
       "      108    0.000    0.000    0.000    0.000 stringsource:518(__getbuffer__)\n",
       "       72    0.000    0.000    0.000    0.000 stringsource:976(__dealloc__)\n",
       "       23    0.000    0.000    0.000    0.000 myUtils.py:203(desired_confidence_factor)\n",
       "       23    0.000    0.000    0.000    0.000 anchor_explanation.py:7(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 myUtils.py:186(update_anchor)\n",
       "       23    0.000    0.000    0.000    0.000 anchor_explanation.py:38(precision)\n",
       "       23    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}\n",
       "        2    0.000    0.000    0.000    0.000 arc_eager.pyx:553(finalize_doc)\n",
       "       44    0.000    0.000    0.000    0.000 multiarray.py:143(concatenate)\n",
       "       53    0.000    0.000    0.000    0.000 threading.py:513(is_set)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'sort' of 'list' objects}\n",
       "        6    0.000    0.000    0.000    0.000 codecs.py:327(reset)\n",
       "       46    0.000    0.000    0.000    0.000 doc.pyx:673(__pyx_fuse_0push_back)\n",
       "       16    0.000    0.000    0.000    0.000 catalogue.py:160(<genexpr>)\n",
       "        4    0.000    0.000    0.000    0.000 doc.pyx:1225(set_children_from_heads)\n",
       "       72    0.000    0.000    0.000    0.000 stringsource:559(__get__)\n",
       "        6    0.000    0.000    0.000    0.000 {method 'flatten' of 'numpy.ndarray' objects}\n",
       "       53    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
       "        1    0.000    0.000    0.000    0.000 myUtils.py:134(update)\n",
       "       23    0.000    0.000    0.000    0.000 __init__.py:145(_DType_reduce)\n",
       "       23    0.000    0.000    0.000    0.000 anchor_text.py:270(<listcomp>)\n",
       "       23    0.000    0.000    0.000    0.000 anchor_base.py:299(<lambda>)\n",
       "       44    0.000    0.000    0.000    0.000 ops.pyx:124(lambda1)\n",
       "        4    0.000    0.000    0.000    0.000 doc.pyx:1255(_set_lr_kids_and_edges)\n",
       "       14    0.000    0.000    0.000    0.000 util.py:14(<lambda>)\n",
       "       23    0.000    0.000    0.000    0.000 myUtils.py:195(should_calculate)\n",
       "        2    0.000    0.000    0.000    0.000 pipes.pyx:1106(__get__)\n",
       "        4    0.000    0.000    0.000    0.000 _ml.py:795(<listcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 api.py:309(<listcomp>)\n",
       "        6    0.000    0.000    0.000    0.000 arraysetops.py:125(_unpack_tuple)\n",
       "       23    0.000    0.000    0.000    0.000 myUtils.py:268(get_fit_examples)\n",
       "        2    0.000    0.000    0.000    0.000 pipes.pyx:85(require_model)\n",
       "        4    0.000    0.000    0.000    0.000 api.py:294(<listcomp>)\n",
       "       24    0.000    0.000    0.000    0.000 ops.pyx:209(get_dropout_mask)\n",
       "       20    0.000    0.000    0.000    0.000 shape_base.py:20(_atleast_1d_dispatcher)\n",
       "        2    0.000    0.000    0.000    0.000 pipes.pyx:411(genexpr)\n",
       "       23    0.000    0.000    0.000    0.000 myUtils.py:308(<lambda>)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_base.py:334(<listcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 util.py:340(get_cuda_stream)\n",
       "        2    0.000    0.000    0.000    0.000 fromnumeric.py:71(<dictcomp>)\n",
       "        6    0.000    0.000    0.000    0.000 function_base.py:1149(_diff_dispatcher)\n",
       "        4    0.000    0.000    0.000    0.000 catalogue.py:144(check_exists)\n",
       "        4    0.000    0.000    0.000    0.000 __init__.py:17(_return_en)\n",
       "        6    0.000    0.000    0.000    0.000 fromnumeric.py:2442(_cumsum_dispatcher)\n",
       "        6    0.000    0.000    0.000    0.000 arraysetops.py:133(_unique_dispatcher)\n",
       "        6    0.000    0.000    0.000    0.000 fromnumeric.py:1823(_nonzero_dispatcher)\n",
       "        6    0.000    0.000    0.000    0.000 shape_base.py:78(_atleast_2d_dispatcher)\n",
       "       10    0.000    0.000    0.000    0.000 doc.pyx:328(__len__)\n",
       "        6    0.000    0.000    0.000    0.000 codecs.py:276(reset)\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method time.time}\n",
       "        2    0.000    0.000    0.000    0.000 fromnumeric.py:2612(_amax_dispatcher)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#torch._C._jit_set_texpr_fuser_enabled(False)\n",
    "my_utils = TextUtils(anchor_examples, test, explainer, predict_sentences, ignored, f\"profile.pickle\", optimize=optimize)\n",
    "myUtils.model = model\n",
    "myUtils.tokenizer = tokenizer\n",
    "set_seed()\n",
    "%prun -s cumtime -T check/profile.txt my_utils.compute_explanations(list(range(len(anchor_examples))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb1bd996-c9ac-4704-b77e-472c29d051bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### my_utils = TextUtils(anchor_examples, test, explainer, predict_sentences, ignored,f\"profile.pickle\")\n",
    "#%lprun -s -m modified_anchor.anchor_text -m modified_anchor.anchor_base -m myUtils -T profile.txt  my_utils.compute_explanations(list(range(len(anchor_examples))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "891a9e98-7566-43f6-8eb1-1ed446033ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-10 22:17:58.159443\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5073ae0a-f2d7-4f9e-95c1-3353fef4304d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "import spacy\n",
    "from optimized_anchor import anchor_text, anchor_base\n",
    "import pickle\n",
    "import myUtils\n",
    "from myUtils import *\n",
    "from models.utils import *\n",
    "from models.dataset_loader import *\n",
    "import datetime\n",
    "import time\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('models')\n",
    "# when apply torchscript to models sometimes\n",
    "#torch._C._jit_set_texpr_fuser_enabled(False)\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_name = 'huawei-noah/TinyBERT_General_4L_312D'\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2444c8e5-2a8b-4797-9248-46565487f5a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# can be sentiment/spam/dilemma\n",
    "dataset_name = 'corona'\n",
    "ds = get_ds(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6358237f-3408-43ff-b1c7-b0c880396138",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/12542 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "anchor_examples, _  = preprocess_examples(ds, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1da1181f-98b5-4f15-9896-d4a99cf231be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast = False)\n",
    "myUtils.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50cb3bc3-5a28-49a4-af65-914fd963edb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ignored = get_ignored(anchor_examples)\n",
    "normal_occurences = get_occurences(anchor_examples)\n",
    "bg = BestGroup('check', normal_occurences, filter_anchors = False, desired_optimize = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f52d42-f62d-4dab-a6a4-2c2e458b7433",
   "metadata": {
    "tags": []
   },
   "source": [
    "## notice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec7977c8-6e39-4b91-94f0-3a934abc63f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ignored = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09e3b250-34e7-4232-afdb-76f1c3ee2221",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples = anchor_examples[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042a3661-59b9-4f72-b84b-8d640ab46247",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c976b285-caee-439e-870d-a43850a4551c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimize = True\n",
    "anchor_text.AnchorText.set_optimize(optimize)\n",
    "explainer = anchor_text.AnchorText(nlp, ['positive', 'negative'], use_unk_distribution=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3dbad78-2676-47ab-97c8-2ffbfe09e81c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "#model = torch.jit.load(f'models/tinybert/{dataset_name}/traced.pt').to(device)\n",
    "model = load_model(f'models/tinybert/{dataset_name}/model').to(device)\n",
    "model = model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D', use_fast = False)\n",
    "myUtils.model = model\n",
    "myUtils.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beffd1f6-fc2a-467b-b7fb-1c7eb2980739",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = explainer.tg.bert_tokenizer([\"i [MASK] to [MASK] party yesterday\"], \n",
    "          add_special_tokens=True, return_tensors=\"pt\", padding=True)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f25c2a0-428e-412b-b0f5-97a7e03294ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_seperate = [explainer.tg.bert_tokenizer.encode(\"i went to a party yesterday\", \n",
    "          add_special_tokens=True) for _ in range(100)]\n",
    "tokenized_seperate = [torch.tensor([t], device=device) for t in tokenized_seperate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c50f32-ea55-4ccc-9fa2-fd79d631ace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b38e97a-028f-4e9b-be40-e203e5c70ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([  101,   103,  2049,  2219, 13109, 28819,  7999,  2126,  1010,\n",
    "        2009,  4152,  2000,  2017,  1012,  2074,  2066,  1045, 18259,\n",
    "        2100,   102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e4647f8f-15db-489d-89f4-0dc8a34f73ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = load_model(f'models/tinybert/{dataset_name}/model').to(device).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D', \n",
    "                                          use_fast = False, do_lower_case = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d13ac6a5-37a6-42e9-915d-148ff3ea9075",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number 0\n",
      "number 1\n",
      "[0.9702970297029703]\n",
      "[0.9803921568627451]\n",
      " \n",
      "*** Profile printout saved to text file 'check/profile.txt'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "         500475 function calls (500079 primitive calls) in 2.554 seconds\n",
       "\n",
       "   Ordered by: cumulative time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "        1    0.000    0.000    2.576    2.576 {built-in method builtins.exec}\n",
       "        1    0.000    0.000    2.576    2.576 <string>:1(<module>)\n",
       "        1    0.000    0.000    2.576    2.576 myUtils.py:267(compute_explanations)\n",
       "        2    0.098    0.049    2.575    1.288 myUtils.py:264(get_exp)\n",
       "        2    0.000    0.000    2.477    1.239 anchor_text.py:267(explain_instance)\n",
       "        2    0.001    0.001    2.283    1.142 anchor_base.py:255(anchor_beam)\n",
       "      158    0.013    0.000    2.270    0.014 anchor_text.py:227(sample_fn)\n",
       "      154    0.000    0.000    2.260    0.015 anchor_base.py:217(<lambda>)\n",
       "      154    0.005    0.000    2.260    0.015 anchor_base.py:186(complete_sample_fn)\n",
       "      158    0.019    0.000    2.002    0.013 anchor_text.py:120(sample)\n",
       "      226    0.004    0.000    1.478    0.007 anchor_text.py:155(probs)\n",
       "      226    0.297    0.001    1.234    0.005 anchor_text.py:47(unmask)\n",
       "     1040    0.022    0.000    0.640    0.001 anchor_text.py:141(<listcomp>)\n",
       "      384    0.623    0.002    0.623    0.002 module.py:1494(_call_impl)\n",
       "    28581    0.536    0.000    0.615    0.000 {method 'choice' of 'numpy.random._generator.Generator' objects}\n",
       "     1093    0.400    0.000    0.400    0.000 {method 'tolist' of 'torch._C._TensorBase' objects}\n",
       "        2    0.000    0.000    0.282    0.141 anchor_base.py:71(lucb)\n",
       "      158    0.001    0.000    0.252    0.002 myUtils.py:32(predict_sentences)\n",
       "     1093    0.023    0.000    0.240    0.000 anchor_text.py:168(<listcomp>)\n",
       "    28305    0.135    0.000    0.217    0.000 anchor_text.py:24(exp_normalize)\n",
       "        2    0.000    0.000    0.193    0.097 anchor_text.py:217(get_sample_fn)\n",
       "        2    0.004    0.002    0.167    0.083 anchor_text.py:91(__init__)\n",
       "    28305    0.081    0.000    0.081    0.000 serialize.py:29(_numba_unpickle)\n",
       "     1093    0.070    0.000    0.070    0.000 {built-in method torch.topk}\n",
       "    28581    0.019    0.000    0.046    0.000 numerictypes.py:356(issubdtype)\n",
       "    57162    0.027    0.000    0.032    0.000 getlimits.py:458(__new__)\n",
       "     1251    0.026    0.000    0.026    0.000 {method 'cpu' of 'torch._C._TensorBase' objects}\n",
       "    57162    0.017    0.000    0.025    0.000 numerictypes.py:282(issubclass_)\n",
       "      158    0.001    0.000    0.024    0.000 myUtils.py:33(<listcomp>)\n",
       "      384    0.022    0.000    0.022    0.000 {built-in method torch.tensor}\n",
       "        2    0.000    0.000    0.018    0.009 language.py:978(__call__)\n",
       "        8    0.002    0.000    0.015    0.002 trainable_pipe.pyx:40(__call__)\n",
       "     1331    0.014    0.000    0.014    0.000 {method 'join' of 'str' objects}\n",
       "        8    0.000    0.000    0.012    0.002 model.py:311(predict)\n",
       "   270/16    0.000    0.000    0.012    0.001 model.py:288(__call__)\n",
       "     94/8    0.000    0.000    0.012    0.001 chain.py:49(forward)\n",
       "       14    0.000    0.000    0.010    0.001 with_array.py:28(forward)\n",
       "    85825    0.010    0.000    0.010    0.000 {built-in method builtins.issubclass}\n",
       "       68    0.008    0.000    0.008    0.000 anchor_text.py:113(<listcomp>)\n",
       "     1251    0.007    0.000    0.007    0.000 {method 'numpy' of 'torch._C._TensorBase' objects}\n",
       "      226    0.003    0.000    0.007    0.000 anchor_text.py:55(<listcomp>)\n",
       "        6    0.000    0.000    0.007    0.001 with_array.py:66(_list_forward)\n",
       "        2    0.000    0.000    0.006    0.003 tok2vec.py:112(predict)\n",
       "       16    0.000    0.000    0.006    0.000 residual.py:28(forward)\n",
       "     5302    0.006    0.000    0.006    0.000 {method 'binomial' of 'numpy.random.mtrand.RandomState' objects}\n",
       "    59615    0.006    0.000    0.006    0.000 {method 'get' of 'dict' objects}\n",
       "        4    0.000    0.000    0.006    0.001 tb_framework.py:32(forward)\n",
       "       20    0.000    0.000    0.005    0.000 maxout.py:46(forward)\n",
       "       30    0.004    0.000    0.004    0.000 numpy_ops.pyx:90(gemm)\n",
       "    31916    0.004    0.000    0.004    0.000 {built-in method builtins.len}\n",
       "     1095    0.001    0.000    0.004    0.000 tokenization_utils_base.py:1182(mask_token_id)\n",
       "        8    0.000    0.000    0.004    0.000 with_array.py:83(_ragged_forward)\n",
       "    37502    0.003    0.000    0.003    0.000 {method 'append' of 'list' objects}\n",
       "1150/1094    0.002    0.000    0.003    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
       "      158    0.003    0.000    0.003    0.000 {built-in method torch.argmax}\n",
       "     1095    0.001    0.000    0.003    0.000 tokenization_utils.py:560(convert_tokens_to_ids)\n",
       "        2    0.000    0.000    0.003    0.001 tokenization_utils.py:481(tokenize)\n",
       "     2216    0.002    0.000    0.002    0.000 {method 'copy' of 'numpy.ndarray' objects}\n",
       "       20    0.000    0.000    0.002    0.000 layernorm.py:24(forward)\n",
       "     1095    0.001    0.000    0.002    0.000 tokenization_utils.py:582(_convert_token_to_id_with_added_voc)\n",
       "        2    0.000    0.000    0.002    0.001 attributeruler.py:133(__call__)\n",
       "     1046    0.001    0.000    0.002    0.000 <__array_function__ internals>:177(where)\n",
       "      382    0.001    0.000    0.002    0.000 grad_mode.py:57(__exit__)\n",
       "      369    0.000    0.000    0.002    0.000 {method 'sum' of 'numpy.ndarray' objects}\n",
       "        2    0.000    0.000    0.002    0.001 tokenization_distilbert.py:198(_tokenize)\n",
       "      431    0.002    0.000    0.002    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
       "      764    0.001    0.000    0.002    0.000 grad_mode.py:149(__init__)\n",
       "      161    0.001    0.000    0.002    0.000 anchor_base.py:52(dlow_bernoulli)\n",
       "      369    0.000    0.000    0.002    0.000 _methods.py:46(_sum)\n",
       "      382    0.001    0.000    0.002    0.000 grad_mode.py:53(__enter__)\n",
       "        2    0.000    0.000    0.001    0.001 tokenization_distilbert.py:363(tokenize)\n",
       "        4    0.000    0.000    0.001    0.000 concatenate.py:43(forward)\n",
       "       20    0.000    0.000    0.001    0.000 layernorm.py:73(_get_moments)\n",
       "        2    0.000    0.000    0.001    0.001 attributeruler.py:149(match)\n",
       "      320    0.001    0.000    0.001    0.000 anchor_base.py:33(kl_bernoulli)\n",
       "     1095    0.001    0.000    0.001    0.000 tokenization_distilbert.py:212(_convert_token_to_id)\n",
       "        2    0.000    0.000    0.001    0.001 matcher.pyx:211(__call__)\n",
       "        2    0.000    0.000    0.001    0.001 matcher.pyx:342(find_matches)\n",
       "      382    0.001    0.000    0.001    0.000 grad_mode.py:48(__init__)\n",
       "       24    0.000    0.000    0.001    0.000 <__array_function__ internals>:177(concatenate)\n",
       "        4    0.000    0.000    0.001    0.000 concatenate.py:44(<listcomp>)\n",
       "       62    0.000    0.000    0.001    0.000 matcher.pyx:413(transition_states)\n",
       "      159    0.000    0.000    0.001    0.000 anchor_base.py:40(dup_bernoulli)\n",
       "       20    0.000    0.000    0.001    0.000 hashembed.py:60(forward)\n",
       "      176    0.001    0.000    0.001    0.000 {built-in method numpy.zeros}\n",
       "      394    0.001    0.000    0.001    0.000 {built-in method numpy.array}\n",
       "        4    0.000    0.000    0.001    0.000 {built-in method builtins.print}\n",
       "     1187    0.001    0.000    0.001    0.000 {built-in method builtins.min}\n",
       "        8    0.000    0.000    0.001    0.000 iostream.py:518(write)\n",
       "        8    0.000    0.000    0.001    0.000 iostream.py:448(_schedule_flush)\n",
       "     1054    0.000    0.000    0.001    0.000 matcher.pyx:525(update_predicate_cache)\n",
       "        3    0.000    0.000    0.001    0.000 iostream.py:202(schedule)\n",
       "       20    0.001    0.000    0.001    0.000 numpy_ops.pyx:164(__pyx_fuse_0maxout)\n",
       "     1215    0.001    0.000    0.001    0.000 {method 'nonzero' of 'numpy.ndarray' objects}\n",
       "      384    0.001    0.000    0.001    0.000 {built-in method torch._C._get_tracing_state}\n",
       "        3    0.001    0.000    0.001    0.000 socket.py:543(send)\n",
       "       20    0.000    0.000    0.001    0.000 {method 'var' of 'numpy.ndarray' objects}\n",
       "        2    0.000    0.000    0.001    0.000 anchor_base.py:125(make_tuples)\n",
       "        2    0.000    0.000    0.001    0.000 <__array_function__ internals>:177(vstack)\n",
       "        2    0.000    0.000    0.001    0.000 shape_base.py:222(vstack)\n",
       "       20    0.000    0.000    0.001    0.000 _methods.py:196(_var)\n",
       "       22    0.000    0.000    0.001    0.000 {method 'mean' of 'numpy.ndarray' objects}\n",
       "        2    0.000    0.000    0.001    0.000 lemmatizer.py:121(__call__)\n",
       "       10    0.000    0.000    0.001    0.000 ops.py:329(unflatten)\n",
       "       22    0.000    0.000    0.001    0.000 _methods.py:163(_mean)\n",
       "        2    0.000    0.000    0.001    0.000 tokenization_distilbert.py:470(_clean_text)\n",
       "     1146    0.001    0.000    0.001    0.000 {built-in method torch.is_grad_enabled}\n",
       "        6    0.000    0.000    0.001    0.000 <__array_function__ internals>:177(hstack)\n",
       "        2    0.000    0.000    0.000    0.000 re.py:202(sub)\n",
       "        6    0.000    0.000    0.000    0.000 shape_base.py:285(hstack)\n",
       "        2    0.000    0.000    0.000    0.000 {method 'sub' of 're.Pattern' objects}\n",
       "        4    0.000    0.000    0.000    0.000 _precomputable_affine.py:19(forward)\n",
       "       60    0.000    0.000    0.000    0.000 tokenization_distilbert.py:411(_run_split_on_punc)\n",
       "       53    0.000    0.000    0.000    0.000 lemmatizer.py:196(rule_lemmatize)\n",
       "        2    0.000    0.000    0.000    0.000 attributeruler.py:158(set_annotations)\n",
       "       16    0.000    0.000    0.000    0.000 expand_window.py:19(forward)\n",
       "     1321    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1013(unk_token)\n",
       "       16    0.000    0.000    0.000    0.000 expand_window.py:26(_expand_window_floats)\n",
       "      156    0.000    0.000    0.000    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
       "       16    0.000    0.000    0.000    0.000 numpy_ops.pyx:227(seq2col)\n",
       "        2    0.000    0.000    0.000    0.000 language.py:1084(_ensure_doc)\n",
       "        4    0.000    0.000    0.000    0.000 model.py:849(set_dropout_rate)\n",
       "        2    0.000    0.000    0.000    0.000 language.py:1072(make_doc)\n",
       "        2    0.000    0.000    0.000    0.000 tagger.pyx:129(predict)\n",
       "        4    0.000    0.000    0.000    0.000 concatenate.py:56(_array_forward)\n",
       "        2    0.000    0.000    0.000    0.000 tokenizer.pyx:147(__call__)\n",
       "      992    0.000    0.000    0.000    0.000 matcher.pyx:917(__call__)\n",
       "       20    0.000    0.000    0.000    0.000 numpy_ops.pyx:440(__pyx_fuse_0_1gather_add)\n",
       "        2    0.000    0.000    0.000    0.000 tokenizer.pyx:160(_tokenize_affixes)\n",
       "      136    0.000    0.000    0.000    0.000 model.py:211(get_param)\n",
       "       54    0.000    0.000    0.000    0.000 numpy_ops.pyx:64(asarray)\n",
       "     1097    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1059(mask_token)\n",
       "      957    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
       "       10    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(split)\n",
       "       10    0.000    0.000    0.000    0.000 shape_base.py:799(split)\n",
       "        4    0.000    0.000    0.000    0.000 ragged2list.py:18(forward)\n",
       "      764    0.000    0.000    0.000    0.000 {built-in method torch._C._set_grad_enabled}\n",
       "       14    0.000    0.000    0.000    0.000 ops.py:284(flatten)\n",
       "       10    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(array_split)\n",
       "       20    0.000    0.000    0.000    0.000 layernorm.py:59(_begin_update_scale_shift)\n",
       "       61    0.000    0.000    0.000    0.000 tokenization_distilbert.py:493(tokenize)\n",
       "      148    0.000    0.000    0.000    0.000 model.py:370(_walk_bfs)\n",
       "      558    0.000    0.000    0.000    0.000 doc.pyx:452(__getitem__)\n",
       "      154    0.000    0.000    0.000    0.000 {method 'update' of 'set' objects}\n",
       "       10    0.000    0.000    0.000    0.000 shape_base.py:739(array_split)\n",
       "        2    0.000    0.000    0.000    0.000 tokenization_distilbert.py:433(_tokenize_chinese_chars)\n",
       "      243    0.000    0.000    0.000    0.000 tokenization_utils.py:292(_is_punctuation)\n",
       "      302    0.000    0.000    0.000    0.000 tokenization_utils.py:514(<lambda>)\n",
       "        2    0.000    0.000    0.000    0.000 {numpy.random._generator.default_rng}\n",
       "      302    0.000    0.000    0.000    0.000 tokenization_utils.py:280(_is_control)\n",
       "        4    0.000    0.000    0.000    0.000 linear.py:36(forward)\n",
       "       30    0.000    0.000    0.000    0.000 ops.py:667(asarray1i)\n",
       "        4    0.000    0.000    0.000    0.000 list2ragged.py:21(forward)\n",
       "     1475    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
       "       80    0.000    0.000    0.000    0.000 numpy_ops.pyx:81(alloc)\n",
       "     1046    0.000    0.000    0.000    0.000 multiarray.py:341(where)\n",
       "       60    0.000    0.000    0.000    0.000 tokenization_distilbert.py:400(_run_strip_accents)\n",
       "       45    0.000    0.000    0.000    0.000 lemmatizer.py:8(is_base_form)\n",
       "        4    0.000    0.000    0.000    0.000 featureextractor.py:13(forward)\n",
       "       20    0.000    0.000    0.000    0.000 ops.py:659(asarray_f)\n",
       "       42    0.000    0.000    0.000    0.000 _methods.py:66(_count_reduce_items)\n",
       "       10    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(cumsum)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_base.py:137(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 tokenization_utils.py:90(split)\n",
       "       40    0.000    0.000    0.000    0.000 fromnumeric.py:51(_wrapfunc)\n",
       "        2    0.000    0.000    0.000    0.000 softmax.py:64(forward)\n",
       "      124    0.000    0.000    0.000    0.000 matcher.pyx:881(__call__)\n",
       "        2    0.000    0.000    0.000    0.000 doc.pyx:180(__init__)\n",
       "       60    0.000    0.000    0.000    0.000 ops.py:621(reshape)\n",
       "        2    0.000    0.000    0.000    0.000 contextlib.py:76(inner)\n",
       "       10    0.000    0.000    0.000    0.000 fromnumeric.py:2497(cumsum)\n",
       "       84    0.000    0.000    0.000    0.000 ops.py:723(as_contig)\n",
       "      301    0.000    0.000    0.000    0.000 tokenization_utils.py:268(_is_whitespace)\n",
       "        4    0.000    0.000    0.000    0.000 model.py:66(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 tagger.pyx:158(set_annotations)\n",
       "      136    0.000    0.000    0.000    0.000 _param_server.py:39(has_param)\n",
       "        2    0.000    0.000    0.000    0.000 ops.py:224(affine)\n",
       "       16    0.000    0.000    0.000    0.000 numpy_ops.pyx:515(check_seq2col_lengths)\n",
       "        6    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1277(all_special_tokens_extended)\n",
       "       30    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(swapaxes)\n",
       "       59    0.000    0.000    0.000    0.000 myUtils.py:212(update_normal)\n",
       "        4    0.000    0.000    0.000    0.000 list2array.py:21(forward)\n",
       "        4    0.000    0.000    0.000    0.000 {method 'to_array' of 'spacy.tokens.doc.Doc' objects}\n",
       "       96    0.000    0.000    0.000    0.000 stringsource:659(memoryview_cwrapper)\n",
       "      382    0.000    0.000    0.000    0.000 _jit_internal.py:1102(is_scripting)\n",
       "        4    0.000    0.000    0.000    0.000 doc.pyx:938(to_array (wrapper))\n",
       "     1031    0.000    0.000    0.000    0.000 {built-in method unicodedata.category}\n",
       "        2    0.000    0.000    0.000    0.000 myUtils.py:242(monitor)\n",
       "      614    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}\n",
       "       98    0.000    0.000    0.000    0.000 {spacy.tokens._retokenize.set_token_attrs}\n",
       "      806    0.000    0.000    0.000    0.000 token.pxd:21(cinit)\n",
       "        4    0.000    0.000    0.000    0.000 doc.pyx:938(to_array)\n",
       "        2    0.000    0.000    0.000    0.000 attributeruler.py:152(<listcomp>)\n",
       "       10    0.000    0.000    0.000    0.000 {method 'cumsum' of 'numpy.ndarray' objects}\n",
       "      256    0.000    0.000    0.000    0.000 doc.pyx:485(__iter__)\n",
       "       45    0.000    0.000    0.000    0.000 {method 'to_dict' of 'spacy.tokens.morphanalysis.MorphAnalysis' objects}\n",
       "       81    0.000    0.000    0.000    0.000 lookups.py:109(get)\n",
       "        2    0.000    0.000    0.000    0.000 language.py:323(pipeline)\n",
       "       20    0.000    0.000    0.000    0.000 numpy_ops.pyx:440(gather_add)\n",
       "       92    0.000    0.000    0.000    0.000 {method 'reshape' of 'numpy.ndarray' objects}\n",
       "      301    0.000    0.000    0.000    0.000 tokenization_distilbert.py:446(_is_chinese_char)\n",
       "       20    0.000    0.000    0.000    0.000 numpy_ops.pyx:303(hash)\n",
       "        2    0.000    0.000    0.000    0.000 _dict_proxies.py:26(__init__)\n",
       "       62    0.000    0.000    0.000    0.000 tokenizer.pyx:359(_try_specials_and_cache)\n",
       "        2    0.000    0.000    0.000    0.000 tokenizer.pyx:388(_tokenize)\n",
       "       20    0.000    0.000    0.000    0.000 ops.py:592(reshape2f)\n",
       "      296    0.000    0.000    0.000    0.000 model.py:116(attrs)\n",
       "       20    0.000    0.000    0.000    0.000 ops.py:589(reshape1f)\n",
       "       20    0.000    0.000    0.000    0.000 numpy_ops.pyx:164(maxout)\n",
       "      276    0.000    0.000    0.000    0.000 model.py:105(layers)\n",
       "       12    0.000    0.000    0.000    0.000 types.py:1162(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_text.py:101(<listcomp>)\n",
       "       20    0.000    0.000    0.000    0.000 dropout.py:20(forward)\n",
       "       96    0.000    0.000    0.000    0.000 model.py:168(get_dim)\n",
       "       30    0.000    0.000    0.000    0.000 fromnumeric.py:550(swapaxes)\n",
       "      136    0.000    0.000    0.000    0.000 _param_server.py:45(get_param)\n",
       "       98    0.000    0.000    0.000    0.000 _retokenize.pyx:455(set_token_attrs)\n",
       "       65    0.000    0.000    0.000    0.000 tokenization_distilbert.py:80(whitespace_tokenize)\n",
       "      846    0.000    0.000    0.000    0.000 {built-in method builtins.ord}\n",
       "        4    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1267(all_special_tokens)\n",
       "       20    0.000    0.000    0.000    0.000 array_getitem.py:38(forward)\n",
       "      497    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}\n",
       "        6    0.000    0.000    0.000    0.000 {method 'writerow' of '_csv.writer' objects}\n",
       "        2    0.000    0.000    0.000    0.000 __init__.py:1091(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 doc.pyx:766(set_ents)\n",
       "       20    0.000    0.000    0.000    0.000 ops.py:595(reshape3f)\n",
       "        6    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1251(special_tokens_map_extended)\n",
       "      604    0.000    0.000    0.000    0.000 {method 'groups' of 're.Match' objects}\n",
       "       42    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
       "        2    0.000    0.000    0.000    0.000 arc_eager.pyx:744(set_annotations)\n",
       "        2    0.000    0.000    0.000    0.000 {function SeedSequence.generate_state at 0x7f2d10265750}\n",
       "        2    0.000    0.000    0.000    0.000 _ufunc_config.py:429(__enter__)\n",
       "      112    0.000    0.000    0.000    0.000 stringsource:346(__cinit__)\n",
       "        6    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(atleast_1d)\n",
       "        4    0.000    0.000    0.000    0.000 _ufunc_config.py:32(seterr)\n",
       "       20    0.000    0.000    0.000    0.000 {built-in method numpy.ascontiguousarray}\n",
       "        2    0.000    0.000    0.000    0.000 tokenization_utils.py:510(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 _collections_abc.py:991(update)\n",
       "      267    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}\n",
       "        2    0.000    0.000    0.000    0.000 nonproj.pyx:172(deprojectivize (wrapper))\n",
       "        4    0.000    0.000    0.000    0.000 ops.py:672(asarray2i)\n",
       "       32    0.000    0.000    0.000    0.000 stringsource:1001(memoryview_fromslice)\n",
       "        2    0.000    0.000    0.000    0.000 nonproj.pyx:172(deprojectivize)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_text.py:222(<listcomp>)\n",
       "       60    0.000    0.000    0.000    0.000 doc.pyx:914(__pyx_fuse_0push_back)\n",
       "        2    0.000    0.000    0.000    0.000 tokenizer.pyx:445(_attach_tokens)\n",
       "       14    0.000    0.000    0.000    0.000 util.py:49(get_array_module)\n",
       "       88    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
       "        2    0.000    0.000    0.000    0.000 anchor_base.py:221(get_initial_statistics)\n",
       "       60    0.000    0.000    0.000    0.000 tokenization_distilbert.py:431(<listcomp>)\n",
       "       24    0.000    0.000    0.000    0.000 pipe.pyx:133(get_error_handler)\n",
       "        2    0.000    0.000    0.000    0.000 tokenizer.pyx:400(_split_affixes)\n",
       "      284    0.000    0.000    0.000    0.000 typing.py:1737(cast)\n",
       "        3    0.000    0.000    0.000    0.000 threading.py:1169(is_alive)\n",
       "       81    0.000    0.000    0.000    0.000 lookups.py:220(get_table)\n",
       "       61    0.000    0.000    0.000    0.000 myUtils.py:223(desired_confidence_factor)\n",
       "      558    0.000    0.000    0.000    0.000 doc.pyx:45(bounds_check)\n",
       "       81    0.000    0.000    0.000    0.000 {spacy.strings.get_string_id}\n",
       "        4    0.000    0.000    0.000    0.000 tok2vec.py:283(forward)\n",
       "        6    0.000    0.000    0.000    0.000 shape_base.py:23(atleast_1d)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_base.py:435(<listcomp>)\n",
       "       20    0.000    0.000    0.000    0.000 re.py:269(escape)\n",
       "        4    0.000    0.000    0.000    0.000 abc.py:117(__instancecheck__)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_base.py:182(get_sample_fns)\n",
       "        2    0.000    0.000    0.000    0.000 language.py:331(<listcomp>)\n",
       "      288    0.000    0.000    0.000    0.000 {built-in method builtins.id}\n",
       "        8    0.000    0.000    0.000    0.000 shape_base.py:218(_vhstack_dispatcher)\n",
       "       14    0.000    0.000    0.000    0.000 typing.py:306(inner)\n",
       "        8    0.000    0.000    0.000    0.000 __init__.py:131(get_current_ops)\n",
       "       12    0.000    0.000    0.000    0.000 types.py:1171(dataXd)\n",
       "        4    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(squeeze)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_base.py:232(get_anchor_from_tuple)\n",
       "        8    0.000    0.000    0.000    0.000 iostream.py:429(_is_master_process)\n",
       "        2    0.000    0.000    0.000    0.000 errors.py:6(__getattribute__)\n",
       "        2    0.000    0.000    0.000    0.000 myUtils.py:206(update_anchor)\n",
       "      146    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}\n",
       "        2    0.000    0.000    0.000    0.000 doc.pyx:104(values)\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
       "        4    0.000    0.000    0.000    0.000 util.py:1127(get_cuda_stream)\n",
       "        4    0.000    0.000    0.000    0.000 ops.py:468(alloc2f)\n",
       "        2    0.000    0.000    0.000    0.000 tokenizer.pyx:238(_apply_special_cases)\n",
       "        2    0.000    0.000    0.000    0.000 myUtils.py:248(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(atleast_2d)\n",
       "      304    0.000    0.000    0.000    0.000 {method 'items' of 'collections.OrderedDict' objects}\n",
       "        6    0.000    0.000    0.000    0.000 codecs.py:327(reset)\n",
       "        2    0.000    0.000    0.000    0.000 tagger.pyx:149(_scores2guesses)\n",
       "       14    0.000    0.000    0.000    0.000 util.py:96(is_numpy_array)\n",
       "       30    0.000    0.000    0.000    0.000 {method 'swapaxes' of 'numpy.ndarray' objects}\n",
       "       65    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}\n",
       "       24    0.000    0.000    0.000    0.000 numpy_ops.pyx:87(cblas)\n",
       "        4    0.000    0.000    0.000    0.000 _ufunc_config.py:131(geterr)\n",
       "       90    0.000    0.000    0.000    0.000 {built-in method numpy.asanyarray}\n",
       "       96    0.000    0.000    0.000    0.000 stringsource:665(memoryview_check)\n",
       "       61    0.000    0.000    0.000    0.000 anchor_base.py:299(<lambda>)\n",
       "       42    0.000    0.000    0.000    0.000 {built-in method numpy.core._multiarray_umath.normalize_axis_index}\n",
       "      112    0.000    0.000    0.000    0.000 stringsource:374(__dealloc__)\n",
       "       20    0.000    0.000    0.000    0.000 {method 'translate' of 'str' objects}\n",
       "       68    0.000    0.000    0.000    0.000 myUtils.py:215(should_calculate)\n",
       "        2    0.000    0.000    0.000    0.000 tok2vec.py:128(set_annotations)\n",
       "        4    0.000    0.000    0.000    0.000 util.py:484(partial)\n",
       "        8    0.000    0.000    0.000    0.000 shape_base.py:207(_arrays_for_stack_dispatcher)\n",
       "        3    0.000    0.000    0.000    0.000 iostream.py:90(_event_pipe)\n",
       "      160    0.000    0.000    0.000    0.000 vocab.pxd:28(__get__)\n",
       "        2    0.000    0.000    0.000    0.000 tokenizer.pyx:528(find_infix)\n",
       "        2    0.000    0.000    0.000    0.000 myUtils.py:155(update)\n",
       "        4    0.000    0.000    0.000    0.000 fromnumeric.py:1478(squeeze)\n",
       "        2    0.000    0.000    0.000    0.000 _ufunc_config.py:434(__exit__)\n",
       "        2    0.000    0.000    0.000    0.000 re.py:288(_compile)\n",
       "        2    0.000    0.000    0.000    0.000 {method 'sort' of 'list' objects}\n",
       "       60    0.000    0.000    0.000    0.000 {built-in method unicodedata.normalize}\n",
       "        4    0.000    0.000    0.000    0.000 _beam_utils.pyx:199(collect_states)\n",
       "       10    0.000    0.000    0.000    0.000 util.py:151(to_numpy)\n",
       "        2    0.000    0.000    0.000    0.000 doc.pyx:1020(_realloc)\n",
       "       29    0.000    0.000    0.000    0.000 {built-in method builtins.any}\n",
       "        4    0.000    0.000    0.000    0.000 {method 'format' of 'str' objects}\n",
       "        4    0.000    0.000    0.000    0.000 doc.pyx:1785(set_children_from_heads)\n",
       "        3    0.000    0.000    0.000    0.000 threading.py:1102(_wait_for_tstate_lock)\n",
       "        2    0.000    0.000    0.000    0.000 tokenizer.pyx:556(find_suffix)\n",
       "      116    0.000    0.000    0.000    0.000 doc.pyx:498(__len__)\n",
       "        2    0.000    0.000    0.000    0.000 shape_base.py:81(atleast_2d)\n",
       "       14    0.000    0.000    0.000    0.000 ops.py:296(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 tokenization_utils.py:243(cut_text)\n",
       "        4    0.000    0.000    0.000    0.000 _param_server.py:17(__init__)\n",
       "       24    0.000    0.000    0.000    0.000 multiarray.py:148(concatenate)\n",
       "        2    0.000    0.000    0.000    0.000 tokenizer.pyx:542(find_prefix)\n",
       "        2    0.000    0.000    0.000    0.000 util.py:593(from_array)\n",
       "        2    0.000    0.000    0.000    0.000 vocab.pyx:142(get)\n",
       "        2    0.000    0.000    0.000    0.000 tagger.pyx:113(labels)\n",
       "       65    0.000    0.000    0.000    0.000 {method 'strip' of 'str' objects}\n",
       "        6    0.000    0.000    0.000    0.000 {built-in method fromkeys}\n",
       "        4    0.000    0.000    0.000    0.000 doc.pyx:1818(_set_lr_kids_and_edges)\n",
       "        2    0.000    0.000    0.000    0.000 enum.py:451(__members__)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_base.py:238(<lambda>)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_base.py:334(<listcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 model.py:355(walk)\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method numpy.seterrobj}\n",
       "        8    0.000    0.000    0.000    0.000 {built-in method posix.getpid}\n",
       "       30    0.000    0.000    0.000    0.000 fromnumeric.py:546(_swapaxes_dispatcher)\n",
       "        6    0.000    0.000    0.000    0.000 with_array.py:71(<listcomp>)\n",
       "       64    0.000    0.000    0.000    0.000 trainable_pipe.pxd:5(__get__)\n",
       "        2    0.000    0.000    0.000    0.000 dep_parser.pyx:295(__get__)\n",
       "        2    0.000    0.000    0.000    0.000 tokenization_utils.py:498(<dictcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 {method 'squeeze' of 'numpy.ndarray' objects}\n",
       "        4    0.000    0.000    0.000    0.000 list2ragged.py:25(<listcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1274(<listcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 tok2vec.py:121(<genexpr>)\n",
       "        2    0.000    0.000    0.000    0.000 util.py:227(__init__)\n",
       "        4    0.000    0.000    0.000    0.000 numpy_ops.pyx:51(__init__)\n",
       "        4    0.000    0.000    0.000    0.000 list2array.py:22(<listcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 concatenate.py:59(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_explanation.py:54(coverage)\n",
       "        4    0.000    0.000    0.000    0.000 trainable_pipe.pxd:6(__get__)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}\n",
       "        8    0.000    0.000    0.000    0.000 {method 'write' of '_io.StringIO' objects}\n",
       "        8    0.000    0.000    0.000    0.000 {built-in method builtins.setattr}\n",
       "       16    0.000    0.000    0.000    0.000 stringsource:596(__get__)\n",
       "        8    0.000    0.000    0.000    0.000 {built-in method numpy.geterrobj}\n",
       "        4    0.000    0.000    0.000    0.000 model.py:157(has_dim)\n",
       "        2    0.000    0.000    0.000    0.000 matcher.pyx:148(_require_patterns)\n",
       "       10    0.000    0.000    0.000    0.000 fromnumeric.py:2493(_cumsum_dispatcher)\n",
       "        4    0.000    0.000    0.000    0.000 ops.py:340(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_text.py:286(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 tagger.pyx:137(genexpr)\n",
       "       16    0.000    0.000    0.000    0.000 {method 'get' of '_contextvars.ContextVar' objects}\n",
       "        4    0.000    0.000    0.000    0.000 tokenization_distilbert.py:183(do_lower_case)\n",
       "       10    0.000    0.000    0.000    0.000 shape_base.py:795(_split_dispatcher)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_explanation.py:12(names)\n",
       "        3    0.000    0.000    0.000    0.000 threading.py:553(is_set)\n",
       "       16    0.000    0.000    0.000    0.000 stringsource:978(__dealloc__)\n",
       "        8    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}\n",
       "        2    0.000    0.000    0.000    0.000 <string>:2(__init__)\n",
       "        6    0.000    0.000    0.000    0.000 codecs.py:276(reset)\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}\n",
       "        2    0.000    0.000    0.000    0.000 {method 'union' of 'set' objects}\n",
       "        2    0.000    0.000    0.000    0.000 doc.pyx:914(__pyx_fuse_1push_back)\n",
       "       16    0.000    0.000    0.000    0.000 stringsource:561(__get__)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_explanation.py:7(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
       "       10    0.000    0.000    0.000    0.000 shape_base.py:735(_array_split_dispatcher)\n",
       "        6    0.000    0.000    0.000    0.000 shape_base.py:19(_atleast_1d_dispatcher)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_explanation.py:38(precision)\n",
       "        6    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
       "        6    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
       "        4    0.000    0.000    0.000    0.000 trainable_pipe.pxd:7(__get__)\n",
       "        4    0.000    0.000    0.000    0.000 tokenizer.pyx:86(__get__)\n",
       "        4    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}\n",
       "        2    0.000    0.000    0.000    0.000 matcher.pyx:64(__len__)\n",
       "        2    0.000    0.000    0.000    0.000 softmax.py:113(validate_temperature)\n",
       "        2    0.000    0.000    0.000    0.000 dep_parser.pyx:341(_ensure_labels_are_added)\n",
       "        8    0.000    0.000    0.000    0.000 doc.pxd:44(__get__)\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method time.time}\n",
       "        4    0.000    0.000    0.000    0.000 fromnumeric.py:1474(_squeeze_dispatcher)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
       "        2    0.000    0.000    0.000    0.000 tokenization_utils.py:821(prepare_for_tokenization)\n",
       "        2    0.000    0.000    0.000    0.000 doc.pxd:44(__set__)\n",
       "        2    0.000    0.000    0.000    0.000 shape_base.py:77(_atleast_2d_dispatcher)\n",
       "        2    0.000    0.000    0.000    0.000 contextlib.py:63(_recreate_cm)\n",
       "        4    0.000    0.000    0.000    0.000 tokenizer.pyx:110(__get__)\n",
       "        4    0.000    0.000    0.000    0.000 tokenizer.pyx:94(__get__)\n",
       "        4    0.000    0.000    0.000    0.000 tokenizer.pyx:102(__get__)\n",
       "        4    0.000    0.000    0.000    0.000 tokenizer.pyx:78(__get__)\n",
       "        2    0.000    0.000    0.000    0.000 tokenizer.pyx:507(_save_cached)\n",
       "        2    0.000    0.000    0.000    0.000 matcher.pyx:545(finish_states)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch._C._jit_set_texpr_fuser_enabled(False)\n",
    "my_utils = TextUtils(examples, explainer, predict_sentences, ignored, optimize=optimize)\n",
    "myUtils.model = model\n",
    "myUtils.tokenizer = tokenizer\n",
    "anchor_base.AnchorBaseBeam.best_group = bg\n",
    "set_seed()\n",
    "%prun -s cumtime -T check/profile.txt my_utils.compute_explanations(list(range(len(examples))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "5e737695",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number 0\n",
      "number 1\n",
      "[0.9702970297029703]\n",
      "[0.9803921568627451]\n",
      " \n",
      "*** Profile printout saved to text file 'check/profile.txt'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "         500475 function calls (500079 primitive calls) in 2.554 seconds\n",
       "\n",
       "   Ordered by: cumulative time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "        1    0.000    0.000    2.576    2.576 {built-in method builtins.exec}\n",
       "        1    0.000    0.000    2.576    2.576 <string>:1(<module>)\n",
       "        1    0.000    0.000    2.576    2.576 myUtils.py:267(compute_explanations)\n",
       "        2    0.098    0.049    2.575    1.288 myUtils.py:264(get_exp)\n",
       "        2    0.000    0.000    2.477    1.239 anchor_text.py:267(explain_instance)\n",
       "        2    0.001    0.001    2.283    1.142 anchor_base.py:255(anchor_beam)\n",
       "      158    0.013    0.000    2.270    0.014 anchor_text.py:227(sample_fn)\n",
       "      154    0.000    0.000    2.260    0.015 anchor_base.py:217(<lambda>)\n",
       "      154    0.005    0.000    2.260    0.015 anchor_base.py:186(complete_sample_fn)\n",
       "      158    0.019    0.000    2.002    0.013 anchor_text.py:120(sample)\n",
       "      226    0.004    0.000    1.478    0.007 anchor_text.py:155(probs)\n",
       "      226    0.297    0.001    1.234    0.005 anchor_text.py:47(unmask)\n",
       "     1040    0.022    0.000    0.640    0.001 anchor_text.py:141(<listcomp>)\n",
       "      384    0.623    0.002    0.623    0.002 module.py:1494(_call_impl)\n",
       "    28581    0.536    0.000    0.615    0.000 {method 'choice' of 'numpy.random._generator.Generator' objects}\n",
       "     1093    0.400    0.000    0.400    0.000 {method 'tolist' of 'torch._C._TensorBase' objects}\n",
       "        2    0.000    0.000    0.282    0.141 anchor_base.py:71(lucb)\n",
       "      158    0.001    0.000    0.252    0.002 myUtils.py:32(predict_sentences)\n",
       "     1093    0.023    0.000    0.240    0.000 anchor_text.py:168(<listcomp>)\n",
       "    28305    0.135    0.000    0.217    0.000 anchor_text.py:24(exp_normalize)\n",
       "        2    0.000    0.000    0.193    0.097 anchor_text.py:217(get_sample_fn)\n",
       "        2    0.004    0.002    0.167    0.083 anchor_text.py:91(__init__)\n",
       "    28305    0.081    0.000    0.081    0.000 serialize.py:29(_numba_unpickle)\n",
       "     1093    0.070    0.000    0.070    0.000 {built-in method torch.topk}\n",
       "    28581    0.019    0.000    0.046    0.000 numerictypes.py:356(issubdtype)\n",
       "    57162    0.027    0.000    0.032    0.000 getlimits.py:458(__new__)\n",
       "     1251    0.026    0.000    0.026    0.000 {method 'cpu' of 'torch._C._TensorBase' objects}\n",
       "    57162    0.017    0.000    0.025    0.000 numerictypes.py:282(issubclass_)\n",
       "      158    0.001    0.000    0.024    0.000 myUtils.py:33(<listcomp>)\n",
       "      384    0.022    0.000    0.022    0.000 {built-in method torch.tensor}\n",
       "        2    0.000    0.000    0.018    0.009 language.py:978(__call__)\n",
       "        8    0.002    0.000    0.015    0.002 trainable_pipe.pyx:40(__call__)\n",
       "     1331    0.014    0.000    0.014    0.000 {method 'join' of 'str' objects}\n",
       "        8    0.000    0.000    0.012    0.002 model.py:311(predict)\n",
       "   270/16    0.000    0.000    0.012    0.001 model.py:288(__call__)\n",
       "     94/8    0.000    0.000    0.012    0.001 chain.py:49(forward)\n",
       "       14    0.000    0.000    0.010    0.001 with_array.py:28(forward)\n",
       "    85825    0.010    0.000    0.010    0.000 {built-in method builtins.issubclass}\n",
       "       68    0.008    0.000    0.008    0.000 anchor_text.py:113(<listcomp>)\n",
       "     1251    0.007    0.000    0.007    0.000 {method 'numpy' of 'torch._C._TensorBase' objects}\n",
       "      226    0.003    0.000    0.007    0.000 anchor_text.py:55(<listcomp>)\n",
       "        6    0.000    0.000    0.007    0.001 with_array.py:66(_list_forward)\n",
       "        2    0.000    0.000    0.006    0.003 tok2vec.py:112(predict)\n",
       "       16    0.000    0.000    0.006    0.000 residual.py:28(forward)\n",
       "     5302    0.006    0.000    0.006    0.000 {method 'binomial' of 'numpy.random.mtrand.RandomState' objects}\n",
       "    59615    0.006    0.000    0.006    0.000 {method 'get' of 'dict' objects}\n",
       "        4    0.000    0.000    0.006    0.001 tb_framework.py:32(forward)\n",
       "       20    0.000    0.000    0.005    0.000 maxout.py:46(forward)\n",
       "       30    0.004    0.000    0.004    0.000 numpy_ops.pyx:90(gemm)\n",
       "    31916    0.004    0.000    0.004    0.000 {built-in method builtins.len}\n",
       "     1095    0.001    0.000    0.004    0.000 tokenization_utils_base.py:1182(mask_token_id)\n",
       "        8    0.000    0.000    0.004    0.000 with_array.py:83(_ragged_forward)\n",
       "    37502    0.003    0.000    0.003    0.000 {method 'append' of 'list' objects}\n",
       "1150/1094    0.002    0.000    0.003    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
       "      158    0.003    0.000    0.003    0.000 {built-in method torch.argmax}\n",
       "     1095    0.001    0.000    0.003    0.000 tokenization_utils.py:560(convert_tokens_to_ids)\n",
       "        2    0.000    0.000    0.003    0.001 tokenization_utils.py:481(tokenize)\n",
       "     2216    0.002    0.000    0.002    0.000 {method 'copy' of 'numpy.ndarray' objects}\n",
       "       20    0.000    0.000    0.002    0.000 layernorm.py:24(forward)\n",
       "     1095    0.001    0.000    0.002    0.000 tokenization_utils.py:582(_convert_token_to_id_with_added_voc)\n",
       "        2    0.000    0.000    0.002    0.001 attributeruler.py:133(__call__)\n",
       "     1046    0.001    0.000    0.002    0.000 <__array_function__ internals>:177(where)\n",
       "      382    0.001    0.000    0.002    0.000 grad_mode.py:57(__exit__)\n",
       "      369    0.000    0.000    0.002    0.000 {method 'sum' of 'numpy.ndarray' objects}\n",
       "        2    0.000    0.000    0.002    0.001 tokenization_distilbert.py:198(_tokenize)\n",
       "      431    0.002    0.000    0.002    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
       "      764    0.001    0.000    0.002    0.000 grad_mode.py:149(__init__)\n",
       "      161    0.001    0.000    0.002    0.000 anchor_base.py:52(dlow_bernoulli)\n",
       "      369    0.000    0.000    0.002    0.000 _methods.py:46(_sum)\n",
       "      382    0.001    0.000    0.002    0.000 grad_mode.py:53(__enter__)\n",
       "        2    0.000    0.000    0.001    0.001 tokenization_distilbert.py:363(tokenize)\n",
       "        4    0.000    0.000    0.001    0.000 concatenate.py:43(forward)\n",
       "       20    0.000    0.000    0.001    0.000 layernorm.py:73(_get_moments)\n",
       "        2    0.000    0.000    0.001    0.001 attributeruler.py:149(match)\n",
       "      320    0.001    0.000    0.001    0.000 anchor_base.py:33(kl_bernoulli)\n",
       "     1095    0.001    0.000    0.001    0.000 tokenization_distilbert.py:212(_convert_token_to_id)\n",
       "        2    0.000    0.000    0.001    0.001 matcher.pyx:211(__call__)\n",
       "        2    0.000    0.000    0.001    0.001 matcher.pyx:342(find_matches)\n",
       "      382    0.001    0.000    0.001    0.000 grad_mode.py:48(__init__)\n",
       "       24    0.000    0.000    0.001    0.000 <__array_function__ internals>:177(concatenate)\n",
       "        4    0.000    0.000    0.001    0.000 concatenate.py:44(<listcomp>)\n",
       "       62    0.000    0.000    0.001    0.000 matcher.pyx:413(transition_states)\n",
       "      159    0.000    0.000    0.001    0.000 anchor_base.py:40(dup_bernoulli)\n",
       "       20    0.000    0.000    0.001    0.000 hashembed.py:60(forward)\n",
       "      176    0.001    0.000    0.001    0.000 {built-in method numpy.zeros}\n",
       "      394    0.001    0.000    0.001    0.000 {built-in method numpy.array}\n",
       "        4    0.000    0.000    0.001    0.000 {built-in method builtins.print}\n",
       "     1187    0.001    0.000    0.001    0.000 {built-in method builtins.min}\n",
       "        8    0.000    0.000    0.001    0.000 iostream.py:518(write)\n",
       "        8    0.000    0.000    0.001    0.000 iostream.py:448(_schedule_flush)\n",
       "     1054    0.000    0.000    0.001    0.000 matcher.pyx:525(update_predicate_cache)\n",
       "        3    0.000    0.000    0.001    0.000 iostream.py:202(schedule)\n",
       "       20    0.001    0.000    0.001    0.000 numpy_ops.pyx:164(__pyx_fuse_0maxout)\n",
       "     1215    0.001    0.000    0.001    0.000 {method 'nonzero' of 'numpy.ndarray' objects}\n",
       "      384    0.001    0.000    0.001    0.000 {built-in method torch._C._get_tracing_state}\n",
       "        3    0.001    0.000    0.001    0.000 socket.py:543(send)\n",
       "       20    0.000    0.000    0.001    0.000 {method 'var' of 'numpy.ndarray' objects}\n",
       "        2    0.000    0.000    0.001    0.000 anchor_base.py:125(make_tuples)\n",
       "        2    0.000    0.000    0.001    0.000 <__array_function__ internals>:177(vstack)\n",
       "        2    0.000    0.000    0.001    0.000 shape_base.py:222(vstack)\n",
       "       20    0.000    0.000    0.001    0.000 _methods.py:196(_var)\n",
       "       22    0.000    0.000    0.001    0.000 {method 'mean' of 'numpy.ndarray' objects}\n",
       "        2    0.000    0.000    0.001    0.000 lemmatizer.py:121(__call__)\n",
       "       10    0.000    0.000    0.001    0.000 ops.py:329(unflatten)\n",
       "       22    0.000    0.000    0.001    0.000 _methods.py:163(_mean)\n",
       "        2    0.000    0.000    0.001    0.000 tokenization_distilbert.py:470(_clean_text)\n",
       "     1146    0.001    0.000    0.001    0.000 {built-in method torch.is_grad_enabled}\n",
       "        6    0.000    0.000    0.001    0.000 <__array_function__ internals>:177(hstack)\n",
       "        2    0.000    0.000    0.000    0.000 re.py:202(sub)\n",
       "        6    0.000    0.000    0.000    0.000 shape_base.py:285(hstack)\n",
       "        2    0.000    0.000    0.000    0.000 {method 'sub' of 're.Pattern' objects}\n",
       "        4    0.000    0.000    0.000    0.000 _precomputable_affine.py:19(forward)\n",
       "       60    0.000    0.000    0.000    0.000 tokenization_distilbert.py:411(_run_split_on_punc)\n",
       "       53    0.000    0.000    0.000    0.000 lemmatizer.py:196(rule_lemmatize)\n",
       "        2    0.000    0.000    0.000    0.000 attributeruler.py:158(set_annotations)\n",
       "       16    0.000    0.000    0.000    0.000 expand_window.py:19(forward)\n",
       "     1321    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1013(unk_token)\n",
       "       16    0.000    0.000    0.000    0.000 expand_window.py:26(_expand_window_floats)\n",
       "      156    0.000    0.000    0.000    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
       "       16    0.000    0.000    0.000    0.000 numpy_ops.pyx:227(seq2col)\n",
       "        2    0.000    0.000    0.000    0.000 language.py:1084(_ensure_doc)\n",
       "        4    0.000    0.000    0.000    0.000 model.py:849(set_dropout_rate)\n",
       "        2    0.000    0.000    0.000    0.000 language.py:1072(make_doc)\n",
       "        2    0.000    0.000    0.000    0.000 tagger.pyx:129(predict)\n",
       "        4    0.000    0.000    0.000    0.000 concatenate.py:56(_array_forward)\n",
       "        2    0.000    0.000    0.000    0.000 tokenizer.pyx:147(__call__)\n",
       "      992    0.000    0.000    0.000    0.000 matcher.pyx:917(__call__)\n",
       "       20    0.000    0.000    0.000    0.000 numpy_ops.pyx:440(__pyx_fuse_0_1gather_add)\n",
       "        2    0.000    0.000    0.000    0.000 tokenizer.pyx:160(_tokenize_affixes)\n",
       "      136    0.000    0.000    0.000    0.000 model.py:211(get_param)\n",
       "       54    0.000    0.000    0.000    0.000 numpy_ops.pyx:64(asarray)\n",
       "     1097    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1059(mask_token)\n",
       "      957    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
       "       10    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(split)\n",
       "       10    0.000    0.000    0.000    0.000 shape_base.py:799(split)\n",
       "        4    0.000    0.000    0.000    0.000 ragged2list.py:18(forward)\n",
       "      764    0.000    0.000    0.000    0.000 {built-in method torch._C._set_grad_enabled}\n",
       "       14    0.000    0.000    0.000    0.000 ops.py:284(flatten)\n",
       "       10    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(array_split)\n",
       "       20    0.000    0.000    0.000    0.000 layernorm.py:59(_begin_update_scale_shift)\n",
       "       61    0.000    0.000    0.000    0.000 tokenization_distilbert.py:493(tokenize)\n",
       "      148    0.000    0.000    0.000    0.000 model.py:370(_walk_bfs)\n",
       "      558    0.000    0.000    0.000    0.000 doc.pyx:452(__getitem__)\n",
       "      154    0.000    0.000    0.000    0.000 {method 'update' of 'set' objects}\n",
       "       10    0.000    0.000    0.000    0.000 shape_base.py:739(array_split)\n",
       "        2    0.000    0.000    0.000    0.000 tokenization_distilbert.py:433(_tokenize_chinese_chars)\n",
       "      243    0.000    0.000    0.000    0.000 tokenization_utils.py:292(_is_punctuation)\n",
       "      302    0.000    0.000    0.000    0.000 tokenization_utils.py:514(<lambda>)\n",
       "        2    0.000    0.000    0.000    0.000 {numpy.random._generator.default_rng}\n",
       "      302    0.000    0.000    0.000    0.000 tokenization_utils.py:280(_is_control)\n",
       "        4    0.000    0.000    0.000    0.000 linear.py:36(forward)\n",
       "       30    0.000    0.000    0.000    0.000 ops.py:667(asarray1i)\n",
       "        4    0.000    0.000    0.000    0.000 list2ragged.py:21(forward)\n",
       "     1475    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
       "       80    0.000    0.000    0.000    0.000 numpy_ops.pyx:81(alloc)\n",
       "     1046    0.000    0.000    0.000    0.000 multiarray.py:341(where)\n",
       "       60    0.000    0.000    0.000    0.000 tokenization_distilbert.py:400(_run_strip_accents)\n",
       "       45    0.000    0.000    0.000    0.000 lemmatizer.py:8(is_base_form)\n",
       "        4    0.000    0.000    0.000    0.000 featureextractor.py:13(forward)\n",
       "       20    0.000    0.000    0.000    0.000 ops.py:659(asarray_f)\n",
       "       42    0.000    0.000    0.000    0.000 _methods.py:66(_count_reduce_items)\n",
       "       10    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(cumsum)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_base.py:137(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 tokenization_utils.py:90(split)\n",
       "       40    0.000    0.000    0.000    0.000 fromnumeric.py:51(_wrapfunc)\n",
       "        2    0.000    0.000    0.000    0.000 softmax.py:64(forward)\n",
       "      124    0.000    0.000    0.000    0.000 matcher.pyx:881(__call__)\n",
       "        2    0.000    0.000    0.000    0.000 doc.pyx:180(__init__)\n",
       "       60    0.000    0.000    0.000    0.000 ops.py:621(reshape)\n",
       "        2    0.000    0.000    0.000    0.000 contextlib.py:76(inner)\n",
       "       10    0.000    0.000    0.000    0.000 fromnumeric.py:2497(cumsum)\n",
       "       84    0.000    0.000    0.000    0.000 ops.py:723(as_contig)\n",
       "      301    0.000    0.000    0.000    0.000 tokenization_utils.py:268(_is_whitespace)\n",
       "        4    0.000    0.000    0.000    0.000 model.py:66(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 tagger.pyx:158(set_annotations)\n",
       "      136    0.000    0.000    0.000    0.000 _param_server.py:39(has_param)\n",
       "        2    0.000    0.000    0.000    0.000 ops.py:224(affine)\n",
       "       16    0.000    0.000    0.000    0.000 numpy_ops.pyx:515(check_seq2col_lengths)\n",
       "        6    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1277(all_special_tokens_extended)\n",
       "       30    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(swapaxes)\n",
       "       59    0.000    0.000    0.000    0.000 myUtils.py:212(update_normal)\n",
       "        4    0.000    0.000    0.000    0.000 list2array.py:21(forward)\n",
       "        4    0.000    0.000    0.000    0.000 {method 'to_array' of 'spacy.tokens.doc.Doc' objects}\n",
       "       96    0.000    0.000    0.000    0.000 stringsource:659(memoryview_cwrapper)\n",
       "      382    0.000    0.000    0.000    0.000 _jit_internal.py:1102(is_scripting)\n",
       "        4    0.000    0.000    0.000    0.000 doc.pyx:938(to_array (wrapper))\n",
       "     1031    0.000    0.000    0.000    0.000 {built-in method unicodedata.category}\n",
       "        2    0.000    0.000    0.000    0.000 myUtils.py:242(monitor)\n",
       "      614    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}\n",
       "       98    0.000    0.000    0.000    0.000 {spacy.tokens._retokenize.set_token_attrs}\n",
       "      806    0.000    0.000    0.000    0.000 token.pxd:21(cinit)\n",
       "        4    0.000    0.000    0.000    0.000 doc.pyx:938(to_array)\n",
       "        2    0.000    0.000    0.000    0.000 attributeruler.py:152(<listcomp>)\n",
       "       10    0.000    0.000    0.000    0.000 {method 'cumsum' of 'numpy.ndarray' objects}\n",
       "      256    0.000    0.000    0.000    0.000 doc.pyx:485(__iter__)\n",
       "       45    0.000    0.000    0.000    0.000 {method 'to_dict' of 'spacy.tokens.morphanalysis.MorphAnalysis' objects}\n",
       "       81    0.000    0.000    0.000    0.000 lookups.py:109(get)\n",
       "        2    0.000    0.000    0.000    0.000 language.py:323(pipeline)\n",
       "       20    0.000    0.000    0.000    0.000 numpy_ops.pyx:440(gather_add)\n",
       "       92    0.000    0.000    0.000    0.000 {method 'reshape' of 'numpy.ndarray' objects}\n",
       "      301    0.000    0.000    0.000    0.000 tokenization_distilbert.py:446(_is_chinese_char)\n",
       "       20    0.000    0.000    0.000    0.000 numpy_ops.pyx:303(hash)\n",
       "        2    0.000    0.000    0.000    0.000 _dict_proxies.py:26(__init__)\n",
       "       62    0.000    0.000    0.000    0.000 tokenizer.pyx:359(_try_specials_and_cache)\n",
       "        2    0.000    0.000    0.000    0.000 tokenizer.pyx:388(_tokenize)\n",
       "       20    0.000    0.000    0.000    0.000 ops.py:592(reshape2f)\n",
       "      296    0.000    0.000    0.000    0.000 model.py:116(attrs)\n",
       "       20    0.000    0.000    0.000    0.000 ops.py:589(reshape1f)\n",
       "       20    0.000    0.000    0.000    0.000 numpy_ops.pyx:164(maxout)\n",
       "      276    0.000    0.000    0.000    0.000 model.py:105(layers)\n",
       "       12    0.000    0.000    0.000    0.000 types.py:1162(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_text.py:101(<listcomp>)\n",
       "       20    0.000    0.000    0.000    0.000 dropout.py:20(forward)\n",
       "       96    0.000    0.000    0.000    0.000 model.py:168(get_dim)\n",
       "       30    0.000    0.000    0.000    0.000 fromnumeric.py:550(swapaxes)\n",
       "      136    0.000    0.000    0.000    0.000 _param_server.py:45(get_param)\n",
       "       98    0.000    0.000    0.000    0.000 _retokenize.pyx:455(set_token_attrs)\n",
       "       65    0.000    0.000    0.000    0.000 tokenization_distilbert.py:80(whitespace_tokenize)\n",
       "      846    0.000    0.000    0.000    0.000 {built-in method builtins.ord}\n",
       "        4    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1267(all_special_tokens)\n",
       "       20    0.000    0.000    0.000    0.000 array_getitem.py:38(forward)\n",
       "      497    0.000    0.000    0.000    0.000 {method 'lower' of 'str' objects}\n",
       "        6    0.000    0.000    0.000    0.000 {method 'writerow' of '_csv.writer' objects}\n",
       "        2    0.000    0.000    0.000    0.000 __init__.py:1091(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 doc.pyx:766(set_ents)\n",
       "       20    0.000    0.000    0.000    0.000 ops.py:595(reshape3f)\n",
       "        6    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1251(special_tokens_map_extended)\n",
       "      604    0.000    0.000    0.000    0.000 {method 'groups' of 're.Match' objects}\n",
       "       42    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
       "        2    0.000    0.000    0.000    0.000 arc_eager.pyx:744(set_annotations)\n",
       "        2    0.000    0.000    0.000    0.000 {function SeedSequence.generate_state at 0x7f2d10265750}\n",
       "        2    0.000    0.000    0.000    0.000 _ufunc_config.py:429(__enter__)\n",
       "      112    0.000    0.000    0.000    0.000 stringsource:346(__cinit__)\n",
       "        6    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(atleast_1d)\n",
       "        4    0.000    0.000    0.000    0.000 _ufunc_config.py:32(seterr)\n",
       "       20    0.000    0.000    0.000    0.000 {built-in method numpy.ascontiguousarray}\n",
       "        2    0.000    0.000    0.000    0.000 tokenization_utils.py:510(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 _collections_abc.py:991(update)\n",
       "      267    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}\n",
       "        2    0.000    0.000    0.000    0.000 nonproj.pyx:172(deprojectivize (wrapper))\n",
       "        4    0.000    0.000    0.000    0.000 ops.py:672(asarray2i)\n",
       "       32    0.000    0.000    0.000    0.000 stringsource:1001(memoryview_fromslice)\n",
       "        2    0.000    0.000    0.000    0.000 nonproj.pyx:172(deprojectivize)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_text.py:222(<listcomp>)\n",
       "       60    0.000    0.000    0.000    0.000 doc.pyx:914(__pyx_fuse_0push_back)\n",
       "        2    0.000    0.000    0.000    0.000 tokenizer.pyx:445(_attach_tokens)\n",
       "       14    0.000    0.000    0.000    0.000 util.py:49(get_array_module)\n",
       "       88    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
       "        2    0.000    0.000    0.000    0.000 anchor_base.py:221(get_initial_statistics)\n",
       "       60    0.000    0.000    0.000    0.000 tokenization_distilbert.py:431(<listcomp>)\n",
       "       24    0.000    0.000    0.000    0.000 pipe.pyx:133(get_error_handler)\n",
       "        2    0.000    0.000    0.000    0.000 tokenizer.pyx:400(_split_affixes)\n",
       "      284    0.000    0.000    0.000    0.000 typing.py:1737(cast)\n",
       "        3    0.000    0.000    0.000    0.000 threading.py:1169(is_alive)\n",
       "       81    0.000    0.000    0.000    0.000 lookups.py:220(get_table)\n",
       "       61    0.000    0.000    0.000    0.000 myUtils.py:223(desired_confidence_factor)\n",
       "      558    0.000    0.000    0.000    0.000 doc.pyx:45(bounds_check)\n",
       "       81    0.000    0.000    0.000    0.000 {spacy.strings.get_string_id}\n",
       "        4    0.000    0.000    0.000    0.000 tok2vec.py:283(forward)\n",
       "        6    0.000    0.000    0.000    0.000 shape_base.py:23(atleast_1d)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_base.py:435(<listcomp>)\n",
       "       20    0.000    0.000    0.000    0.000 re.py:269(escape)\n",
       "        4    0.000    0.000    0.000    0.000 abc.py:117(__instancecheck__)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_base.py:182(get_sample_fns)\n",
       "        2    0.000    0.000    0.000    0.000 language.py:331(<listcomp>)\n",
       "      288    0.000    0.000    0.000    0.000 {built-in method builtins.id}\n",
       "        8    0.000    0.000    0.000    0.000 shape_base.py:218(_vhstack_dispatcher)\n",
       "       14    0.000    0.000    0.000    0.000 typing.py:306(inner)\n",
       "        8    0.000    0.000    0.000    0.000 __init__.py:131(get_current_ops)\n",
       "       12    0.000    0.000    0.000    0.000 types.py:1171(dataXd)\n",
       "        4    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(squeeze)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_base.py:232(get_anchor_from_tuple)\n",
       "        8    0.000    0.000    0.000    0.000 iostream.py:429(_is_master_process)\n",
       "        2    0.000    0.000    0.000    0.000 errors.py:6(__getattribute__)\n",
       "        2    0.000    0.000    0.000    0.000 myUtils.py:206(update_anchor)\n",
       "      146    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}\n",
       "        2    0.000    0.000    0.000    0.000 doc.pyx:104(values)\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
       "        4    0.000    0.000    0.000    0.000 util.py:1127(get_cuda_stream)\n",
       "        4    0.000    0.000    0.000    0.000 ops.py:468(alloc2f)\n",
       "        2    0.000    0.000    0.000    0.000 tokenizer.pyx:238(_apply_special_cases)\n",
       "        2    0.000    0.000    0.000    0.000 myUtils.py:248(__init__)\n",
       "        2    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(atleast_2d)\n",
       "      304    0.000    0.000    0.000    0.000 {method 'items' of 'collections.OrderedDict' objects}\n",
       "        6    0.000    0.000    0.000    0.000 codecs.py:327(reset)\n",
       "        2    0.000    0.000    0.000    0.000 tagger.pyx:149(_scores2guesses)\n",
       "       14    0.000    0.000    0.000    0.000 util.py:96(is_numpy_array)\n",
       "       30    0.000    0.000    0.000    0.000 {method 'swapaxes' of 'numpy.ndarray' objects}\n",
       "       65    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}\n",
       "       24    0.000    0.000    0.000    0.000 numpy_ops.pyx:87(cblas)\n",
       "        4    0.000    0.000    0.000    0.000 _ufunc_config.py:131(geterr)\n",
       "       90    0.000    0.000    0.000    0.000 {built-in method numpy.asanyarray}\n",
       "       96    0.000    0.000    0.000    0.000 stringsource:665(memoryview_check)\n",
       "       61    0.000    0.000    0.000    0.000 anchor_base.py:299(<lambda>)\n",
       "       42    0.000    0.000    0.000    0.000 {built-in method numpy.core._multiarray_umath.normalize_axis_index}\n",
       "      112    0.000    0.000    0.000    0.000 stringsource:374(__dealloc__)\n",
       "       20    0.000    0.000    0.000    0.000 {method 'translate' of 'str' objects}\n",
       "       68    0.000    0.000    0.000    0.000 myUtils.py:215(should_calculate)\n",
       "        2    0.000    0.000    0.000    0.000 tok2vec.py:128(set_annotations)\n",
       "        4    0.000    0.000    0.000    0.000 util.py:484(partial)\n",
       "        8    0.000    0.000    0.000    0.000 shape_base.py:207(_arrays_for_stack_dispatcher)\n",
       "        3    0.000    0.000    0.000    0.000 iostream.py:90(_event_pipe)\n",
       "      160    0.000    0.000    0.000    0.000 vocab.pxd:28(__get__)\n",
       "        2    0.000    0.000    0.000    0.000 tokenizer.pyx:528(find_infix)\n",
       "        2    0.000    0.000    0.000    0.000 myUtils.py:155(update)\n",
       "        4    0.000    0.000    0.000    0.000 fromnumeric.py:1478(squeeze)\n",
       "        2    0.000    0.000    0.000    0.000 _ufunc_config.py:434(__exit__)\n",
       "        2    0.000    0.000    0.000    0.000 re.py:288(_compile)\n",
       "        2    0.000    0.000    0.000    0.000 {method 'sort' of 'list' objects}\n",
       "       60    0.000    0.000    0.000    0.000 {built-in method unicodedata.normalize}\n",
       "        4    0.000    0.000    0.000    0.000 _beam_utils.pyx:199(collect_states)\n",
       "       10    0.000    0.000    0.000    0.000 util.py:151(to_numpy)\n",
       "        2    0.000    0.000    0.000    0.000 doc.pyx:1020(_realloc)\n",
       "       29    0.000    0.000    0.000    0.000 {built-in method builtins.any}\n",
       "        4    0.000    0.000    0.000    0.000 {method 'format' of 'str' objects}\n",
       "        4    0.000    0.000    0.000    0.000 doc.pyx:1785(set_children_from_heads)\n",
       "        3    0.000    0.000    0.000    0.000 threading.py:1102(_wait_for_tstate_lock)\n",
       "        2    0.000    0.000    0.000    0.000 tokenizer.pyx:556(find_suffix)\n",
       "      116    0.000    0.000    0.000    0.000 doc.pyx:498(__len__)\n",
       "        2    0.000    0.000    0.000    0.000 shape_base.py:81(atleast_2d)\n",
       "       14    0.000    0.000    0.000    0.000 ops.py:296(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 tokenization_utils.py:243(cut_text)\n",
       "        4    0.000    0.000    0.000    0.000 _param_server.py:17(__init__)\n",
       "       24    0.000    0.000    0.000    0.000 multiarray.py:148(concatenate)\n",
       "        2    0.000    0.000    0.000    0.000 tokenizer.pyx:542(find_prefix)\n",
       "        2    0.000    0.000    0.000    0.000 util.py:593(from_array)\n",
       "        2    0.000    0.000    0.000    0.000 vocab.pyx:142(get)\n",
       "        2    0.000    0.000    0.000    0.000 tagger.pyx:113(labels)\n",
       "       65    0.000    0.000    0.000    0.000 {method 'strip' of 'str' objects}\n",
       "        6    0.000    0.000    0.000    0.000 {built-in method fromkeys}\n",
       "        4    0.000    0.000    0.000    0.000 doc.pyx:1818(_set_lr_kids_and_edges)\n",
       "        2    0.000    0.000    0.000    0.000 enum.py:451(__members__)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_base.py:238(<lambda>)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_base.py:334(<listcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 model.py:355(walk)\n",
       "        4    0.000    0.000    0.000    0.000 {built-in method numpy.seterrobj}\n",
       "        8    0.000    0.000    0.000    0.000 {built-in method posix.getpid}\n",
       "       30    0.000    0.000    0.000    0.000 fromnumeric.py:546(_swapaxes_dispatcher)\n",
       "        6    0.000    0.000    0.000    0.000 with_array.py:71(<listcomp>)\n",
       "       64    0.000    0.000    0.000    0.000 trainable_pipe.pxd:5(__get__)\n",
       "        2    0.000    0.000    0.000    0.000 dep_parser.pyx:295(__get__)\n",
       "        2    0.000    0.000    0.000    0.000 tokenization_utils.py:498(<dictcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 {method 'squeeze' of 'numpy.ndarray' objects}\n",
       "        4    0.000    0.000    0.000    0.000 list2ragged.py:25(<listcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 tokenization_utils_base.py:1274(<listcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 tok2vec.py:121(<genexpr>)\n",
       "        2    0.000    0.000    0.000    0.000 util.py:227(__init__)\n",
       "        4    0.000    0.000    0.000    0.000 numpy_ops.pyx:51(__init__)\n",
       "        4    0.000    0.000    0.000    0.000 list2array.py:22(<listcomp>)\n",
       "        4    0.000    0.000    0.000    0.000 concatenate.py:59(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_explanation.py:54(coverage)\n",
       "        4    0.000    0.000    0.000    0.000 trainable_pipe.pxd:6(__get__)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}\n",
       "        8    0.000    0.000    0.000    0.000 {method 'write' of '_io.StringIO' objects}\n",
       "        8    0.000    0.000    0.000    0.000 {built-in method builtins.setattr}\n",
       "       16    0.000    0.000    0.000    0.000 stringsource:596(__get__)\n",
       "        8    0.000    0.000    0.000    0.000 {built-in method numpy.geterrobj}\n",
       "        4    0.000    0.000    0.000    0.000 model.py:157(has_dim)\n",
       "        2    0.000    0.000    0.000    0.000 matcher.pyx:148(_require_patterns)\n",
       "       10    0.000    0.000    0.000    0.000 fromnumeric.py:2493(_cumsum_dispatcher)\n",
       "        4    0.000    0.000    0.000    0.000 ops.py:340(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_text.py:286(<listcomp>)\n",
       "        2    0.000    0.000    0.000    0.000 tagger.pyx:137(genexpr)\n",
       "       16    0.000    0.000    0.000    0.000 {method 'get' of '_contextvars.ContextVar' objects}\n",
       "        4    0.000    0.000    0.000    0.000 tokenization_distilbert.py:183(do_lower_case)\n",
       "       10    0.000    0.000    0.000    0.000 shape_base.py:795(_split_dispatcher)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_explanation.py:12(names)\n",
       "        3    0.000    0.000    0.000    0.000 threading.py:553(is_set)\n",
       "       16    0.000    0.000    0.000    0.000 stringsource:978(__dealloc__)\n",
       "        8    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}\n",
       "        2    0.000    0.000    0.000    0.000 <string>:2(__init__)\n",
       "        6    0.000    0.000    0.000    0.000 codecs.py:276(reset)\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}\n",
       "        2    0.000    0.000    0.000    0.000 {method 'union' of 'set' objects}\n",
       "        2    0.000    0.000    0.000    0.000 doc.pyx:914(__pyx_fuse_1push_back)\n",
       "       16    0.000    0.000    0.000    0.000 stringsource:561(__get__)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_explanation.py:7(__init__)\n",
       "        3    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
       "       10    0.000    0.000    0.000    0.000 shape_base.py:735(_array_split_dispatcher)\n",
       "        6    0.000    0.000    0.000    0.000 shape_base.py:19(_atleast_1d_dispatcher)\n",
       "        2    0.000    0.000    0.000    0.000 anchor_explanation.py:38(precision)\n",
       "        6    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
       "        6    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
       "        4    0.000    0.000    0.000    0.000 trainable_pipe.pxd:7(__get__)\n",
       "        4    0.000    0.000    0.000    0.000 tokenizer.pyx:86(__get__)\n",
       "        4    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}\n",
       "        2    0.000    0.000    0.000    0.000 matcher.pyx:64(__len__)\n",
       "        2    0.000    0.000    0.000    0.000 softmax.py:113(validate_temperature)\n",
       "        2    0.000    0.000    0.000    0.000 dep_parser.pyx:341(_ensure_labels_are_added)\n",
       "        8    0.000    0.000    0.000    0.000 doc.pxd:44(__get__)\n",
       "        2    0.000    0.000    0.000    0.000 {built-in method time.time}\n",
       "        4    0.000    0.000    0.000    0.000 fromnumeric.py:1474(_squeeze_dispatcher)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
       "        2    0.000    0.000    0.000    0.000 tokenization_utils.py:821(prepare_for_tokenization)\n",
       "        2    0.000    0.000    0.000    0.000 doc.pxd:44(__set__)\n",
       "        2    0.000    0.000    0.000    0.000 shape_base.py:77(_atleast_2d_dispatcher)\n",
       "        2    0.000    0.000    0.000    0.000 contextlib.py:63(_recreate_cm)\n",
       "        4    0.000    0.000    0.000    0.000 tokenizer.pyx:110(__get__)\n",
       "        4    0.000    0.000    0.000    0.000 tokenizer.pyx:94(__get__)\n",
       "        4    0.000    0.000    0.000    0.000 tokenizer.pyx:102(__get__)\n",
       "        4    0.000    0.000    0.000    0.000 tokenizer.pyx:78(__get__)\n",
       "        2    0.000    0.000    0.000    0.000 tokenizer.pyx:507(_save_cached)\n",
       "        2    0.000    0.000    0.000    0.000 matcher.pyx:545(finish_states)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch._C._jit_set_texpr_fuser_enabled(False)\n",
    "my_utils = TextUtils(examples, explainer, predict_sentences, ignored, optimize=optimize)\n",
    "myUtils.model = model\n",
    "myUtils.tokenizer = tokenizer\n",
    "anchor_base.AnchorBaseBeam.best_group = bg\n",
    "set_seed()\n",
    "%prun -s cumtime -T check/profile.txt my_utils.compute_explanations(list(range(len(examples))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1bd996-c9ac-4704-b77e-472c29d051bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### my_utils = TextUtils(anchor_examples, test, explainer, predict_sentences, ignored,f\"profile.pickle\")\n",
    "#%lprun -s -m modified_anchor.anchor_text -m modified_anchor.anchor_base -m myUtils -T profile.txt  my_utils.compute_explanations(list(range(len(anchor_examples))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891a9e98-7566-43f6-8eb1-1ed446033ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.now())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

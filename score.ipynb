{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5073ae0a-f2d7-4f9e-95c1-3353fef4304d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "import spacy\n",
    "import pickle\n",
    "import myUtils\n",
    "import os\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from myUtils import *\n",
    "from models.utils import *\n",
    "\n",
    "SEED = 84\n",
    "torch.manual_seed(SEED)\n",
    "warnings.simplefilter(\"ignore\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2444c8e5-2a8b-4797-9248-46565487f5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can be sentiment/offensive/corona\n",
    "dataset_name = 'corona'\n",
    "model_type = 'tinybert'\n",
    "model_name = 'huawei-noah/TinyBERT_General_4L_312D'\n",
    "sorting = 'polarity'\n",
    "folder_name = f'results/{dataset_name}/{sorting}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "256ac717-4908-418b-a8bb-b762ab071203",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.jit.load(f'models/{model_type}/{dataset_name}/traced.pt').to(device)\n",
    "model = model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "myUtils.model = model\n",
    "myUtils.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "897c090d-1401-4c85-8d1e-528beec36e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1da1181f-98b5-4f15-9896-d4a99cf231be",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa74e7fe-f7b1-4544-b3f6-5485e00a9ecc",
   "metadata": {},
   "source": [
    "# Loading Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba2781ed-fb71-478e-9d30-e393c645b769",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations  = pickle.load(open(f\"{folder_name}/exps_list.pickle\", \"rb\" ))\n",
    "anchor_examples = pickle.load(open(f\"{folder_name}/anchor_examples.pickle\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1a5f566-1476-4b96-b5f9-2366fd83f0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anchor examples len: 3744\n",
      "explanations len: 68928\n"
     ]
    }
   ],
   "source": [
    "print(f'anchor examples len: {len(anchor_examples)}')\n",
    "print(f'explanations len: {len(explanations)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "857cef8f-598b-4633-874b-86dc55dfa8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need in the new version of transformers\n",
    "#torch._C._jit_set_texpr_fuser_enabled(False)\n",
    "if not os.path.exists(f\"{folder_name}/predictions.pickle\"):\n",
    "    predictions = [predict_sentences([tokenizer.tokenize(anchor_example)])[0] for anchor_example in anchor_examples]\n",
    "    pickle.dump(predictions, open(f\"{folder_name}/predictions.pickle\", \"wb\" ))\n",
    "    \n",
    "labels = pickle.load(open(f\"{folder_name}/predictions.pickle\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcf8368d-4154-4bdb-bd79-017fd65aa076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "# get all anchor above 0.95, multiple in a sentence but word counts only once in a sentence\n",
    "def get_best(explanations):\n",
    "    best_exps = dict()\n",
    "    for exp in explanations:\n",
    "        if exp.precision < 0.95:\n",
    "            continue\n",
    "        if exp.index not in best_exps.keys():\n",
    "            best_exps[exp.index]=[exp]\n",
    "        # if word already appeard in sentence\n",
    "        elif any([cur_exp.names[0]==exp.names[0] for cur_exp in best_exps[exp.index]]):\n",
    "            continue\n",
    "        else:\n",
    "            best_exps[exp.index].append(exp)\n",
    "    print(len(best_exps))\n",
    "    return reduce(lambda x,y: x+y, best_exps.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "931f7a26-bdb7-4cbe-b2ee-6f9fde16110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_anchor_occurences(explanations):\n",
    "    c = Counter()\n",
    "    for exp in explanations:\n",
    "        c.update([exp.names[0]])\n",
    " \n",
    "    return c\n",
    "\n",
    "def get_normal_occurences(sentences, anchor_occurences):\n",
    "    c = Counter()\n",
    "    for sentence in sentences:\n",
    "        c.update([x.text for x in nlp.tokenizer(sentence)])\n",
    "   \n",
    "    #removing occurences of the words as anchor\n",
    "    for word in anchor_occurences.keys():\n",
    "        c[word]-=anchor_occurences[word]\n",
    "        \n",
    "    return c\n",
    "\n",
    "def smooth_before(normal_occurences, anchor_occurences_list):\n",
    "    for w in normal_occurences:\n",
    "        normal_occurences[w]+=1\n",
    "        for anchor_occurences in anchor_occurences_list:\n",
    "            anchor_occurences[w]+=1\n",
    "\n",
    "def smooth_after(teta1, type_occurences):\n",
    "    # removing words we added 1 at the start smooth\n",
    "    words = list(teta1.keys())\n",
    "    for word in words:\n",
    "        if type_occurences[word]<=1:\n",
    "            del teta1[word]\n",
    "            \n",
    "    min_val = min(teta1.values()) \n",
    "    if min_val<0:\n",
    "        for w in teta1:\n",
    "            teta1[w]-= min_val\n",
    "        sum_val = sum(teta1.values())\n",
    "        for w in teta1:\n",
    "            teta1[w]= teta1[w]/sum_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60535acf-32f9-4729-9596-eaf3953b39ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_teta0(normal_occurences):\n",
    "    teta0 = dict()\n",
    "    sum_occurences = sum(normal_occurences.values())\n",
    "    for word, count in normal_occurences.items():\n",
    "        teta0[word] = count/sum_occurences\n",
    "    \n",
    "    return teta0\n",
    "\n",
    "def calculate_teta1(anchor_occurences, teta0, alpha):\n",
    "    teta1 = dict()\n",
    "    sum_occurences = sum(anchor_occurences.values())\n",
    "    for word, count in anchor_occurences.items():\n",
    "        teta1[word] = count/sum_occurences -(1-alpha)*teta0[word]\n",
    "        teta1[word] = teta1[word]/alpha\n",
    "    \n",
    "    return teta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b81eace0-cc44-41a1-b1df-7f681d185f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores():\n",
    "    alphas = [0.95, 0.8, 0.65, 0.5]\n",
    "    dfs = []\n",
    "    columns = ['name', 'anchor score', 'type occurences', 'total occurences','+%', '-%', 'both', 'normal']\n",
    "    \n",
    "    exps = get_best(explanations)\n",
    "    pos_exps = [exp for exp in exps if labels[exp.index]==0]\n",
    "    neg_exps = [exp for exp in exps if labels[exp.index]==1]\n",
    "    \n",
    "    anchor_occurences = get_anchor_occurences(exps)\n",
    "    pos_occurences = get_anchor_occurences(pos_exps)\n",
    "    neg_occurences = get_anchor_occurences(neg_exps)\n",
    "    \n",
    "    normal_occurences = get_normal_occurences(anchor_examples, anchor_occurences)\n",
    "    smooth_before(normal_occurences, [pos_occurences, neg_occurences])\n",
    "\n",
    "    teta0 = calculate_teta0(normal_occurences)\n",
    "    \n",
    "    \n",
    "    for alpha in alphas:\n",
    "        df_pos, df_neg = [], []\n",
    "        \n",
    "        teta_pos = calculate_teta1(pos_occurences, teta0, alpha)\n",
    "        smooth_after(teta_pos, pos_occurences)\n",
    "        \n",
    "        teta_neg = calculate_teta1(neg_occurences, teta0, alpha)\n",
    "        smooth_after(teta_neg, neg_occurences)\n",
    "        \n",
    "        # substracting 1 because of the smoothing\n",
    "        for anchor, score in teta_pos.items():\n",
    "            pos_percent = round((pos_occurences[anchor]-1)/anchor_occurences[anchor], 2)\n",
    "            neg_percent = 1-pos_percent\n",
    "            both = (pos_occurences[anchor]-1)>0 and (neg_occurences[anchor]-1)>0\n",
    "            df_pos.append([anchor, score , pos_occurences[anchor]-1, anchor_occurences[anchor], pos_percent, neg_percent, both,  normal_occurences[anchor]-1]) \n",
    "            \n",
    "        \n",
    "        for anchor, score in teta_neg.items():\n",
    "            pos_percent = round((pos_occurences[anchor]-1)/anchor_occurences[anchor], 2)\n",
    "            neg_percent = 1-pos_percent\n",
    "            both = (pos_occurences[anchor]-1)>0 and (neg_occurences[anchor]-1)>0\n",
    "            df_neg.append([anchor, score , neg_occurences[anchor]-1, anchor_occurences[anchor], pos_percent, neg_percent, both,  normal_occurences[anchor]-1]) \n",
    "            \n",
    "        df_pos.sort(key=lambda exp: -exp[1])\n",
    "        df_neg.sort(key=lambda exp: -exp[1])\n",
    "        df_pos = pd.DataFrame(data = df_pos, columns = columns ).set_index('name')\n",
    "        df_neg = pd.DataFrame(data = df_neg, columns = columns ).set_index('name')\n",
    "        \n",
    "        dfs.extend([df_pos, df_neg])\n",
    "        \n",
    "    writer = pd.ExcelWriter(f'{folder_name}/scores.xlsx',engine='xlsxwriter') \n",
    "    \n",
    "    workbook=writer.book\n",
    "    worksheet=workbook.add_worksheet('Sheet1')\n",
    "    writer.sheets['Sheet1'] = worksheet\n",
    "    \n",
    "    cur_col = 0\n",
    "    is_positive = False\n",
    "    alphas = np.repeat(alphas, 2)\n",
    "    \n",
    "    for df, alpha in zip(dfs, alphas):\n",
    "        cur_type = 'positive' if is_positive else 'negative'\n",
    "        is_positive = not is_positive\n",
    "        worksheet.write(0, cur_col, f'{alpha}-{cur_type}')\n",
    "        df.to_excel(writer, sheet_name=f'Sheet1', startrow=1, startcol=cur_col)\n",
    "        cur_col+= len(columns) + 1\n",
    "\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "245d0db5-007c-4b77-83bc-dd588c65fa1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2156\n"
     ]
    }
   ],
   "source": [
    "calculate_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a652cc91-4048-4b6d-95e3-3d6fb342ea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_loss(path1, path2):\n",
    "    results = []\n",
    "    \n",
    "    def intersect_df(d1, d2, top):\n",
    "        s1 = set(d1.head(top).tolist())\n",
    "        s2 = set(d2.head(top).tolist())\n",
    "        percentage = len(s1.intersection(s2))/top\n",
    "        \n",
    "        return percentage\n",
    "              \n",
    "    alphas = [0.95, 0.8, 0.65, 0.5]\n",
    "    top = 25\n",
    "    df1 = pd.read_excel(path1).drop(0)\n",
    "    df2 = pd.read_excel(path2).drop(0)\n",
    "    \n",
    "    for alpha in alphas:\n",
    "      \n",
    "        pos_percentage = intersect_df(df1[f'{alpha}-positive'], df2[f'{alpha}-positive'], top)\n",
    "        \n",
    "        neg_percentage = intersect_df(df1[f'{alpha}-negative'], df2[f'{alpha}-negative'], top)\n",
    "        \n",
    "        results.append([alpha, pos_percentage, neg_percentage])\n",
    "    \n",
    "    df = pd.DataFrame(data = results, columns = ['alpha', 'pos', 'neg']).set_index('alpha')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9680f27a-fa4f-47f3-ad50-a16d6bddb6ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/corona/polarity/scores.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_346733/3060830872.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompare_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{folder_name}/scores.xlsx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'results/{dataset_name}/{sorting}/scores.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_346733/527473127.py\u001b[0m in \u001b[0;36mcompare_loss\u001b[0;34m(path1, path2)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1189\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1192\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m                 )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1071\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m     ) as handle:\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/corona/polarity/scores.xlsx'"
     ]
    }
   ],
   "source": [
    "compare_loss(f'{folder_name}/scores.xlsx', f'results/{dataset_name}/{sorting}/scores.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4043150-8eaf-4be4-9b5c-5c2c8596f278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def present_alpha_monitor():\n",
    "    top = 25\n",
    "    df = pd.read_excel(f'{folder_name}/scores.xlsx').drop(0).head(top)\n",
    "    \n",
    "    with open(f'{folder_name}/pos_monitor.csv', \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        pos_lines = [line for line in reader]\n",
    "\n",
    "    with open(f'{folder_name}/neg_monitor.csv', \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        neg_lines = [line for line in reader]\n",
    "        \n",
    "    with open(f'{folder_name}/time_monitor.csv', \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        time_lines = [float(line[0]) for line in reader]\n",
    "\n",
    "    alphas = [0.95, 0.8, 0.65, 0.5]\n",
    "    results = dict.fromkeys(alphas, {'pos': [], 'neg': []})\n",
    "    \n",
    "    for i, alpha in enumerate(alphas): \n",
    "        top_pos = set(df[f'{alpha}-positive'].to_list())\n",
    "        top_neg = set(df[f'{alpha}-negative'].to_list())\n",
    "        \n",
    "        results[alpha]['pos'] = [len(top_pos.intersection(set(line)))/top for line in pos_lines]\n",
    "        results[alpha]['neg'] = [len(top_neg.intersection(set(line)))/top for line in neg_lines]\n",
    "        \n",
    "        plt.plot(time_lines, results[alpha]['pos'], label = 'positive')\n",
    "        plt.plot(time_lines, results[alpha]['neg'], label = 'negative')\n",
    "        plt.xlabel('time (minutes)')\n",
    "        plt.ylabel('percent')\n",
    "\n",
    "        plt.title(alpha)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03bdf56-3d9f-41fc-a32c-8378d171c2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "present_alpha_monitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229e166f-1cb2-4a5c-b193-a9ee25a175e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def present_group_monitor():\n",
    "    top = 25\n",
    "    \n",
    "    with open(f'{folder_name}/pos_monitor.csv', \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        pos_lines = [line for line in reader]\n",
    "\n",
    "    with open(f'{folder_name}/neg_monitor.csv', \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        neg_lines = [line for line in reader]\n",
    "        \n",
    "    with open(f'{folder_name}/time_monitor.csv', \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        time_lines = [float(line[0]) for line in reader]\n",
    "\n",
    "    results = dict()\n",
    "    \n",
    "    top_pos = set(pos_lines[-1])\n",
    "    top_neg = set(neg_lines[-1])\n",
    "\n",
    "    results['pos'] = [len(top_pos.intersection(set(line)))/top for line in pos_lines]\n",
    "    results['neg'] = [len(top_neg.intersection(set(line)))/top for line in neg_lines]\n",
    "\n",
    "    plt.plot(time_lines, results['pos'], label = 'positive')\n",
    "    plt.plot(time_lines, results['neg'], label = 'negative')\n",
    "    plt.xlabel('time (minutes)')\n",
    "    plt.ylabel('percent')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f1f007-6b50-4a39-bb85-0290a80bea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "present_group_monitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c4b503-baf4-4b76-bacf-7885d3a182fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def get_lines(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        return [line for line in reader]\n",
    "    \n",
    "def present_deltas_monitor(deltas):\n",
    "    \"\"\" \n",
    "    compare final topk for different deltas to the default delta (0.1)\n",
    "    \"\"\"\n",
    "    top = 25\n",
    "        \n",
    "    default_pos_lines = get_lines(f'{folder_name}/{0.1}/pos_monitor.csv')\n",
    "    default_neg_lines = get_lines(f'{folder_name}/{0.1}/neg_monitor.csv')\n",
    "    default_time_lines = [float(line[0]) for line in get_lines(f'{folder_name}/{0.1}/time_monitor.csv')]\n",
    "    default_results = dict()\n",
    "    \n",
    "    top_pos = set(default_pos_lines[-1])\n",
    "    top_neg = set(default_neg_lines[-1])\n",
    "    \n",
    "    default_results['pos'] = [len(top_pos.intersection(set(line)))/top for line in default_pos_lines]\n",
    "    default_results['neg'] = [len(top_neg.intersection(set(line)))/top for line in default_neg_lines]\n",
    "    \n",
    "    results = defaultdict(dict)\n",
    "    \n",
    "    for delta in deltas:\n",
    "        pos_lines = get_lines(f'{folder_name}/{delta}/pos_monitor.csv')\n",
    "        neg_lines = get_lines(f'{folder_name}/{delta}/neg_monitor.csv')\n",
    "        results['time'][delta] = [float(line[0]) for line in get_lines(f'{folder_name}/{delta}/time_monitor.csv')]\n",
    "        results['pos'][delta] = [len(top_pos.intersection(set(line)))/top for line in pos_lines]\n",
    "        results['neg'][delta] = [len(top_neg.intersection(set(line)))/top for line in neg_lines]\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 1, figsize=(15, 18))\n",
    "    axs[0].plot(default_time_lines, default_results['pos'], label = 'default (0.1)')\n",
    "    axs[0].set_title(f'positive {delta}')\n",
    "    axs[1].plot(default_time_lines, default_results['neg'], label = 'default (0.1)')\n",
    "    axs[1].set_title(f'negative {delta}')\n",
    "    \n",
    "    for delta in deltas:\n",
    "        axs[0].plot(results['time'][delta] , results['pos'][delta], label = str(delta))\n",
    "        axs[1].plot(results['time'][delta] , results['neg'][delta], label = str(delta))\n",
    "        \n",
    "    for ax in axs.flat:\n",
    "        ax.set(xlabel='time (minutes)', ylabel='percent')\n",
    "        ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc9cac6-f375-425f-a3dc-f232884fcc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas = [0.20, 0.35, 0.5]\n",
    "present_deltas_monitor(deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9ac2ed-6529-45fc-aae3-3fec7a2736e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

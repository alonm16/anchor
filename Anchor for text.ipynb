{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.linear_model\n",
    "import sklearn.ensemble\n",
    "import spacy\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from anchor import anchor_text\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset from http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "# Link: http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz\n",
    "def load_polarity(path='sentiment-sentences'):\n",
    "    data = []\n",
    "    labels = []\n",
    "    f_names = ['rt-polarity.neg', 'rt-polarity.pos']\n",
    "    for (l, f) in enumerate(f_names):\n",
    "        for line in open(os.path.join(path, f), 'rb'):\n",
    "            try:\n",
    "                line.decode('utf8')\n",
    "            except:\n",
    "                continue\n",
    "            data.append(line.strip())\n",
    "            labels.append(l)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you must have spacy installed. Run:\n",
    "\n",
    "        pip install spacy && python -m spacy download en_core_web_sm\n",
    "\n",
    "If you want to run BERT, you have to install transformers and torch or tf: \n",
    "\n",
    "        pip install torch transformers spacy && python -m spacy download en_core_web_sm\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = load_polarity()\n",
    "train, test, train_labels, test_labels = sklearn.model_selection.train_test_split(data, labels, test_size=.2, random_state=42)\n",
    "train, val, train_labels, val_labels = sklearn.model_selection.train_test_split(train, train_labels, test_size=.1, random_state=42)\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)\n",
    "val_labels = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=1)\n",
    "vectorizer.fit(train)\n",
    "train_vectors = vectorizer.transform(train)\n",
    "test_vectors = vectorizer.transform(test)\n",
    "val_vectors = vectorizer.transform(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy 0.7544910179640718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alon\\miniconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "c = sklearn.linear_model.LogisticRegression()\n",
    "# c = sklearn.ensemble.RandomForestClassifier(n_estimators=500, n_jobs=10)\n",
    "c.fit(train_vectors, train_labels)\n",
    "preds = c.predict(val_vectors)\n",
    "print('Val accuracy', sklearn.metrics.accuracy_score(val_labels, preds))\n",
    "def predict_lr(texts):\n",
    "    return c.predict(vectorizer.transform(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining a prediction\n",
    "use_unk_distribution=True means we will perturb examples by replacing words with UNKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = anchor_text.AnchorText(nlp, ['negative', 'positive'], use_unk_distribution=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: positive\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "text = 'This is a good book .'\n",
    "pred = explainer.class_names[predict_lr([text])[0]]\n",
    "alternative =  explainer.class_names[1 - predict_lr([text])[0]]\n",
    "print('Prediction: %s' % pred)\n",
    "exp = explainer.explain_instance(text, predict_lr, threshold=0.95)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the anchor. Note that using this perturbation distribution, having the word 'good' in the text virtually guarantees a positive prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: good\n",
      "Precision: 1.00\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "\n",
      "UNK is UNK good book UNK\n",
      "This is a good book .\n",
      "UNK is UNK good book .\n",
      "UNK UNK a good book .\n",
      "UNK is UNK good UNK UNK\n",
      "UNK is UNK good UNK .\n",
      "This is UNK good book .\n",
      "UNK UNK UNK good book UNK\n",
      "UNK is UNK good UNK .\n",
      "This is a good UNK UNK\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Anchor: %s' % (' AND '.join(exp.names())))\n",
    "print('Precision: %.2f' % exp.precision())\n",
    "print()\n",
    "print('Examples where anchor applies and model predicts %s:' % pred)\n",
    "print()\n",
    "print('\\n'.join([x[0] for x in exp.examples(only_same_prediction=True)]))\n",
    "print()\n",
    "print('Examples where anchor applies and model predicts %s:' % alternative)\n",
    "print()\n",
    "print('\\n'.join([x[0] for x in exp.examples(partial_index=0, only_different_prediction=True)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution above is a bit naive, and you get a lot of sentences that don't look realistic.  \n",
    "Let's use BERT to perturb the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = anchor_text.AnchorText(nlp, ['negative', 'positive'], use_unk_distribution=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: positive\n",
      "Time: 283.98552346229553\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "text = 'This is a good book .'\n",
    "pred = explainer.class_names[predict_lr([text])[0]]\n",
    "alternative =  explainer.class_names[1 - predict_lr([text])[0]]\n",
    "print('Prediction: %s' % pred)\n",
    "b = time.time()\n",
    "exp = explainer.explain_instance(text, predict_lr, threshold=0.95, verbose=False)\n",
    "print('Time: %s' % (time.time() - b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: good AND book\n",
      "Precision: 0.95\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "\n",
      "book ##marks a good book series\n",
      "it is a good book .\n",
      "bold = certified good book seller\n",
      "It is a good book ?\n",
      "dear ##ing them good book .\n",
      "it is a good book .\n",
      "future is a good book ?\n",
      "https : One good book reviews\n",
      "he is a good book .\n",
      "In here a good book ;\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "\n",
      "there is no good book ##ners\n",
      "can any single good book ?\n",
      "t ##was a good book before\n",
      "there is no good book there\n",
      "now here goes good book .\n",
      "everything is a good book .\n",
      "this is a good book edition\n",
      "poor chances for good book .\n",
      "everything is a good book !\n",
      "here comes a good book ##let\n"
     ]
    }
   ],
   "source": [
    "print('Anchor: %s' % (' AND '.join(exp.names())))\n",
    "print('Precision: %.2f' % exp.precision())\n",
    "print()\n",
    "print('Examples where anchor applies and model predicts %s:' % pred)\n",
    "print()\n",
    "print('\\n'.join([x[0] for x in exp.examples(only_same_prediction=True)]))\n",
    "print()\n",
    "print('Examples where anchor applies and model predicts %s:' % alternative)\n",
    "print()\n",
    "print('\\n'.join([x[0] for x in exp.examples(only_different_prediction=True)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the partial anchor 'good' to see why it's not sufficient in this case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial anchor: good\n",
      "Precision: 0.83\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "\n",
      "sleep is not good enough ;\n",
      "that lasted a good hour .\n",
      "there is always good luck :\n",
      "wheat is a good crop .\n",
      "1970 / 71 good ##wood ##cut\n",
      "all for a good mortal .\n",
      "thou wit a good favour !\n",
      "this is very good information because\n",
      "photography is generally good quality .\n",
      "You make a good fortune .\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "\n",
      "Their names meant good people .\n",
      "anything else provided good fortune .\n",
      "This is very good video ##graphy\n",
      "love is everything good and bad\n",
      "multiplayer is a good idea .\n",
      "God is doing good deeds …\n",
      ": wins by good ##mans loss\n",
      "he gave a good appearance .\n",
      "your conception a good idea .\n",
      "This is now good news !\n"
     ]
    }
   ],
   "source": [
    "print('Partial anchor: %s' % (' AND '.join(exp.names(0))))\n",
    "print('Precision: %.2f' % exp.precision(0))\n",
    "print()\n",
    "print('Examples where anchor applies and model predicts %s:' % pred)\n",
    "print()\n",
    "print('\\n'.join([x[0] for x in exp.examples(partial_index=0, only_same_prediction=True)]))\n",
    "print()\n",
    "print('Examples where anchor applies and model predicts %s:' % alternative)\n",
    "print()\n",
    "print('\\n'.join([x[0] for x in exp.examples(partial_index=0, only_different_prediction=True)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the examples are much more realistic than just using UNKS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are sampling from BERT sequentially above (one mask at a time), to get sentences that are more coherent.  \n",
    "If you want to do a single BERT pass per sample, you can set `onepass=True`. You may lose some coherence, but it will be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: positive\n",
      "Time: 9.157501697540283\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "text = 'This is a good book .'\n",
    "pred = explainer.class_names[predict_lr([text])[0]]\n",
    "alternative =  explainer.class_names[1 - predict_lr([text])[0]]\n",
    "print('Prediction: %s' % pred)\n",
    "b = time.time()\n",
    "exp = explainer.explain_instance(text, predict_lr, threshold=0.95, verbose=False, onepass=True)\n",
    "print('Time: %s' % (time.time() - b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: good AND book\n",
      "Precision: 0.96\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "\n",
      "her writing a good book .\n",
      "what teen not good book award\n",
      "• ##r reads good book ?\n",
      "< write a good book ?\n",
      "award / say good book ##keeper\n",
      "jack is a good book !\n",
      "1971 chapter for good book signing\n",
      "2000 : a good book ##worm\n",
      "• ##with A good book award\n",
      "It was a good book .\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "\n",
      "me thy say good book .\n",
      "everything was a good book .\n",
      "hardly for a good book .\n",
      "this merit her good book .\n",
      "everything is a good book .\n",
      "everything is a good book cover\n",
      "he me a good book .\n",
      "no all made good book .\n",
      "chapter excellent awful good book .\n",
      "Everything – a good book !\n"
     ]
    }
   ],
   "source": [
    "print('Anchor: %s' % (' AND '.join(exp.names())))\n",
    "print('Precision: %.2f' % exp.precision())\n",
    "print()\n",
    "print('Examples where anchor applies and model predicts %s:' % pred)\n",
    "print()\n",
    "print('\\n'.join([x[0] for x in exp.examples(only_same_prediction=True)]))\n",
    "print()\n",
    "print('Examples where anchor applies and model predicts %s:' % alternative)\n",
    "print()\n",
    "print('\\n'.join([x[0] for x in exp.examples(only_different_prediction=True)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the partial anchor 'good' to see why it's not sufficient in this case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial anchor: good\n",
      "Precision: 0.85\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7248/258786095.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Examples where anchor applies and model predicts %s:'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartial_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monly_same_prediction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Examples where anchor applies and model predicts %s:'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0malternative\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\anchor\\anchor\\anchor_explanation.py\u001b[0m in \u001b[0;36mexamples\u001b[1;34m(self, only_different_prediction, only_same_prediction, partial_index)\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'examples'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mas_html\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print('Partial anchor: %s' % (' AND '.join(exp.names(0))))\n",
    "print('Precision: %.2f' % exp.precision(0))\n",
    "print()\n",
    "print('Examples where anchor applies and model predicts %s:' % pred)\n",
    "print()\n",
    "print('\\n'.join([x[0] for x in exp.examples(partial_index=2, only_same_prediction=True)]))\n",
    "print()\n",
    "print('Examples where anchor applies and model predicts %s:' % alternative)\n",
    "print()\n",
    "print('\\n'.join([x[0] for x in exp.examples(partial_index=2, only_different_prediction=True)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See a visualization of the anchor with examples and etc (won't work if you're seeing this on github)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

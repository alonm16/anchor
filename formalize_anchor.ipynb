{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5073ae0a-f2d7-4f9e-95c1-3353fef4304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "import spacy\n",
    "from modified_anchor import anchor_text\n",
    "import pickle\n",
    "import myUtils\n",
    "from myUtils import *\n",
    "from transformer.utils import *\n",
    "from dataset.dataset_loader import *\n",
    "import datetime\n",
    "%load_ext line_profiler\n",
    "\n",
    "SEED = 84\n",
    "torch.manual_seed(SEED)\n",
    "warnings.simplefilter(\"ignore\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2444c8e5-2a8b-4797-9248-46565487f5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18046\n",
      "15398\n",
      "Number of tokens in training samples: 8755\n",
      "Number of tokens in training labels: 2\n"
     ]
    }
   ],
   "source": [
    "# can be sentiment/spam/offensive\n",
    "dataset_name = 'corona-topk'\n",
    "text_parser, label_parser, ds_train, ds_val = get_dataset('corona')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "153fa98e-e8bc-4543-8f98-e5fde929cc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_dim': 100, 'batch_size': 32, 'hidden_dim': 256, 'num_layers': 2, 'dropout': 0.3, 'lr': 5e-05, 'early_stopping': 5, 'output_classes': 2}\n",
      "VanillaGRU(\n",
      "  (embedding_layer): Embedding(8755, 100)\n",
      "  (GRU_layer): GRU(100, 256, num_layers=2, dropout=0.3)\n",
      "  (dropout_layer): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = load_model('gru' , f'transformer/corona/gru.pt', text_parser)\n",
    "myUtils.model = torch.jit.script(model)\n",
    "myUtils.text_parser = text_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0ae844-8cfc-48c9-b46f-c41106d4dd8e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Anchor Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1da1181f-98b5-4f15-9896-d4a99cf231be",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c976b285-caee-439e-870d-a43850a4551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = anchor_text.AnchorText(nlp, ['positive', 'negative'], use_unk_distribution=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa74e7fe-f7b1-4544-b3f6-5485e00a9ecc",
   "metadata": {},
   "source": [
    "# Loading Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba2781ed-fb71-478e-9d30-e393c645b769",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array(pickle.load( open(f\"{dataset_name}/test.pickle\", \"rb\" )))\n",
    "test_labels = np.array(pickle.load( open(f\"{dataset_name}/test_labels.pickle\", \"rb\" )))\n",
    "explanations  = pickle.load(open(f\"{dataset_name}/exps_list.pickle\", \"rb\" ))\n",
    "anchor_examples = pickle.load( open(  f\"{dataset_name}/anchor_examples.pickle\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1a5f566-1476-4b96-b5f9-2366fd83f0d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3744"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anchor_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0157d1c2-7146-4945-9c93-30f1d2a12151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58352"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(explanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "857cef8f-598b-4633-874b-86dc55dfa8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"{dataset_name}/predictions.pickle\"):\n",
    "    predictions = [predict_sentences([str(anchor_example)])[0] for anchor_example in anchor_examples]\n",
    "    pickle.dump( predictions, open( f\"{dataset_name}/predictions.pickle\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9b278ed-8174-4650-922a-d12cc6c5d60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"{dataset_name}/extended_exps.pickle\"):\n",
    "    test_predictions = np.array([predict_sentences([text])[0] for text in test])\n",
    "    explanations = [ExtendedExplanation(exp, anchor_examples, test, test_labels, test_predictions ,predict_sentences, explainer) for exp in explanations if len(exp.fit_examples) > 0]\n",
    "    pickle.dump( explanations, open( f\"{dataset_name}/extended_exps.pickle\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce2037c3-892c-468a-b14f-a31f47c765ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations = pickle.load(open( f\"{dataset_name}/extended_exps.pickle\", \"rb\" ))\n",
    "labels = pickle.load(open( f\"{dataset_name}/predictions.pickle\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcf8368d-4154-4bdb-bd79-017fd65aa076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "# get all anchor above 0.95, multiple in a sentence but word counts only once in a sentence\n",
    "def get_best(explanations):\n",
    "    best_exps = dict()\n",
    "    for exp in explanations:\n",
    "        if exp.precision < 0.95:\n",
    "            continue\n",
    "        if exp.index not in best_exps.keys():\n",
    "            best_exps[exp.index]=[exp]\n",
    "        # if word already appeard in sentence\n",
    "        elif any([cur_exp.names[0]==exp.names[0] for cur_exp in best_exps[exp.index]]):\n",
    "            continue\n",
    "        else:\n",
    "            best_exps[exp.index].append(exp)\n",
    "    print(len(best_exps))\n",
    "    return reduce(lambda x,y: x+y, best_exps.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "931f7a26-bdb7-4cbe-b2ee-6f9fde16110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_anchor_occurences(explanations):\n",
    "    c = Counter()\n",
    "    for exp in explanations:\n",
    "        c.update([exp.names[0]])\n",
    " \n",
    "    return c\n",
    "\n",
    "def get_normal_occurences(sentences, anchor_occurences):\n",
    "    c = Counter()\n",
    "    for sentence in sentences:\n",
    "        c.update([x.text for x in nlp.tokenizer(sentence)])\n",
    "   \n",
    "    #removing occurences of the words as anchor\n",
    "    for word in anchor_occurences.keys():\n",
    "        c[word]-=anchor_occurences[word]\n",
    "        \n",
    "    return c\n",
    "\n",
    "def smooth_before(normal_occurences, anchor_occurences_list):\n",
    "    for w in normal_occurences:\n",
    "        normal_occurences[w]+=1\n",
    "        for anchor_occurences in anchor_occurences_list:\n",
    "            anchor_occurences[w]+=1\n",
    "\n",
    "def smooth_after(teta1, type_occurences):\n",
    "    # removing words we added 1 at the start smooth\n",
    "    words = list(teta1.keys())\n",
    "    for word in words:\n",
    "        if type_occurences[word]<=1:\n",
    "            del teta1[word]\n",
    "    \n",
    "    min_val = min(teta1.values())\n",
    "    if min_val<0:\n",
    "        for w in teta1:\n",
    "            teta1[w]-= min_val\n",
    "        sum_val = sum(teta1.values())\n",
    "        for w in teta1:\n",
    "            teta1[w]= teta1[w]/sum_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60535acf-32f9-4729-9596-eaf3953b39ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_teta0(normal_occurences):\n",
    "    teta0 = dict()\n",
    "    sum_occurences = sum(normal_occurences.values())\n",
    "    for word, count in normal_occurences.items():\n",
    "        teta0[word] = count/sum_occurences\n",
    "    \n",
    "    return teta0\n",
    "\n",
    "def calculate_teta1(anchor_occurences, teta0, alpha):\n",
    "    teta1 = dict()\n",
    "    sum_occurences = sum(anchor_occurences.values())\n",
    "    for word, count in anchor_occurences.items():\n",
    "        teta1[word] = count/sum_occurences -(1-alpha)*teta0[word]\n",
    "        teta1[word] = teta1[word]/alpha\n",
    "    \n",
    "    return teta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "009468b6-ccbb-4a12-920f-42595f0686ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores():\n",
    "    alphas = [0.95, 0.85, 0.65, 0.5]\n",
    "    dfs = []\n",
    "    columns = ['name', 'anchor score',  'anchor occurences', 'normal score', 'normal occurences']\n",
    "    \n",
    "    exps = get_best(explanations)\n",
    "    anchor_occurences = get_anchor_occurences(exps)\n",
    "    normal_occurences = get_normal_occurences(anchor_examples, anchor_occurences)\n",
    "    smooth_before(normal_occurences, [anchor_occurences])\n",
    "    \n",
    "    teta0 = calculate_teta0(normal_occurences)\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        df_list = []\n",
    "        teta1 = calculate_teta1(anchor_occurences, teta0, alpha)\n",
    "        smooth_after(teta1, anchor_occurences)\n",
    "        \n",
    "        for anchor, score in teta1.items():\n",
    "            anchor_score = round(score, 5)\n",
    "            normal_score = round(teta0[anchor], 5)\n",
    "            df_list.append([anchor, anchor_score , anchor_occurences[anchor], normal_score, normal_occurences[anchor]]) \n",
    "            \n",
    "        df_list.sort(key=lambda exp: -exp[1])\n",
    "        df = pd.DataFrame(data = df_list, columns = columns).set_index('name')\n",
    "        \n",
    "        dfs.append(df)\n",
    "        \n",
    "    writer = pd.ExcelWriter(f'{dataset_name}/formalized_scores.xlsx',engine='xlsxwriter') \n",
    "    \n",
    "    workbook=writer.book\n",
    "    worksheet=workbook.add_worksheet('Sheet1')\n",
    "    writer.sheets['Sheet1'] = worksheet\n",
    "    \n",
    "    cur_col = 0\n",
    "    \n",
    "    for df, alpha in zip(dfs, alphas):\n",
    "        worksheet.write(0, cur_col, alpha)\n",
    "        df.to_excel(writer, sheet_name='Sheet1', startrow=1, startcol=cur_col)\n",
    "        cur_col+= len(columns) + 1\n",
    "\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7ae53ce-91e5-4f7a-8f46-256b4c6c9c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores_double():\n",
    "    alphas = [0.95, 0.8, 0.65, 0.5]\n",
    "    dfs = []\n",
    "    columns = ['name', 'anchor score', 'type', 'type occurences', 'total occurences','+%', '-%', 'both', 'normal']\n",
    "    \n",
    "    exps = get_best(explanations)\n",
    "    pos_exps = [exp for exp in exps if labels[exp.index]==0]\n",
    "    neg_exps = [exp for exp in exps if labels[exp.index]==1]\n",
    "    \n",
    "    anchor_occurences = get_anchor_occurences(exps)\n",
    "    pos_occurences = get_anchor_occurences(pos_exps)\n",
    "    neg_occurences = get_anchor_occurences(neg_exps)\n",
    "    \n",
    "    normal_occurences = get_normal_occurences(anchor_examples, anchor_occurences)\n",
    "    smooth_before(normal_occurences, [pos_occurences, neg_occurences])\n",
    "\n",
    "    teta0 = calculate_teta0(normal_occurences)\n",
    "    \n",
    "    \n",
    "    for alpha in alphas:\n",
    "        df_list = []\n",
    "        \n",
    "        teta_pos = calculate_teta1(pos_occurences, teta0, alpha)\n",
    "        smooth_after(teta_pos, pos_occurences)\n",
    "        \n",
    "        teta_neg = calculate_teta1(neg_occurences, teta0, alpha)\n",
    "        smooth_after(teta_neg, neg_occurences)\n",
    "        \n",
    "        # substracting 1 because of the smoothing\n",
    "        for anchor, score in teta_pos.items():\n",
    "            pos_percent = round((pos_occurences[anchor]-1)/anchor_occurences[anchor], 2)\n",
    "            neg_percent = 1-pos_percent\n",
    "            both = (pos_occurences[anchor]-1)>0 and (neg_occurences[anchor]-1)>0\n",
    "            df_list.append([anchor, score , '+', pos_occurences[anchor]-1, anchor_occurences[anchor], pos_percent, neg_percent, both,  normal_occurences[anchor]-1]) \n",
    "            \n",
    "        \n",
    "        for anchor, score in teta_neg.items():\n",
    "            pos_percent = round((pos_occurences[anchor]-1)/anchor_occurences[anchor], 2)\n",
    "            neg_percent = 1-pos_percent\n",
    "            both = (pos_occurences[anchor]-1)>0 and (neg_occurences[anchor]-1)>0\n",
    "            df_list.append([anchor, score , '-', neg_occurences[anchor]-1, anchor_occurences[anchor], pos_percent, neg_percent, both,  normal_occurences[anchor]-1]) \n",
    "            \n",
    "        df_list.sort(key=lambda exp: -exp[1])\n",
    "        df = pd.DataFrame(data = df_list, columns = columns ).set_index('name')\n",
    "        \n",
    "        dfs.append(df)\n",
    "        \n",
    "    writer = pd.ExcelWriter(f'{dataset_name}/formalized_scores_double.xlsx',engine='xlsxwriter') \n",
    "    \n",
    "    workbook=writer.book\n",
    "    worksheet=workbook.add_worksheet('Sheet1')\n",
    "    writer.sheets['Sheet1'] = worksheet\n",
    "    \n",
    "    cur_col = 0\n",
    "    \n",
    "    for df, alpha in zip(dfs, alphas):\n",
    "        worksheet.write(0, cur_col, alpha)\n",
    "        df.to_excel(writer, sheet_name=f'Sheet1', startrow=1, startcol=cur_col)\n",
    "        cur_col+= len(columns) + 1\n",
    "\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "245d0db5-007c-4b77-83bc-dd588c65fa1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2145\n"
     ]
    }
   ],
   "source": [
    "calculate_scores_double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a652cc91-4048-4b6d-95e3-3d6fb342ea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_loss(path1, path2):\n",
    "    def intersect_df(d1, d2):\n",
    "        s1 = set(d1.head(top).tolist())\n",
    "        s2 = set(d2.head(top).tolist())\n",
    "        print(len(s1.intersection(s2))/top)\n",
    "              \n",
    "    alphas = [0.95, 0.8, 0.65, 0.5]\n",
    "    top = 20\n",
    "    df1 = pd.read_excel(path1).drop(0)\n",
    "    df2 = pd.read_excel(path2).drop(0)\n",
    "    \n",
    "    for i, alpha in enumerate(alphas):\n",
    "        print(f'\\nalpha: {alpha}\\n')\n",
    "        print('all')\n",
    "        intersect_df(df1[alpha], df2[alpha])\n",
    "     \n",
    "        pos_df1 = df1[df1.iloc[:,2+i*10]=='+']\n",
    "        pos_df2 = df2[df2.iloc[:,2+i*10]=='+']\n",
    "        neg_df1 = df1[df1.iloc[:,2+i*10]=='-']\n",
    "        neg_df2 = df2[df2.iloc[:,2+i*10]=='-']\n",
    "\n",
    "        print('\\npos')\n",
    "        intersect_df(pos_df1[alpha], pos_df2[alpha])\n",
    "        print('\\nneg')\n",
    "        intersect_df(neg_df1[alpha], neg_df2[alpha])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9680f27a-fa4f-47f3-ad50-a16d6bddb6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "alpha: 0.95\n",
      "\n",
      "all\n",
      "0.9\n",
      "\n",
      "pos\n",
      "0.9\n",
      "\n",
      "neg\n",
      "0.9\n",
      "\n",
      "alpha: 0.8\n",
      "\n",
      "all\n",
      "0.95\n",
      "\n",
      "pos\n",
      "1.0\n",
      "\n",
      "neg\n",
      "0.85\n",
      "\n",
      "alpha: 0.65\n",
      "\n",
      "all\n",
      "0.95\n",
      "\n",
      "pos\n",
      "1.0\n",
      "\n",
      "neg\n",
      "0.85\n",
      "\n",
      "alpha: 0.5\n",
      "\n",
      "all\n",
      "0.9\n",
      "\n",
      "pos\n",
      "1.0\n",
      "\n",
      "neg\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "compare_loss(f'{dataset_name}/formalized_scores_double.xlsx', f'corona/formalized_scores_double.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "deca214b-a6cf-4ccb-869d-23921fe430cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.95</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 29</th>\n",
       "      <th>0.5</th>\n",
       "      <th>Unnamed: 31</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "      <th>Unnamed: 33</th>\n",
       "      <th>Unnamed: 34</th>\n",
       "      <th>Unnamed: 35</th>\n",
       "      <th>Unnamed: 36</th>\n",
       "      <th>Unnamed: 37</th>\n",
       "      <th>Unnamed: 38</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>name</td>\n",
       "      <td>anchor score</td>\n",
       "      <td>type</td>\n",
       "      <td>type occurences</td>\n",
       "      <td>total occurences</td>\n",
       "      <td>+%</td>\n",
       "      <td>-%</td>\n",
       "      <td>both</td>\n",
       "      <td>normal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>name</td>\n",
       "      <td>anchor score</td>\n",
       "      <td>type</td>\n",
       "      <td>type occurences</td>\n",
       "      <td>total occurences</td>\n",
       "      <td>+%</td>\n",
       "      <td>-%</td>\n",
       "      <td>both</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.</td>\n",
       "      <td>0.060611</td>\n",
       "      <td>-</td>\n",
       "      <td>324</td>\n",
       "      <td>324</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2360</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wonderful</td>\n",
       "      <td>0.035915</td>\n",
       "      <td>+</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>0.015714</td>\n",
       "      <td>-</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>resonant</td>\n",
       "      <td>0.024113</td>\n",
       "      <td>+</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>,</td>\n",
       "      <td>0.015386</td>\n",
       "      <td>-</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>844</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>warm</td>\n",
       "      <td>0.020814</td>\n",
       "      <td>+</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-</td>\n",
       "      <td>0.010728</td>\n",
       "      <td>-</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>394</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mesmerizing</td>\n",
       "      <td>0.018073</td>\n",
       "      <td>+</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1381</th>\n",
       "      <td>entertaining</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>+</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>-</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1382</th>\n",
       "      <td>way</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>-</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383</th>\n",
       "      <td>work</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384</th>\n",
       "      <td>very</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>funny</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>its</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>148</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>and</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1386 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0.95    Unnamed: 1 Unnamed: 2       Unnamed: 3  \\\n",
       "0             name  anchor score       type  type occurences   \n",
       "1                .      0.060611          -              324   \n",
       "2                a      0.015714          -               89   \n",
       "3                ,      0.015386          -               85   \n",
       "4                -      0.010728          -               56   \n",
       "...            ...           ...        ...              ...   \n",
       "1381  entertaining      0.000354          +                1   \n",
       "1382           way      0.000349          -                1   \n",
       "1383          work      0.000324          -                1   \n",
       "1384          very      0.000302          -                1   \n",
       "1385           its             0          -                1   \n",
       "\n",
       "            Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8  \\\n",
       "0     total occurences         +%         -%       both     normal   \n",
       "1                  324          0          1      False       2360   \n",
       "2                   89          0          1      False       1007   \n",
       "3                   85          0          1      False        844   \n",
       "4                   56          0          1      False        394   \n",
       "...                ...        ...        ...        ...        ...   \n",
       "1381                 1          1          0      False         21   \n",
       "1382                 1          0          1      False         36   \n",
       "1383                 1          0          1      False         44   \n",
       "1384                 1          0          1      False         51   \n",
       "1385                 1          0          1      False        148   \n",
       "\n",
       "      Unnamed: 9  ... Unnamed: 29          0.5   Unnamed: 31 Unnamed: 32  \\\n",
       "0            NaN  ...         NaN         name  anchor score        type   \n",
       "1            NaN  ...         NaN    wonderful      0.035915           +   \n",
       "2            NaN  ...         NaN     resonant      0.024113           +   \n",
       "3            NaN  ...         NaN         warm      0.020814           +   \n",
       "4            NaN  ...         NaN  mesmerizing      0.018073           +   \n",
       "...          ...  ...         ...          ...           ...         ...   \n",
       "1381         NaN  ...         NaN           of      0.000355           -   \n",
       "1382         NaN  ...         NaN            a      0.000187           -   \n",
       "1383         NaN  ...         NaN          the      0.000084           -   \n",
       "1384         NaN  ...         NaN        funny             0           +   \n",
       "1385         NaN  ...         NaN          and             0           -   \n",
       "\n",
       "          Unnamed: 33       Unnamed: 34 Unnamed: 35 Unnamed: 36 Unnamed: 37  \\\n",
       "0     type occurences  total occurences          +%          -%        both   \n",
       "1                  10                10           1           0       False   \n",
       "2                   6                 6           1           0       False   \n",
       "3                   5                 5           1           0       False   \n",
       "4                   4                 4           1           0       False   \n",
       "...               ...               ...         ...         ...         ...   \n",
       "1381               45                45           0           1       False   \n",
       "1382               89                89           0           1       False   \n",
       "1383               58                58           0           1       False   \n",
       "1384                3                 3           1           0       False   \n",
       "1385               19                19           0           1       False   \n",
       "\n",
       "      Unnamed: 38  \n",
       "0          normal  \n",
       "1               1  \n",
       "2               0  \n",
       "3               1  \n",
       "4               0  \n",
       "...           ...  \n",
       "1381          593  \n",
       "1382         1007  \n",
       "1383          872  \n",
       "1384           54  \n",
       "1385          670  \n",
       "\n",
       "[1386 rows x 39 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_excel( f'sentiment/formalized_scores_double.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54d2722-6326-4a17-a0c4-3e0142bcd3aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
